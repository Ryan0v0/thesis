@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{A_Primer_on_Pretrained_Multilingual_LM,
  author       = {Sumanth Doddapaneni and
                  Gowtham Ramesh and
                  Anoop Kunchukuttan and
                  Pratyush Kumar and
                  Mitesh M. Khapra},
  title        = {A Primer on Pretrained Multilingual Language Models},
  journal      = {CoRR},
  volume       = {abs/2107.00676},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.00676},
  eprinttype    = {arXiv},
  eprint       = {2107.00676},
  timestamp    = {Wed, 07 Jul 2021 15:23:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-00676.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{prompt_tuning,
  author       = {Brian Lester and
                  Rami Al{-}Rfou and
                  Noah Constant},
  title        = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  journal      = {CoRR},
  volume       = {abs/2104.08691},
  year         = {2021},
  url          = {https://arxiv.org/abs/2104.08691},
  eprinttype    = {arXiv},
  eprint       = {2104.08691},
  timestamp    = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2104-08691.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Weller2022PretrainedMF,
  title={Pretrained Models for Multilingual Federated Learning},
  author={Orion Weller and Marc Marone and Vladimir Braverman and Dawn J Lawrie and Benjamin Van Durme},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  year={2022}
}

@misc{xue2020mt5,
    title = {{mT5}: A massively multilingual pre-trained text-to-text transformer},
    author = {Linting Xue and Noah Constant and Adam Roberts and Mihir Kale and Rami Al-Rfou and Aditya Siddhant and Aditya Barua and Colin Raffel},
    year = {2020},
    eprint = {2010.11934},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL}
}

@article{tirumala2022memorization,
  title={Memorization without overfitting: Analyzing the training dynamics of large language models},
  author={Tirumala, Kushal and Markosyan, Aram and Zettlemoyer, Luke and Aghajanyan, Armen},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38274--38290},
  year={2022}
}

@inproceedings{sitaram-etal-2023-everything,
    title = "Everything you need to know about Multilingual {LLM}s: Towards fair, performant and reliable models for languages of the world",
    author = "Sitaram, Sunayana  and
      Choudhury, Monojit  and
      Patra, Barun  and
      Chaudhary, Vishrav  and
      Ahuja, Kabir  and
      Bali, Kalika",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-tutorials.3",
    doi = "10.18653/v1/2023.acl-tutorials.3",
    pages = "21--26",
    abstract = "This tutorial will describe various aspects of scaling up language technologies to many of the world{'}s languages by describing the latest research in Massively Multilingual Language Models (MMLMs). We will cover topics such as data collection, training and fine-tuning of models, Responsible AI issues such as fairness, bias and toxicity, linguistic diversity and evaluation in the context of MMLMs, specifically focusing on issues in non-English and low-resource languages. Further, we will also talk about some of the real-world challenges in deploying these models in language communities in the field. With the performance of MMLMs improving in the zero-shot setting for many languages, it is now becoming feasible to use them for building language technologies in many languages of the world, and this tutorial will provide the computational linguistics community with unique insights from the latest research in multilingual models.",
}

@article{konevcny2016federated2,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@inproceedings{fl1,
  title={Privacy-preserving deep learning},
  author={Shokri, Reza and Shmatikov, Vitaly},
  booktitle={Proceedings of the 22nd ACM SIGSAC conference on computer and communications security},
  pages={1310--1321},
  year={2015}
}

@inproceedings{fedavg,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  year={2017},
  organization={PMLR}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{chowdhery2022palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  year={2022}
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@inproceedings{Prefix-Tuning,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
}

@article{p-tuning,
    title={GPT Understands, Too},
    author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
    journal={arXiv:2103.10385},
    year={2021}
    }


@InProceedings{adapter,
  title = 	 {Parameter-Efficient Transfer Learning for {NLP}},
  author =       {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2790--2799},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/houlsby19a.html},
  abstract = 	 {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to $26$ diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within $0.8%$ of the performance of full fine-tuning, adding only $3.6%$ parameters per task. By contrast, fine-tuning trains $100%$ of the parameters per task.}
}


@article{lora,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Weizhu Chen},
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2106.09685},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.09685},
  eprinttype    = {arXiv},
  eprint       = {2106.09685},
  timestamp    = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-09685.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bitfit,
    title = "{B}it{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
    author = "Ben Zaken, Elad  and
      Goldberg, Yoav  and
      Ravfogel, Shauli",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.1",
    doi = "10.18653/v1/2022.acl-short.1",
    pages = "1--9",
    abstract = "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
}

@misc{flower,
  doi = {10.48550/ARXIV.2007.14390},
  
  url = {https://arxiv.org/abs/2007.14390},
  
  author = {Beutel, Daniel J. and Topal, Taner and Mathur, Akhil and Qiu, Xinchi and Fernandez-Marques, Javier and Gao, Yan and Sani, Lorenzo and Li, Kwing Hei and Parcollet, Titouan and de Gusmão, Pedro Porto Buarque and Lane, Nicholas D.},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Flower: A Friendly Federated Learning Research Framework},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{huggingface-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}

@inproceedings{pytorch,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{adamw,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Decoupled Weight Decay Regularization},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=Bkg6RiCqY7},
  timestamp    = {Thu, 25 Jul 2019 14:26:04 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LoshchilovH19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{adam,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lang2vec,
  title={Uriel and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors},
  author={Littell, Patrick and Mortensen, David R and Lin, Ke and Kairis, Katherine and Turner, Carlisle and Levin, Lori},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  volume={2},
  pages={8--14},
  year={2017}
}

@article{XGLUE,
  title={XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation},
  author={Yaobo Liang and Nan Duan and Yeyun Gong and Ning Wu and Fenfei Guo and Weizhen Qi and Ming Gong and Linjun Shou and Daxin Jiang and Guihong Cao and Xiaodong Fan and Ruofei Zhang and Rahul Agrawal and Edward Cui and Sining Wei and Taroon Bharti and Ying Qiao and Jiun-Hung Chen and Winnie Wu and Shuguang Liu and Fan Yang and Daniel Campos and Rangan Majumder and Ming Zhou},
  journal={arXiv},
  year={2020},
  volume={abs/2004.01401}
}

@inproceedings{XNLI,
  title={XNLI: Evaluating Cross-lingual Sentence Representations},
  author={Alexis Conneau and Guillaume Lample and Ruty Rinott and Adina Williams and Samuel R. Bowman and Holger Schwenk and Veselin Stoyanov},
  booktitle={EMNLP},
  year={2018}
}

@inproceedings{XLM-R,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
    abstract = "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}

@inproceedings{mBERT,
    title = "How Multilingual is Multilingual {BERT}?",
    author = "Pires, Telmo  and
      Schlinger, Eva  and
      Garrette, Dan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1493",
    doi = "10.18653/v1/P19-1493",
    pages = "4996--5001",
    abstract = "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",
}

@inbook{XLM,
author = {Conneau, Alexis and Lample, Guillaume},
title = {Cross-Lingual Language Model Pretraining},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsu-pervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models are publicly available.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {634},
numpages = {11}
}

@inproceedings{mT5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498",
    abstract = "The recent {``}Text-to-Text Transfer Transformer{''} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {``}accidental translation{''} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
}

@article{SeamlessM4T,
  title={SeamlessM4T—Massively Multilingual \& Multimodal Machine Translation},
  author={{Seamless Communication}, Lo\"{i}c Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye,  Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-juss\`{a} \footnotemark[3], Onur \,{C}elebi,Maha Elbayad,Cynthia Gao, Francisco Guzm\'an, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang},
  journal={ArXiv},
  year={2023}
}

@inproceedings{wu-dredze-2020-languages,
    title = "Are All Languages Created Equal in Multilingual {BERT}?",
    author = "Wu, Shijie  and
      Dredze, Mark",
    booktitle = "Proceedings of the 5th Workshop on Representation Learning for NLP",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.repl4nlp-1.16",
    doi = "10.18653/v1/2020.repl4nlp-1.16",
    pages = "120--130",
    abstract = "Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.",
}

@article{hu2020xtreme,
      author    = {Junjie Hu and Sebastian Ruder and Aditya Siddhant and Graham Neubig and Orhan Firat and Melvin Johnson},
      title     = {XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization},
      journal   = {CoRR},
      volume    = {abs/2003.11080},
      year      = {2020},
      archivePrefix = {arXiv},
      eprint    = {2003.11080}
}

@inproceedings{lauscher-etal-2020-zero,
    title = "From Zero to Hero: {O}n the Limitations of Zero-Shot Language Transfer with Multilingual {T}ransformers",
    author = "Lauscher, Anne  and
      Ravishankar, Vinit  and
      Vuli{\'c}, Ivan  and
      Glava{\v{s}}, Goran",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.363",
    doi = "10.18653/v1/2020.emnlp-main.363",
    pages = "4483--4499",
    abstract = "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.",
}

@inproceedings{artetxe-etal-2020-cross,
    title = "On the Cross-lingual Transferability of Monolingual Representations",
    author = "Artetxe, Mikel  and
      Ruder, Sebastian  and
      Yogatama, Dani",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.421",
    doi = "10.18653/v1/2020.acl-main.421",
    pages = "4623--4637",
    abstract = "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
}

@inproceedings{ebrahimi-etal-2022-americasnli,
    title = "{A}mericas{NLI}: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages",
    author = "Ebrahimi, Abteen  and
      Mager, Manuel  and
      Oncevay, Arturo  and
      Chaudhary, Vishrav  and
      Chiruzzo, Luis  and
      Fan, Angela  and
      Ortega, John  and
      Ramos, Ricardo  and
      Rios, Annette  and
      Meza Ruiz, Ivan Vladimir  and
      Gim{\'e}nez-Lugo, Gustavo  and
      Mager, Elisabeth  and
      Neubig, Graham  and
      Palmer, Alexis  and
      Coto-Solano, Rolando  and
      Vu, Thang  and
      Kann, Katharina",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.435",
    doi = "10.18653/v1/2022.acl-long.435",
    pages = "6279--6299",
    }


@inproceedings{mager2021findings,
  title={Findings of the AmericasNLP 2021 shared task on open machine translation for indigenous languages of the Americas},
  author={Mager, Manuel and Oncevay, Arturo and Ebrahimi, Abteen and Ortega, John and Gonzales, Annette Rios and Fan, Angela and Gutierrez-Vasques, Ximena and Chiruzzo, Luis and Gim{\'e}nez-Lugo, Gustavo and Ramos, Ricardo and others},
  booktitle={Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas},
  pages={202--217},
  year={2021}
}

@article{muhammad2022naijasenti,
  title={Naijasenti: A nigerian twitter sentiment corpus for multilingual sentiment analysis},
  author={Muhammad, Shamsuddeen Hassan and Adelani, David Ifeoluwa and Ruder, Sebastian and Ahmad, Ibrahim Said and Abdulmumin, Idris and Bello, Bello Shehu and Choudhury, Monojit and Emezue, Chris Chinenye and Abdullahi, Saheed Salahudeen and Aremu, Anuoluwapo and others},
  journal={arXiv preprint arXiv:2201.08277},
  year={2022}
}

@inproceedings{akera2022machine,
  title={Machine translation for african languages: Community creation of datasets and models in uganda},
  author={Akera, Benjamin and Mukiibi, Jonathan and Naggayi, Lydia Sanyu and Babirye, Claire and Owomugisha, Isaac and Nsumba, Solomon and Nakatumba-Nabende, Joyce and Bainomugisha, Engineer and Mwebaze, Ernest and Quinn, John},
  booktitle={3rd Workshop on African Natural Language Processing},
  year={2022}
}

@article{adelani2021masakhaner,
  title={MasakhaNER: Named entity recognition for African languages},
  author={Adelani, David Ifeoluwa and Abbott, Jade and Neubig, Graham and D’souza, Daniel and Kreutzer, Julia and Lignos, Constantine and Palen-Michel, Chester and Buzaaba, Happy and Rijhwani, Shruti and Ruder, Sebastian and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1116--1131},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@inproceedings{ansell2022composable,
  title={Composable Sparse Fine-Tuning for Cross-Lingual Transfer},
  author={Ansell, Alan and Ponti, Edoardo and Korhonen, Anna and Vuli{\'c}, Ivan},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1778--1796},
  year={2022}
}

@inproceedings{adebara-abdul-mageed-2022-towards,
    title = "Towards Afrocentric {NLP} for {A}frican Languages: Where We Are and Where We Can Go",
    author = "Adebara, Ife  and
      Abdul-Mageed, Muhammad",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.265",
    doi = "10.18653/v1/2022.acl-long.265",
    pages = "3814--3841",
    abstract = "Aligning with ACL 2022 special Theme on {``}Language Diversity: from Low Resource to Endangered Languages{''}, we discuss the major linguistic and sociopolitical challenges facing development of NLP technologies for African languages. Situating African languages in a typological framework, we discuss how the particulars of these languages can be harnessed. To facilitate future research, we also highlight current efforts, communities, venues, datasets, and tools. Our main objective is to motivate and advocate for an Afrocentric approach to technology development. With this in mind, we recommend \textit{what} technologies to build and \textit{how} to build, evaluate, and deploy them based on the needs of local African communities.",
}

@article{chen2023improving,
  title={Improving Language Plasticity via Pretraining with Active Forgetting},
  author={Chen, Yihong and Marchisio, Kelly and Raileanu, Roberta and Adelani, David Ifeoluwa and Stenetor, Pontus and Riedel, Sebastian and Artetx, Mikel},
  journal={NeurIPS 2023},
  year={2023}
}

@article{marchisio2022mini,
  title={Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training},
  author={Marchisio, Kelly and Lewis, Patrick and Chen, Yihong and Artetxe, Mikel},
  journal={ACL 2023, Findings of the Association for Computational Linguistics},
  year={2022}
}
@inproceedings{chau-etal-2020-parsing,
    title = "Parsing with Multilingual {BERT}, a Small Corpus, and a Small Treebank",
    author = "Chau, Ethan C.  and
      Lin, Lucy H.  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.118",
    doi = "10.18653/v1/2020.findings-emnlp.118",
    pages = "1324--1334",
    abstract = "Pretrained multilingual contextual representations have shown great success, but due to the limits of their pretraining data, their benefits do not apply equally to all language varieties. This presents a challenge for language varieties unfamiliar to these models, whose labeled and unlabeled data is too limited to train a monolingual model effectively. We propose the use of additional language-specific pretraining and vocabulary augmentation to adapt multilingual models to low-resource settings. Using dependency parsing of four diverse low-resource language varieties as a case study, we show that these methods significantly improve performance over baselines, especially in the lowest-resource cases, and demonstrate the importance of the relationship between such models{'} pretraining data and target language varieties.",
}

@inproceedings{pfeiffer-etal-2020-mad,
    title = "{MAD-X}: {A}n {A}dapter-{B}ased {F}ramework for {M}ulti-{T}ask {C}ross-{L}ingual {T}ransfer",
    author = "Pfeiffer, Jonas  and
      Vuli{\'c}, Ivan  and
      Gurevych, Iryna  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.617",
    doi = "10.18653/v1/2020.emnlp-main.617",
    pages = "7654--7673",
    abstract = "The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.",
}

@inproceedings{ponti-etal-2020-xcopa,
    title = "{XCOPA}: A Multilingual Dataset for Causal Commonsense Reasoning",
    author = "Ponti, Edoardo Maria  and
      Glava{\v{s}}, Goran  and
      Majewska, Olga  and
      Liu, Qianchu  and
      Vuli{\'c}, Ivan  and
      Korhonen, Anna",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.185",
    doi = "10.18653/v1/2020.emnlp-main.185",
    pages = "2362--2376",
    abstract = "In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apur{\'\i}mac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.",
}

@inproceedings{visualloss,
  title={Visualizing the Loss Landscape of Neural Nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Neural Information Processing Systems},
  year={2018}
}

@inproceedings{Izmailov2018AveragingWL,
  title={Averaging Weights Leads to Wider Optima and Better Generalization},
  author={Pavel Izmailov and Dmitrii Podoprikhin and T. Garipov and Dmitry P. Vetrov and Andrew Gordon Wilson},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:3833416}
}

@article{Keskar2016,
	author = {Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy and Ping Tak Peter Tang},
	title = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
	journal = {arXiv preprint arXiv:1609.04836},
	year = {2016}
}

@inproceedings{fedavg,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}
@article{fedprox,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={429--450},
  year={2020}
}

@article{yang2019federated,
  title={Federated machine learning: Concept and applications},
  author={Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={10},
  number={2},
  pages={1--19},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@inproceedings{learnedvector,
    title = {Learning Language Representations for Typology Prediction},
    author = {Malaviya, Chaitanya and Neubig, Graham and Littell, Patrick},
    booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    address = {Copenhagen, Denmark},
    month = {September},
    year = {2017}
}

@article{lim2020federated,
  title={Federated learning in mobile edge networks: A comprehensive survey},
  author={Lim, Wei Yang Bryan and Luong, Nguyen Cong and Hoang, Dinh Thai and Jiao, Yutao and Liang, Ying-Chang and Yang, Qiang and Niyato, Dusit and Miao, Chunyan},
  journal={IEEE Communications Surveys \& Tutorials},
  volume={22},
  number={3},
  pages={2031--2063},
  year={2020},
  publisher={IEEE}
}



@article{leaf,
  title={Leaf: A benchmark for federated settings},
  author={Caldas, Sebastian and Duddu, Sai Meher Karthik and Wu, Peter and Li, Tian and Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Smith, Virginia and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:1812.01097},
  year={2018}
}

@inproceedings{scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International conference on machine learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

@Misc{PEFT,
  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}

@inproceedings{FedKC,
author = {Wang, Haoyu and Zhao, Handong and Wang, Yaqing and Yu, Tong and Gu, Jiuxiang and Gao, Jing},
title = {FedKC: Federated Knowledge Composition for Multilingual Natural Language Understanding},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511988},
doi = {10.1145/3485447.3511988},
abstract = {Multilingual natural language understanding, which aims to comprehend multilingual documents, is an important task. Existing efforts have been focusing on the analysis of centrally stored text data, but in real practice, multilingual data is usually distributed. Federated learning is a promising paradigm to solve this problem, which trains local models with decentralized data on local clients and aggregates local models on the central server to achieve a good global model. However, existing federated learning methods assume that data are independent and identically distributed (IID), and cannot handle multilingual data, that are usually non-IID with severely skewed distributions: First, multilingual data is stored on local client devices such that there are only monolingual or bilingual data stored on each client. This makes it difficult for local models to know the information of documents in other languages. Second, the distribution over different languages could be skewed. High resource language data is much more abundant than low resource language data. The model trained on such skewed data may focus more on high resource languages but fail to consider the key information of low resource languages. To solve the aforementioned challenges of multilingual federated NLU, we propose a plug-and-play knowledge composition&nbsp;(KC) module, called FedKC, which exchanges knowledge among clients without sharing raw data. Specifically, we propose an effective way to calculate a consistency loss defined based on the shared knowledge across clients, which enables models trained on different clients achieve similar predictions on similar data. Leveraging this consistency loss, joint training is thus conducted on distributed data respecting the privacy constraints. We also analyze the potential risk of FedKC and provide theoretical bound to show that it is difficult to recover data from the corrupted data. We conduct extensive experiments on three public multilingual datasets for three typical NLU tasks, including paraphrase identification, question answering matching, and news classification. The experiment results show that the proposed FedKC can outperform state-of-the-art baselines on the three datasets significantly.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {1839–1850},
numpages = {12},
keywords = {Federated learning, Multilingual natural language understanding},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{FedNLP,
    title = "{F}ed{NLP}: Benchmarking Federated Learning Methods for Natural Language Processing Tasks",
    author = "Lin, Bill Yuchen  and
      He, Chaoyang  and
      Ze, Zihang  and
      Wang, Hulin  and
      Hua, Yufen  and
      Dupuy, Christophe  and
      Gupta, Rahul  and
      Soltanolkotabi, Mahdi  and
      Ren, Xiang  and
      Avestimehr, Salman",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.13",
    doi = "10.18653/v1/2022.findings-naacl.13",
    pages = "157--175",
    abstract = "Increasing concerns and regulations about data privacy and sparsity necessitate the study of privacy-preserving, decentralized learning methods for natural language processing (NLP) tasks. Federated learning (FL) provides promising approaches for a large number of clients (e.g., personal devices or organizations) to collaboratively learn a shared global model to benefit all clients while allowing users to keep their data locally. Despite interest in studying FL methods for NLP tasks, a systematic comparison and analysis is lacking in the literature. Herein, we present the FedNLP, a benchmarking framework for evaluating federated learning methods on four different task formulations: text classification, sequence tagging, question answering, and seq2seq. We propose a universal interface between Transformer-based language models (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) under various non-IID partitioning strategies. Our extensive experiments with FedNLP provide empirical comparisons between FL methods and help us better understand the inherent challenges of this direction. The comprehensive analysis points to intriguing and exciting future research aimed at developing FL methods for NLP tasks.",
}

@article{BANABILAH2022103061,
title = {Federated learning review: Fundamentals, enabling technologies, and future applications},
journal = {Information Processing & Management},
volume = {59},
number = {6},
pages = {103061},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103061},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322001649},
author = {Syreen Banabilah and Moayad Aloqaily and Eitaa Alsayed and Nida Malik and Yaser Jararweh},
keywords = {Federated learning, Decentralized learning, Distributed learning, Machine learning, Mobile edge networks, Data privacy, Data security},
abstract = {Federated Learning (FL) has been foundational in improving the performance of a wide range of applications since it was first introduced by Google. Some of the most prominent and commonly used FL-powered applications are Android’s Gboard for predictive text and Google Assistant. FL can be defined as a setting that makes on-device, collaborative Machine Learning possible. A wide range of literature has studied FL technical considerations, frameworks, and limitations with several works presenting a survey of the prominent literature on FL. However, prior surveys have focused on technical considerations and challenges of FL, and there has been a limitation in more recent work that presents a comprehensive overview of the status and future trends of FL in applications and markets. In this survey, we introduce the basic fundamentals of FL, describing its underlying technologies, architectures, system challenges, and privacy-preserving methods. More importantly, the contribution of this work is in scoping a wide variety of FL current applications and future trends in technology and markets today. We present a classification and clustering of literature progress in FL in application to technologies including Artificial Intelligence, Internet of Things, blockchain, Natural Language Processing, autonomous vehicles, and resource allocation, as well as in application to market use cases in domains of Data Science, healthcare, education, and industry. We discuss future open directions and challenges in FL within recommendation engines, autonomous vehicles, IoT, battery management, privacy, fairness, personalization, and the role of FL for governments and public sectors. By presenting a comprehensive review of the status and prospects of FL, this work serves as a reference point for researchers and practitioners to explore FL applications under a wide range of domains.}
}

@inproceedings{pires-etal-2019-multilingual,
    title = "How Multilingual is Multilingual {BERT}?",
    author = "Pires, Telmo  and
      Schlinger, Eva  and
      Garrette, Dan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1493",
    doi = "10.18653/v1/P19-1493",
    pages = "4996--5001",
    abstract = "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",
}

@inproceedings{wu-dredze-2019-beto,
    title = "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of {BERT}",
    author = "Wu, Shijie  and
      Dredze, Mark",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1077",
    doi = "10.18653/v1/D19-1077",
    pages = "833--844",
    abstract = "Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.",
}

@inproceedings{artetxe-etal-2020-cross,
    title = "On the Cross-lingual Transferability of Monolingual Representations",
    author = "Artetxe, Mikel  and
      Ruder, Sebastian  and
      Yogatama, Dani",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.421",
    doi = "10.18653/v1/2020.acl-main.421",
    pages = "4623--4637",
    abstract = "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
}

@inproceedings{xu-etal-2020-understanding,
    title = "Understanding Pre-trained {BERT} for Aspect-based Sentiment Analysis",
    author = "Xu, Hu  and
      Shu, Lei  and
      Yu, Philip  and
      Liu, Bing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.21",
    doi = "10.18653/v1/2020.coling-main.21",
    pages = "244--250",
    abstract = "This paper analyzes the pre-trained hidden representations learned from reviews on BERT for tasks in aspect-based sentiment analysis (ABSA). Our work is motivated by the recent progress in BERT-based language models for ABSA. However, it is not clear how the general proxy task of (masked) language model trained on unlabeled corpus without annotations of aspects or opinions can provide important features for downstream tasks in ABSA. By leveraging the annotated datasets in ABSA, we investigate both the attentions and the learned representations of BERT pre-trained on reviews. We found that BERT uses very few self-attention heads to encode context words (such as prepositions or pronouns that indicating an aspect) and opinion words for an aspect. Most features in the representation of an aspect are dedicated to the fine-grained semantics of the domain (or product category) and the aspect itself, instead of carrying summarized opinions from its context. We hope this investigation can help future research in improving self-supervised learning, unsupervised learning and fine-tuning for ABSA. The pre-trained model and code can be found at \url{https://github.com/howardhsu/BERT-for-RRC-ABSA}.",
}

@inproceedings{chung-etal-2020-improving,
    title = "Improving Multilingual Models with Language-Clustered Vocabularies",
    author = "Chung, Hyung Won  and
      Garrette, Dan  and
      Tan, Kiat Chuan  and
      Riesa, Jason",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.367",
    doi = "10.18653/v1/2020.emnlp-main.367",
    pages = "4536--4546",
    abstract = "State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications. In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies. Our experiments show improvements across languages on key multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1{\%}), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.",
}

@misc{chowdhery2022palm,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liupengfei,
  author       = {Pengfei Liu and
                  Weizhe Yuan and
                  Jinlan Fu and
                  Zhengbao Jiang and
                  Hiroaki Hayashi and
                  Graham Neubig},
  title        = {Pre-train, Prompt, and Predict: {A} Systematic Survey of Prompting
                  Methods in Natural Language Processing},
  journal      = {CoRR},
  volume       = {abs/2107.13586},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.13586},
  eprinttype    = {arXiv},
  eprint       = {2107.13586},
  timestamp    = {Tue, 03 Aug 2021 14:53:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-13586.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{schick-schutze-2022-true,
    title = "True Few-Shot Learning with {P}rompts{---}{A} Real-World Perspective",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.41",
    doi = "10.1162/tacl_a_00485",
    pages = "716--731",
    abstract = "Prompt-based approaches excel at few-shot learning. However, Perez et al. (2021) recently cast doubt on their performance as they had difficulty getting good results in a {``}true{''} few-shot setting in which prompts and hyperparameters cannot be tuned on a dev set. In view of this, we conduct an extensive study of Pet, a method that combines textual instructions with example-based finetuning. We show that, if correctly configured, Pet performs strongly in true few-shot settings without a dev set. Crucial for this strong performance is a number of design choices, including Pet{'}s ability to intelligently handle multiple prompts. We put our findings to a real-world test by running Pet on RAFT, a benchmark of tasks taken from realistic NLP applications for which no labeled dev or test sets are available. Pet achieves a new state of the art on RAFT and performs close to non-expert humans for 7 out of 11 tasks. These results demonstrate that prompt-based learners can successfully be applied in true few-shot settings and underpin our belief that learning from instructions will play an important role on the path towards human-like few-shot learning capabilities.",
}

@inproceedings{autoprompt,
  author = {Taylor Shin and Yasaman Razeghi and Robert L. Logan IV and Eric Wallace and Sameer Singh},
  title = { {AutoPrompt}: Eliciting Knowledge from Language Models with Automatically Generated Prompts },
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2020}
}

%--------------------vFedSec:

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2018}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


@INPROCEEDINGS{9634881,
  author={Zhang, Junpeng and Li, Mengqian and Zeng, Shuiguang and Xie, Bin and Zhao, Dongmei},
  booktitle={2021 International Conference on Networking and Network Applications (NaNA)}, 
  title={A survey on security and privacy threats to federated learning}, 
  year={2021},
  volume={},
  number={},
  pages={319-326},
  doi={10.1109/NaNA53684.2021.00062}
  }

  @article{konevcny2016federated1,
  title={Federated optimization: Distributed machine learning for on-device intelligence},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Ramage, Daniel and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1610.02527},
  year={2016}
}

@misc{phe,
  author = {CSIRO's Data61},
  title = {Python Paillier Library},
  year = {2013},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/data61/python-paillier}},
}

@misc{sealpy,
  author = {Huelse},
  title = {Microsoft SEAL For Python},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/Huelse/SEAL-Python}},
}

@misc{pybind11,
   author = {Wenzel Jakob and Jason Rhinelander and Dean Moldovan},
   year = {2017},
   note = {https://github.com/pybind/pybind11},
   title = {pybind11 -- Seamless operability between C++11 and Python}
}

@misc{sealcrypto,
    title = {{M}icrosoft {SEAL} (release 4.1)},
    howpublished = {\url{https://github.com/Microsoft/SEAL}},
    month = jan,
    year = 2023,
    note = {Microsoft Research, Redmond, WA.},
    key = {SEAL}
}

@article{zhou2021psi,
  title={Privacy-preserving federated learning framework with general aggregation and multiparty entity matching},
  author={Zhou, Zhou and Tian, Youliang and Peng, Changgen},
  journal={Wireless Communications and Mobile Computing},
  volume={2021},
  pages={1--14},
  year={2021},
  publisher={Hindawi Limited}
}
@inproceedings{lu2020psi,
  title={Multi-party private set intersection in vertical federated learning},
  author={Lu, Linpeng and Ding, Ning},
  booktitle={2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)},
  pages={707--714},
  year={2020},
  organization={IEEE}
}

@article{konevcny2016federated2,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@misc{tf_experiment,
    author = {Tensorflow Federated},
    title = {Federated Learning with Differential Privacy},
    year = {2022},
    howpublished = {\url{https://www.tensorflow.org/federated/tutorials/federated_learning_with_differential_privacy}}
}%   note = "accessed 25-Augr-22"

@inproceedings{carlini2019secret,
  title={The secret sharer: Evaluating and testing unintended memorization in neural networks},
  author={Carlini, Nicholas and Liu, Chang and Erlingsson, {\'U}lfar and Kos, Jernej and Song, Dawn},
  booktitle={28th USENIX Security Symposium (USENIX Security 19)},
  pages={267--284},
  year={2019}
}

@article{kasiviswanathan2011can,
  title={What can we learn privately?},
  author={Kasiviswanathan, Shiva Prasad and Lee, Homin K and Nissim, Kobbi and Raskhodnikova, Sofya and Smith, Adam},
  journal={SIAM Journal on Computing},
  volume={40},
  number={3},
  pages={793--826},
  year={2011},
  publisher={SIAM}
}

@inproceedings{kairouz2016discrete,
  title={Discrete distribution estimation under local privacy},
  author={Kairouz, Peter and Bonawitz, Keith and Ramage, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={2436--2444},
  year={2016},
  organization={PMLR}
}


@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{abadi2016tensorflow,
  title={$\{$TensorFlow$\}$: A System for $\{$Large-Scale$\}$ Machine Learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th USENIX symposium on operating systems design and implementation (OSDI 16)},
  pages={265--283},
  year={2016}
}

@article{beutel2020flower,
  title={Flower: A friendly federated learning research framework},
  author={Beutel, Daniel J and Topal, Taner and Mathur, Akhil and Qiu, Xinchi and Parcollet, Titouan and de Gusm{\~a}o, Pedro PB and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2007.14390},
  year={2020}
}

@article{li2019fair,
  title={Fair resource allocation in federated learning},
  author={Li, Tian and Sanjabi, Maziar and Beirami, Ahmad and Smith, Virginia},
  journal={arXiv preprint arXiv:1905.10497},
  year={2019}
}
@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE symposium on security and privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}
@inproceedings{fredrikson2015model,
  title={Model inversion attacks that exploit confidence information and basic countermeasures},
  author={Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
  booktitle={Proceedings of the 22nd ACM SIGSAC conference on computer and communications security},
  pages={1322--1333},
  year={2015}
}

@inproceedings{melis2019exploiting,
  title={Exploiting unintended feature leakage in collaborative learning},
  author={Melis, Luca and Song, Congzheng and De Cristofaro, Emiliano and Shmatikov, Vitaly},
  booktitle={2019 IEEE symposium on security and privacy (SP)},
  pages={691--706},
  year={2019},
  organization={IEEE}
}

@article{mcmahan2017learning,
  title={Learning differentially private recurrent language models},
  author={McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  journal={arXiv preprint arXiv:1710.06963},
  year={2017}
}

@article{andrew2021differentially,
  title={Differentially private learning with adaptive clipping},
  author={Andrew, Galen and Thakkar, Om and McMahan, Brendan and Ramaswamy, Swaroop},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{nilsonreport,
 author = {Ullmann, Julian R.},
 title = {Bit-vector Algorithms for Binary Constraint Satisfaction and Subgraph Isomorphism},
 journal = {J. Exp. Algorithmics},
 issue_date = {2010},
 volume = {15},
 month = feb,
 year = {2011},
 issn = {1084-6654},
 pages = {1.6:1.1--1.6:1.64},
 articleno = {1.6},
 numpages = {1.54},
 url = {http://doi.acm.org/10.1145/1671970.1921702},
 doi = {10.1145/1671970.1921702},
 acmid = {1921702},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AllDifferent constraint, backtrack, binary constraints, bit-vector, constraint propagation, constraint satisfaction, domain reduction, focus search, forward checking, graph indexing, molecule matching, prematching, signature file, subgraph isomorphism},
}
@misc{ukfinance,
  title = {Fraud- The Facts 2021:
The Definite Overview of Payment Industry Fraud},
  author = {UK Finance},
  note  =  {\url{https://www.ukfinance.org.uk/system/files/Fraud%20The%20Facts%202021-%20FINAL.pdf}},
  note = {Accessed: 14-Sep-2021}
}
@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}
@article{pang2021deep,
  title={Deep learning for anomaly detection: A review},
  author={Pang, Guansong and Shen, Chunhua and Cao, Longbing and Hengel, Anton Van Den},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={2},
  pages={1--38},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@article{chandola2009anomaly,
  title={Anomaly detection: A survey},
  author={Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  journal={ACM computing surveys (CSUR)},
  volume={41},
  number={3},
  pages={1--58},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@software{np_galois,
   title = {{Galois: A performant NumPy extension for Galois fields}},
   author = {Hostetter, Matt},
   month = {11},
   year = {2020},
   url = {https://github.com/mhostetter/galois},
}

@article{he2020fedml,
  title={Fedml: A research library and benchmark for federated machine learning},
  author={He, Chaoyang and Li, Songze and So, Jinhyun and Zeng, Xiao and Zhang, Mi and Wang, Hongyi and Wang, Xiaoyang and Vepakomma, Praneeth and Singh, Abhishek and Qiu, Hang and others},
  journal={arXiv preprint arXiv:2007.13518},
  year={2020}
}

@inproceedings{cld_cmp_privacy,
  title={Security and privacy in cloud computing: A survey},
  author={Zhou, Minqi and Zhang, Rong and Xie, Wei and Qian, Weining and Zhou, Aoying},
  booktitle={2010 Sixth International Conference on Semantics, Knowledge and Grids},
  pages={105--112},
  year={2010},
  organization={IEEE}
}
@inproceedings{bonawitz2017secagg,
  title={Practical secure aggregation for privacy-preserving machine learning},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1175--1191},
  year={2017}
}
@inproceedings{bell2020secaggplus,
  title={Secure single-server aggregation with (poly) logarithmic overhead},
  author={Bell, James Henry and Bonawitz, Kallista A and Gasc{\'o}n, Adri{\`a} and Lepoint, Tancr{\`e}de and Raykova, Mariana},
  booktitle={Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1253--1269},
  year={2020}
}

@article{shamir1979share,
  title={How to share a secret},
  author={Shamir, Adi},
  journal={Communications of the ACM},
  volume={22},
  number={11},
  pages={612--613},
  year={1979},
  publisher={ACm New York, NY, USA}
}

@inproceedings{fedavg,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{mothukuri2021survey,
  title={A survey on security and privacy of federated learning},
  author={Mothukuri, Viraaji and Parizi, Reza M and Pouriyeh, Seyedamin and Huang, Yan and Dehghantanha, Ali and Srivastava, Gautam},
  journal={Future Generation Computer Systems},
  volume={115},
  pages={619--640},
  year={2021},
  publisher={Elsevier}
}

@article{papernot2016ml_privacy,
  title={Towards the science of security and privacy in machine learning},
  author={Papernot, Nicolas and McDaniel, Patrick and Sinha, Arunesh and Wellman, Michael},
  journal={arXiv preprint arXiv:1611.03814},
  year={2016}
}

@article{liu2021ml_privacy,
  title={When machine learning meets privacy: A survey and outlook},
  author={Liu, Bo and Ding, Ming and Shaham, Sina and Rahayu, Wenny and Farokhi, Farhad and Lin, Zihuai},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={2},
  pages={1--36},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@inproceedings{fl1,
  title={Privacy-preserving deep learning},
  author={Shokri, Reza and Shmatikov, Vitaly},
  booktitle={Proceedings of the 22nd ACM SIGSAC conference on computer and communications security},
  pages={1310--1321},
  year={2015}
}

@misc{AES,
  author = {Morris Dworkin and Elaine Barker and James Nechvatal and James Foti and Lawrence Bassham and E. Roback and James Dray},
  title = {Advanced Encryption Standard (AES)},
  year = {2001},
  month = {2001-11-26},
  publisher = {Federal Inf. Process. Stds. (NIST FIPS), National Institute of Standards and Technology, Gaithersburg, MD},
  doi = {https://doi.org/10.6028/NIST.FIPS.197},
  language = {en},
}
@Article{numpy,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@online{keras,
  title={Keras},
  author={Chollet, Francois and others},
  year={2015},
  publisher={GitHub},
  url={https://github.com/fchollet/keras},
}
@article{fedprox,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={429--450},
  year={2020}
}
@article{fedoptim,
  title={Adaptive federated optimization},
  author={Reddi, Sashank and Charles, Zachary and Zaheer, Manzil and Garrett, Zachary and Rush, Keith and Kone{\v{c}}n{\`y}, Jakub and Kumar, Sanjiv and McMahan, H Brendan},
  journal={arXiv preprint arXiv:2003.00295},
  year={2020}
}
@inproceedings{salvia,
  title={Secure aggregation for federated learning in flower},
  author={Li, Kwing Hei and de Gusm{\~a}o, Pedro Porto Buarque and Beutel, Daniel J and Lane, Nicholas D},
  booktitle={Proceedings of the 2nd ACM International Workshop on Distributed Machine Learning},
  pages={8--14},
  year={2021}
}
@article{kadhe2020fastsecagg,
  title={Fastsecagg: Scalable secure aggregation for privacy-preserving federated learning},
  author={Kadhe, Swanand and Rajaraman, Nived and Koyluoglu, O Ozan and Ramchandran, Kannan},
  journal={arXiv preprint arXiv:2009.11248},
  year={2020}
}

@article{vfl,
  title={Vertical Federated Learning}, 
  author={Yang Liu and Yan Kang and Tianyuan Zou and Yanhong Pu and Yuanqin He and Xiaozhou Ye and Ye Ouyang and Ya-Qin Zhang and Qiang Yang},
  year={2022},
  eprint={2211.12814},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}


@misc{pycrypto,
   author = {{Python Cryptographic Authority}},
   year = {2021},
   note = {https://github.com/pyca/cryptography},
   title = {Cryptography (version 3.4.8)}
} 

@techreport{ecdh,
  title={Recommendation for pair-wise key-establishment schemes using discrete logarithm cryptography},
  author={Barker, Elaine and Chen, Lily and Keller, Sharon and Roginsky, Allen and Vassilev, Apostol and Davis, Richard},
  year={2017},
  institution={National Institute of Standards and Technology}
}

@article{diffie-hellman,
  title={New directions in cryptography},
  author={Diffie, Whitfield and Hellman, Martin},
  journal={IEEE transactions on Information Theory},
  volume={22},
  number={6},
  pages={644--654},
  year={1976},
  publisher={IEEE}
}

@inproceedings{LCC,
  title={Lagrange coded computing: Optimal design for resiliency, security, and privacy},
  author={Yu, Qian and Li, Songze and Raviv, Netanel and Kalan, Seyed Mohammadreza Mousavi and Soltanolkotabi, Mahdi and Avestimehr, Salman A},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1215--1225},
  year={2019},
  organization={PMLR}
}

@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE symposium on security and privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}

@inproceedings{wang2019user_lvl,
  title={Beyond inferring class representatives: User-level privacy leakage from federated learning},
  author={Wang, Zhibo and Song, Mengkai and Zhang, Zhifei and Song, Yang and Wang, Qian and Qi, Hairong},
  booktitle={IEEE INFOCOM 2019-IEEE Conference on Computer Communications},
  pages={2512--2520},
  year={2019},
  organization={IEEE}
}

@article{wei2020dp_fl,
  title={Federated learning with differential privacy: Algorithms and performance analysis},
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H and Farokhi, Farhad and Jin, Shi and Quek, Tony QS and Poor, H Vincent},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={15},
  pages={3454--3469},
  year={2020},
  publisher={IEEE}
}

@inproceedings{sabt2015TEE,
  title={Trusted execution environment: what it is, and what it is not},
  author={Sabt, Mohamed and Achemlal, Mohammed and Bouabdallah, Abdelmadjid},
  booktitle={2015 IEEE Trustcom/BigDataSE/ISPA},
  volume={1},
  pages={57--64},
  year={2015},
  organization={IEEE}
}

@article{pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{tensorflow,
  title={$\{$TensorFlow$\}$: A System for $\{$Large-Scale$\}$ Machine Learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th USENIX symposium on operating systems design and implementation (OSDI 16)},
  pages={265--283},
  year={2016}
}

@article{knott2021crypten,
  title={Crypten: Secure multi-party computation meets machine learning},
  author={Knott, Brian and Venkataraman, Shobha and Hannun, Awni and Sengupta, Shubho and Ibrahim, Mark and van der Maaten, Laurens},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{ibmfl,
title={IBM Federated Learning: an Enterprise Framework White Paper V0. 1},
author={Ludwig, Heiko and Baracaldo, Nathalie and Thomas, Gegi and Zhou, Yi and Anwar, Ali and Rajamoni, Shashank and Ong, Yuya and Radhakrishnan, Jayaram and Verma, Ashish and Sinn, Mathieu and others},
journal={arXiv preprint arXiv:2007.10987},
year={2020}
}

@incollection{pysyft,
  title={Pysyft: A library for easy federated learning},
  author={Ziller, Alexander and Trask, Andrew and Lopardo, Antonio and Szymkow, Benjamin and Wagner, Bobby and Bluemke, Emma and Nounahon, Jean-Mickael and Passerat-Palmbach, Jonathan and Prakash, Kritika and Rose, Nick and others},
  booktitle={Federated Learning Systems},
  pages={111--139},
  year={2021},
  publisher={Springer}
}

@article{openfl,
  title={OpenFL: An open-source framework for Federated Learning},
  author={Reina, G Anthony and Gruzdev, Alexey and Foley, Patrick and Perepelkina, Olga and Sharma, Mansi and Davidyuk, Igor and Trushkin, Ilya and Radionov, Maksim and Mokrov, Aleksandr and Agapov, Dmitry and others},
  journal={arXiv preprint arXiv:2105.06413},
  year={2021}
}

@misc{tffl,
  title={TensorFlow federated: machine learning on decentralized data.},
  author={Bonawitz, K and Eichner, H and Grieskamp, W and others},
  year={2020}
}

@inproceedings{homo_en,
  title={Multiparty computation from somewhat homomorphic encryption},
  author={Damg{\aa}rd, Ivan and Pastro, Valerio and Smart, Nigel and Zakarias, Sarah},
  booktitle={Annual Cryptology Conference},
  pages={643--662},
  year={2012},
  organization={Springer}
}

@misc{flower,
  doi = {10.48550/ARXIV.2007.14390},
  
  url = {https://arxiv.org/abs/2007.14390},
  
  author = {Beutel, Daniel J. and Topal, Taner and Mathur, Akhil and Qiu, Xinchi and Fernandez-Marques, Javier and Gao, Yan and Sani, Lorenzo and Li, Kwing Hei and Parcollet, Titouan and de Gusmão, Pedro Porto Buarque and Lane, Nicholas D.},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Flower: A Friendly Federated Learning Research Framework},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{dwork_dp_survey,
  title={Differential privacy: A survey of results},
  author={Dwork, Cynthia},
  booktitle={International conference on theory and applications of models of computation},
  pages={1--19},
  year={2008},
  organization={Springer}
}

@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy.},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Found. Trends Theor. Comput. Sci.},
  volume={9},
  number={3-4},
  pages={211--407},
  year={2014}
}

@inproceedings{abadi_dp_dl,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@article{wei_dp_fl,
  title={Federated learning with differential privacy: Algorithms and performance analysis},
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H and Farokhi, Farhad and Jin, Shi and Quek, Tony QS and Poor, H Vincent},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={15},
  pages={3454--3469},
  year={2020},
  publisher={IEEE}
}

@inproceedings{truex_hb,
  title={A hybrid approach to privacy-preserving federated learning},
  author={Truex, Stacey and Baracaldo, Nathalie and Anwar, Ali and Steinke, Thomas and Ludwig, Heiko and Zhang, Rui and Zhou, Yi},
  booktitle={Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security},
  pages={1--11},
  year={2019}
}

@inproceedings{paillier1999public,
  title={Public-key cryptosystems based on composite degree residuosity classes},
  author={Paillier, Pascal},
  booktitle={International conference on the theory and applications of cryptographic techniques},
  pages={223--238},
  year={1999},
  organization={Springer}
}

@inproceedings{fedscale,
  title={FedScale: Benchmarking model and system performance of federated learning},
  author={Lai, Fan and Dai, Yinwei and Zhu, Xiangfeng and Madhyastha, Harsha V and Chowdhury, Mosharaf},
  booktitle={Proceedings of the First Workshop on Systems Challenges in Reliable and Secure Federated Learning},
  pages={1--3},
  year={2021}
}

@article{mathur2021device,
  title={On-device federated learning with flower},
  author={Mathur, Akhil and Beutel, Daniel J and de Gusmao, Pedro Porto Buarque and Fernandez-Marques, Javier and Topal, Taner and Qiu, Xinchi and Parcollet, Titouan and Gao, Yan and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2104.03042},
  year={2021}
}

@article{li2021fedbn,
  title={Fedbn: Federated learning on non-iid features via local batch normalization},
  author={Li, Xiaoxiao and Jiang, Meirui and Zhang, Xiaofei and Kamp, Michael and Dou, Qi},
  journal={arXiv preprint arXiv:2102.07623},
  year={2021}
}

@article{horvath2021fjord,
  title={Fjord: Fair and accurate federated learning under heterogeneous targets with ordered dropout},
  author={Horvath, Samuel and Laskaridis, Stefanos and Almeida, Mario and Leontiadis, Ilias and Venieris, Stylianos and Lane, Nicholas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{nasr2019comprehensive,
  title={Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning},
  author={Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
  booktitle={2019 IEEE symposium on security and privacy (SP)},
  pages={739--753},
  year={2019},
  organization={IEEE}
}
@inproceedings{10.1145/3488659.3493776,
author = {Li, Kwing Hei and de Gusm\~{a}o, Pedro Porto Buarque and Beutel, Daniel J. and Lane, Nicholas D.},
title = {Secure Aggregation for Federated Learning in Flower},
year = {2021},
isbn = {9781450391344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488659.3493776},
doi = {10.1145/3488659.3493776},
abstract = {Federated Learning (FL) allows parties to learn a shared prediction model by delegating the training computation to clients and aggregating all the separately trained models on the server. To prevent private information being inferred from local models, Secure Aggregation (SA) protocols are used to ensure that the server is unable to inspect individual trained models as it aggregates them. However, current implementations of SA in FL frameworks have limitations, including vulnerability to client dropouts or configuration difficulties.In this paper, we present Salvia, an implementation of SA for Python users in the Flower FL framework. Based on the SecAgg(+) protocols for a semi-honest threat model, Salvia is robust against client dropouts and exposes a flexible and easy-to-use API that is compatible with various machine learning frameworks. We show that Salvia's experimental performance is consistent with SecAgg(+)'s theoretical computation and communication complexities.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Distributed Machine Learning},
pages = {8–14},
numpages = {7},
keywords = {federated learning, secure aggregation, secure multi-party computation},
location = {Virtual Event, Germany},
series = {DistributedML '21}
}

@article{yasargao2022eccv,
  title={Federated Self-supervised Learning for Video Understanding},
  author={ Abbas ur Rehman, Yasar and  Gao, Yan and Shen, Jiajun and  Porto Buarque de Gusm{\~a}o, Pedro and  Lane, Nicholas D },
  journal={European Conference on Computer Vision (ECCV)},
  year={2022}
}
@inproceedings{gao2021end,
  title={End-to-end speech recognition from federated acoustic models}, 
  author={Gao, Yan and Parcollet, Titouan and Zaiem, Salah and Fernandez-Marques, Javier and Porto Buarque de Gusmão, Pedro and Beutel, Daniel J and Lane, Nicholas D},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2022},
  volume={},
  number={},
}
@inproceedings{qiu2021zerofl,
  title={ZeroFL: Efficient On-Device Training for Federated Learning with Local Sparsity},
  author={Qiu, Xinchi and Fernandez-Marques, Javier and Porto Buarque de Gusmão, Pedro and Gao, Yan and Parcollet, Titouan and Lane, Nicholas Donald},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2022}
}
@article{so2022lightsecagg,
  title={Lightsecagg: a lightweight and versatile design for secure aggregation in federated learning},
  author={So, Jinhyun and Nolet, Corey J and Yang, Chien-Sheng and Li, Songze and Yu, Qian and E Ali, Ramy and Guler, Basak and Avestimehr, Salman},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={694--720},
  year={2022}
}



@article{wei2022vertical,
  title={Vertical federated learning: Challenges, methodologies and experiments},
  author={Wei, Kang and Li, Jun and Ma, Chuan and Ding, Ming and Wei, Sha and Wu, Fan and Chen, Guihai and Ranbaduge, Thilina},
  journal={arXiv preprint arXiv:2202.04309},
  year={2022}
}

@article{yang2019federated,
  title={Federated machine learning: Concept and applications},
  author={Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={10},
  number={2},
  pages={1--19},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{yu2021toward,
  title={Toward resource-efficient federated learning in mobile edge computing},
  author={Yu, Rong and Li, Peichun},
  journal={IEEE Network},
  volume={35},
  number={1},
  pages={148--155},
  year={2021},
  publisher={IEEE}
}

@article{secagg,
  title={Practical secure aggregation for federated learning on user-held data},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  journal={arXiv preprint arXiv:1611.04482},
  year={2016}
}

@article{dlg,
  title={Deep leakage from gradients},
  author={Zhu, Ligeng and Liu, Zhijian and Han, Song},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{idlg,
  title={idlg: Improved deep leakage from gradients},
  author={Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan},
  journal={arXiv preprint arXiv:2001.02610},
  year={2020}
}

@inproceedings{fu2021vf2boost,
  title={Vf2boost: Very fast vertical federated gradient boosting for cross-enterprise learning},
  author={Fu, Fangcheng and Shao, Yingxia and Yu, Lele and Jiang, Jiawei and Xue, Huanran and Tao, Yangyu and Cui, Bin},
  booktitle={Proceedings of the 2021 International Conference on Management of Data},
  pages={563--576},
  year={2021}
}

@article{jin2021cafe,
  title={CAFE: Catastrophic data leakage in vertical federated learning},
  author={Jin, Xiao and Chen, Pin-Yu and Hsu, Chia-Yi and Yu, Chia-Mu and Chen, Tianyi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={994--1006},
  year={2021}
}


@article{liu2020asymmetrical,
  title={Asymmetrical vertical federated learning},
  author={Liu, Yang and Zhang, Xiong and Wang, Libin},
  journal={arXiv preprint arXiv:2004.07427},
  year={2020}
}

@article{li2021survey,
  title={A survey on federated learning systems: vision, hype and reality for data privacy and protection},
  author={Li, Qinbin and Wen, Zeyi and Wu, Zhaomin and Hu, Sixu and Wang, Naibo and Li, Yuan and Liu, Xu and He, Bingsheng},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2021},
  publisher={IEEE}
}

@article{chen2020vafl,
  title={Vafl: a method of vertical asynchronous federated learning},
  author={Chen, Tianyi and Jin, Xiao and Sun, Yuejiao and Yin, Wotao},
  journal={arXiv preprint arXiv:2007.06081},
  year={2020}
}

@article{xgboost,
  title={Xgboost: extreme gradient boosting},
  author={Chen, Tianqi and He, Tong and Benesty, Michael and Khotilovich, Vadim and Tang, Yuan and Cho, Hyunsu and Chen, Kailong and Mitchell, Rory and Cano, Ignacio and Zhou, Tianyi and others},
  journal={R package version 0.4-2},
  volume={1},
  number={4},
  pages={1--4},
  year={2015}
}

\textbf{@inproceedings{taobao,
  title={A Novel CTR Prediction Model Based On DeepFM For Taobao Data},
  author={Li, LinShu and Hong, Jianbo and Min, Sitao and Xue, Yunfan},
  booktitle={2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID)},
  pages={184--187},
  year={2021},
  organization={IEEE}
}}

@article{bankdataset,
  title={Using data mining for bank direct marketing: An application of the crisp-dm methodology},
  author={Moro, Sergio and Laureano, Raul and Cortez, Paulo},
  year={2011},
  publisher={EUROSIS-ETI}
}

@inproceedings{fu2022blindfl,
  title={Blindfl: Vertical federated machine learning without peeking into your data},
  author={Fu, Fangcheng and Xue, Huanran and Cheng, Yong and Tao, Yangyu and Cui, Bin},
  booktitle={Proceedings of the 2022 International Conference on Management of Data},
  pages={1316--1330},
  year={2022}
}
@article{zhang2020acml,
  title={Additively homomorphical encryption based deep neural network for asymmetrically collaborative machine learning},
  author={Zhang, Yifei and Zhu, Hao},
  journal={arXiv preprint arXiv:2007.06849},
  year={2020}
}
@article{kang2022prada,
  title={Privacy-preserving federated adversarial domain adaptation over feature groups for interpretability},
  author={Kang, Yan and He, Yuanqin and Luo, Jiahuan and Fan, Tao and Liu, Yang and Yang, Qiang},
  journal={IEEE Transactions on Big Data},
  year={2022},
  publisher={IEEE}
}
@article{cai2022sfa,
  title={Secure Forward Aggregation for Vertical Federated Neural Networks},
  author={Cai, Shuowei and Chai, Di and Yang, Liu and Zhang, Junxue and Jin, Yilun and Wang, Leye and Guo, Kun and Chen, Kai},
  journal={arXiv preprint arXiv:2207.00165},
  year={2022}
}
@inproceedings{adultincome,
  title={Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid.},
  author={Kohavi, Ron and others},
  booktitle={Kdd},
  volume={96},
  pages={202--207},
  year={1996}
}

@article{mimic-benchmark,
  author={Harutyunyan, Hrayr and Khachatrian, Hrant and Kale, David C. and Ver Steeg, Greg and Galstyan, Aram},
  title={Multitask learning and benchmarking with clinical time series data},
  journal={Scientific Data},
  year={2019},
  volume={6},
  number={1},
  pages={96},
  issn={2052-4463},
  doi={10.1038/s41597-019-0103-9},
  url={https://doi.org/10.1038/s41597-019-0103-9}
}

@article{mimic3,
author = {Johnson, Alistair and Pollard, Tom and Shen, Lu and Lehman, Li-wei and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Celi, Leo and Mark, Roger},
year = {2016},
month = {05},
pages = {160035},
title = {MIMIC-III, a freely accessible critical care database},
volume = {3},
journal = {Scientific Data},
doi = {10.1038/sdata.2016.35}
}

@inproceedings{yin2021see,
  title={See through gradients: Image batch recovery via gradinversion},
  author={Yin, Hongxu and Mallya, Arun and Vahdat, Arash and Alvarez, Jose M and Kautz, Jan and Molchanov, Pavlo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16337--16346},
  year={2021}
}

@misc{dpvfl,
      title={Differentially Private Vertical Federated Learning}, 
      author={Thilina Ranbaduge and Ming Ding},
      year={2022},
      eprint={2211.06782},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tradeoffvfl,
      title={A Framework for Evaluating Privacy-Utility Trade-off in Vertical Federated Learning}, 
      author={Yan Kang and Jiahuan Luo and Yuanqin He and Xiaojin Zhang and Lixin Fan and Qiang Yang},
      year={2022},
      eprint={2209.03885},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{cai2023secure,
  title={Secure forward aggregation for vertical federated neural networks},
  author={Cai, Shuowei and Chai, Di and Yang, Liu and Zhang, Junxue and Jin, Yilun and Wang, Leye and Guo, Kun and Chen, Kai},
  booktitle={Trustworthy Federated Learning: First International Workshop, FL 2022, Held in Conjunction with IJCAI 2022, Vienna, Austria, July 23, 2022, Revised Selected Papers},
  pages={115--129},
  year={2023},
  organization={Springer}
}


@article{zheng2022making,
  title={Making Split Learning Resilient to Label Leakage by Potential Energy Loss},
  author={Zheng, Fei and Chen, Chaochao and Yao, Binhui and Zheng, Xiaolin},
  journal={arXiv preprint arXiv:2210.09617},
  year={2022}
}

@misc{sun2022label,
      title={Label Leakage and Protection from Forward Embedding in Vertical Federated Learning}, 
      author={Jiankai Sun and Xin Yang and Yuanshun Yao and Chong Wang},
      year={2022},
      eprint={2203.01451},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{jiang2022vfps,
title={{VF}-{PS}: How to Select Important Participants in Vertical Federated Learning, Efficiently and Securely?},
author={Jiawei Jiang and Lukas Burkhalter and Fangcheng Fu and Bolin Ding and Bo Du and Anwar Hithnawi and Bo Li and Ce Zhang},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=vNrSXIFJ9wz}
}



@inproceedings{Fang_2021,
  title={Large-scale secure XGB for vertical federated learning},
  author={Fang, Wenjing and Zhao, Derun and Tan, Jin and Chen, Chaochao and Yu, Chaofan and Wang, Li and Wang, Lei and Zhou, Jun and Zhang, Benyu},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={443--452},
  year={2021}
}


@article{gu2021privacy,
  title={Privacy-preserving asynchronous vertical federated learning algorithms for multiparty collaborative learning},
  author={Gu, Bin and Xu, An and Huo, Zhouyuan and Deng, Cheng and Huang, Heng},
  journal={IEEE transactions on neural networks and learning systems},
  volume={33},
  number={11},
  pages={6103--6115},
  year={2021},
  publisher={IEEE}
}

@inproceedings{shi2022,
author = {Yan, Yang and Yang, Guozheng and Gao, Yan and Zang, Cheng and Chen, Jiajun and Wang, Qiang},
title = {Multi-Participant Vertical Federated Learning Based Time Series Prediction},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532238},
doi = {10.1145/3532213.3532238},
abstract = {Federated learning (FL) ensures multi-party can train a model together while avoiding privacy leakage. Our vertical federated learning (VFL) task tackles the following scenarios: i) all parties share the same sample space but differ in feature space, ii) only one party holds the label data. Our contribution is twofold: i) proposing a novel aggregation strategy to show that embedding learning is qualified to handle the challenge of VFL, ii) Incorporating specific strategy of Secure Multi-party Computation (MPC) into the training phase to remain the dataset at each local machine. We focus on time series scenarios and choose Gated Recurrent Unit (GRU) as our basic algorithm. We evaluate our method on both Google stock data for regression prediction and Kyoto University Benchmark Data for classification prediction to illustrate the performance of the results in terms of computational and communication complexities.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {165–171},
numpages = {7},
keywords = {time series, federated learning, privacy, vertical federated learning, secure multi-party computation, ensemble learning},
location = {Tianjin, China},
series = {ICCAI '22}
}

@misc{li2023fedvs,
      title={FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models}, 
      author={Songze Li and Duanyi Yao and Jin Liu},
      year={2023},
      eprint={2304.13407},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{cohen2017emnist,
  title={EMNIST: Extending MNIST to handwritten letters},
  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
  booktitle={2017 international joint conference on neural networks (IJCNN)},
  pages={2921--2926},
  year={2017},
  organization={IEEE}
}

@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

%-----------------latent representation:

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{mixmatch,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{semifl,
  title={SemiFL: Semi-supervised federated learning for unlabeled clients with alternate training},
  author={Diao, Enmao and Ding, Jie and Tarokh, Vahid},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17871--17884},
  year={2022}
}

@article{fixmatch,
  title={Fixmatch: Simplifying semi-supervised learning with consistency and confidence},
  author={Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={596--608},
  year={2020}
}

@article{fedmatch,
  title={Federated semi-supervised learning with inter-client consistency \& disjoint learning},
  author={Jeong, Wonyong and Yoon, Jaehong and Yang, Eunho and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:2006.12097},
  year={2020}
}

@inproceedings{fedavg,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  year={2017},
  organization={PMLR}
}


@article{jin2020towards,
  title={Towards utilizing unlabeled data in federated learning: A survey and prospective},
  author={Jin, Yilun and Wei, Xiguang and Liu, Yang and Yang, Qiang},
  journal={arXiv preprint arXiv:2002.11545},
  year={2020}
}

@inproceedings{zhang2021improving,
  title={Improving semi-supervised federated learning by reducing the gradient diversity of models},
  author={Zhang, Zhengming and Yang, Yaoqing and Yao, Zhewei and Yan, Yujun and Gonzalez, Joseph E and Ramchandran, Kannan and Mahoney, Michael W},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)},
  pages={1214--1225},
  year={2021},
  organization={IEEE}
}

@article{xie2020unsupervised,
  title={Unsupervised data augmentation for consistency training},
  author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6256--6268},
  year={2020}
}

@article{yang2021federated,
  title={Federated semi-supervised learning for COVID region segmentation in chest CT using multi-national data from China, Italy, Japan},
  author={Yang, Dong and Xu, Ziyue and Li, Wenqi and Myronenko, Andriy and Roth, Holger R and Harmon, Stephanie and Xu, Sheng and Turkbey, Baris and Turkbey, Evrim and Wang, Xiaosong and others},
  journal={Medical image analysis},
  volume={70},
  pages={101992},
  year={2021},
  publisher={Elsevier}
}

@article{zhou2005tri,
  title={Tri-training: Exploiting unlabeled data using three classifiers},
  author={Zhou, Zhi-Hua and Li, Ming},
  journal={IEEE Transactions on knowledge and Data Engineering},
  volume={17},
  number={11},
  pages={1529--1541},
  year={2005},
  publisher={IEEE}
}

@article{rasmus2015semi,
  title={Semi-supervised learning with ladder networks},
  author={Rasmus, Antti and Berglund, Mathias and Honkala, Mikko and Valpola, Harri and Raiko, Tapani},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{jeong2020federated,
  title={Federated semi-supervised learning with inter-client consistency \& disjoint learning},
  author={Jeong, Wonyong and Yoon, Jaehong and Yang, Eunho and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:2006.12097},
  year={2020}
}

@article{gao2022federated,
  title={Federated self-supervised speech representations: Are we there yet?},
  author={Gao, Yan and Fernandez-Marques, Javier and Parcollet, Titouan and Mehrotra, Abhinav and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2204.02804},
  year={2022}
}

@inproceedings{lee2013pseudo,
  title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
  author={Lee, Dong-Hyun and others},
  booktitle={Workshop on challenges in representation learning, ICML},
  volume={3},
  number={2},
  pages={896},
  year={2013},
  organization={Atlanta}
}

@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={702--703},
  year={2020}
}

@article{thulasidasan2019mixup,
  title={On mixup training: Improved calibration and predictive uncertainty for deep neural networks},
  author={Thulasidasan, Sunil and Chennupati, Gopinath and Bilmes, Jeff A and Bhattacharya, Tanmoy and Michalak, Sarah},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{rehman2022federated,
  title={Federated self-supervised learning for video understanding},
  author={Rehman, Yasar Abbas Ur and Gao, Yan and Shen, Jiajun and de Gusmao, Pedro Porto Buarque and Lane, Nicholas},
  booktitle={European Conference on Computer Vision},
  pages={506--522},
  year={2022},
  organization={Springer}
}

@article{rehman2023dawa,
  title={L-DAWA: Layer-wise Divergence Aware Weight Aggregation in Federated Self-Supervised Visual Representation Learning},
  author={Rehman, Yasar Abbas Ur and Gao, Yan and de Gusm{\~a}o, Pedro Porto Buarque and Alibeigi, Mina and Shen, Jiajun and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2307.07393},
  year={2023}
}

@article{french2017self,
  title={Self-ensembling for visual domain adaptation},
  author={French, Geoffrey and Mackiewicz, Michal and Fisher, Mark},
  journal={arXiv preprint arXiv:1706.05208},
  year={2017}
}

@inproceedings{simclr,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@article{yang2022survey,
  title={A survey on deep semi-supervised learning},
  author={Yang, Xiangli and Song, Zixing and King, Irwin and Xu, Zenglin},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2022},
  publisher={IEEE}
}

@article{chapelle2009semi,
  title={Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]},
  author={Chapelle, Olivier and Scholkopf, Bernhard and Zien, Alexander},
  journal={IEEE Transactions on Neural Networks},
  volume={20},
  number={3},
  pages={542--542},
  year={2009},
  publisher={IEEE}
}

@article{miyato2018virtual,
  title={Virtual adversarial training: a regularization method for supervised and semi-supervised learning},
  author={Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={8},
  pages={1979--1993},
  year={2018},
  publisher={IEEE}
}

@article{sajjadi2016regularization,
  title={Regularization with stochastic transformations and perturbations for deep semi-supervised learning},
  author={Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen, Tolga},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}
@article{tarvainen2017mean,
  title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{lim2020federated,
  title={Federated learning in mobile edge networks: A comprehensive survey},
  author={Lim, Wei Yang Bryan and Luong, Nguyen Cong and Hoang, Dinh Thai and Jiao, Yutao and Liang, Ying-Chang and Yang, Qiang and Niyato, Dusit and Miao, Chunyan},
  journal={IEEE Communications Surveys \& Tutorials},
  volume={22},
  number={3},
  pages={2031--2063},
  year={2020},
  publisher={IEEE}
}

@article{nguyen2023boosting,
  title={Boosting Semi-Supervised Learning by bridging high and low-confidence predictions},
  author={Nguyen, Khanh-Binh and Yang, Joon-Sung},
  journal={arXiv preprint arXiv:2308.07509},
  year={2023}
}

@inproceedings{DBLP:conf/iclr/MoschellaMFNLR23,
  author       = {Luca Moschella and
                  Valentino Maiorca and
                  Marco Fumero and
                  Antonio Norelli and
                  Francesco Locatello and
                  Emanuele Rodol{\`{a}}},
  title        = {Relative representations enable zero-shot latent space communication},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=SrC-nwieGJ},
  timestamp    = {Fri, 30 Jun 2023 14:55:52 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/MoschellaMFNLR23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/MorcosRB18,
  author       = {Ari S. Morcos and
                  Maithra Raghu and
                  Samy Bengio},
  editor       = {Samy Bengio and
                  Hanna M. Wallach and
                  Hugo Larochelle and
                  Kristen Grauman and
                  Nicol{\`{o}} Cesa{-}Bianchi and
                  Roman Garnett},
  title        = {Insights on representational similarity in neural networks with canonical
                  correlation},
  booktitle    = {Advances in Neural Information Processing Systems 31: Annual Conference
                  on Neural Information Processing Systems 2018, NeurIPS 2018, December
                  3-8, 2018, Montr{\'{e}}al, Canada},
  pages        = {5732--5741},
  year         = {2018},
  url          = {https://proceedings.neurips.cc/paper/2018/hash/a7a3d70c6d17a73140918996d03c014f-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/MorcosRB18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:journals/corr/LiYCLH15,
  author       = {Yixuan Li and
                  Jason Yosinski and
                  Jeff Clune and
                  Hod Lipson and
                  John E. Hopcroft},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Convergent Learning: Do different neural networks learn the same representations?},
  booktitle    = {4th International Conference on Learning Representations, {ICLR} 2016,
                  San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year         = {2016},
  url          = {http://arxiv.org/abs/1511.07543},
  timestamp    = {Fri, 18 Nov 2022 15:40:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/LiYCLH15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/Kornblith0LH19,
  author       = {Simon Kornblith and
                  Mohammad Norouzi and
                  Honglak Lee and
                  Geoffrey E. Hinton},
  editor       = {Kamalika Chaudhuri and
                  Ruslan Salakhutdinov},
  title        = {Similarity of Neural Network Representations Revisited},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning,
                  {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {97},
  pages        = {3519--3529},
  publisher    = {{PMLR}},
  year         = {2019},
  url          = {http://proceedings.mlr.press/v97/kornblith19a.html},
  timestamp    = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/Kornblith0LH19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/TsitsulinMMKBOM20,
  author       = {Anton Tsitsulin and
                  Marina Munkhoeva and
                  Davide Mottin and
                  Panagiotis Karras and
                  Alexander M. Bronstein and
                  Ivan V. Oseledets and
                  Emmanuel M{\"{u}}ller},
  title        = {The Shape of Data: Intrinsic Distance for Data Distributions},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=HyebplHYwB},
  timestamp    = {Thu, 21 Jan 2021 17:36:45 +0100},
  biburl       = {https://dblp.org/rec/conf/iclr/TsitsulinMMKBOM20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/VulicRS20,
  author       = {Ivan Vulic and
                  Sebastian Ruder and
                  Anders S{\o}gaard},
  editor       = {Bonnie Webber and
                  Trevor Cohn and
                  Yulan He and
                  Yang Liu},
  title        = {Are All Good Word Vector Spaces Isomorphic?},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages        = {3178--3192},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.emnlp-main.257},
  doi          = {10.18653/v1/2020.emnlp-main.257},
  timestamp    = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/VulicRS20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/ZaheerKRPSS17,
  author       = {Manzil Zaheer and
                  Satwik Kottur and
                  Siamak Ravanbakhsh and
                  Barnab{\'{a}}s P{\'{o}}czos and
                  Ruslan Salakhutdinov and
                  Alexander J. Smola},
  editor       = {Isabelle Guyon and
                  Ulrike von Luxburg and
                  Samy Bengio and
                  Hanna M. Wallach and
                  Rob Fergus and
                  S. V. N. Vishwanathan and
                  Roman Garnett},
  title        = {Deep Sets},
  booktitle    = {Advances in Neural Information Processing Systems 30: Annual Conference
                  on Neural Information Processing Systems 2017, December 4-9, 2017,
                  Long Beach, CA, {USA}},
  pages        = {3391--3401},
  year         = {2017},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/ZaheerKRPSS17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/frai/HenselMR21,
  author       = {Felix Hensel and
                  Michael Moor and
                  Bastian Rieck},
  title        = {A Survey of Topological Machine Learning Methods},
  journal      = {Frontiers Artif. Intell.},
  volume       = {4},
  pages        = {681108},
  year         = {2021},
  url          = {https://doi.org/10.3389/frai.2021.681108},
  doi          = {10.3389/frai.2021.681108},
  timestamp    = {Wed, 07 Dec 2022 23:04:15 +0100},
  biburl       = {https://dblp.org/rec/journals/frai/HenselMR21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/MoorHRB20,
  author       = {Michael Moor and
                  Max Horn and
                  Bastian Rieck and
                  Karsten M. Borgwardt},
  title        = {Topological Autoencoders},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning,
                  {ICML} 2020, 13-18 July 2020, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {7045--7054},
  publisher    = {{PMLR}},
  year         = {2020},
  url          = {http://proceedings.mlr.press/v119/moor20a.html},
  timestamp    = {Tue, 15 Dec 2020 17:40:19 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/MoorHRB20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/cvpr/HeZRS16,
  author       = {Kaiming He and
                  Xiangyu Zhang and
                  Shaoqing Ren and
                  Jian Sun},
  title        = {Deep Residual Learning for Image Recognition},
  booktitle    = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages        = {770--778},
  publisher    = {{IEEE} Computer Society},
  year         = {2016},
  url          = {https://doi.org/10.1109/CVPR.2016.90},
  doi          = {10.1109/CVPR.2016.90},
  timestamp    = {Fri, 24 Mar 2023 00:02:57 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/HeZRS16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{yurochkin2019bayesian,
  title={Bayesian nonparametric federated learning of neural networks},
  author={Yurochkin, Mikhail and Agarwal, Mayank and Ghosh, Soumya and Greenewald, Kristjan and Hoang, Nghia and Khazaeni, Yasaman},
  booktitle={International Conference on Machine Learning},
  pages={7252--7261},
  year={2019},
  organization={PMLR}
}

@inproceedings{
reddi2020adaptive,
title={Adaptive Federated Optimization},
author={Sashank J. Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone{\v{c}}n{\'y} and Sanjiv Kumar and Hugh Brendan McMahan},
booktitle={International Conference on Learning Representations},
year={2021},
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}

@article{bachman2014learning,
  title={Learning with pseudo-ensembles},
  author={Bachman, Philip and Alsharif, Ouais and Precup, Doina},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}


@article{fedprox,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={Proceedings of Machine learning and systems},
  volume={2},
  pages={429--450},
  year={2020}
}

@inproceedings{scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International conference on machine learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

@article{zhao2018federated,
  title={Federated learning with non-iid data},
  author={Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  journal={arXiv preprint arXiv:1806.00582},
  year={2018}
}


%-----------------mphil thesis

@misc{py-spy,
   author = {Ben Frederickson},
   title = {py-spy: Sampling profiler for Python programs},
   howpublished = {\url{https://github.com/benfred/py-spy}}
}

@misc{py-spy,
   author = {Ben Frederickson},
   title = {py-spy: Sampling profiler for Python programs},
   howpublished = {\url{https://github.com/google/pprof}}
}

@conference {perf,
author = {Brendan Gregg},
title = {Linux Performance Analysis: New Tools and Old Secrets},
year = {2014},
address = {Seattle, WA},
publisher = {USENIX Association},
month = nov,
}

%-----------------R255 Begin---------------

@misc{personalization_survey,
      title={Survey of Personalization Techniques for Federated Learning}, 
      author={Viraj Kulkarni and Milind Kulkarni and Aniruddha Pant},
      year={2020},
      eprint={2003.08673},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{personalization,
      title={Federated Learning with Personalization Layers}, 
      author={Manoj Ghuhan Arivazhagan and Vinay Aggarwal and Aaditya Kumar Singh and Sunav Choudhary},
      year={2019},
      eprint={1912.00818},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{li2019fedmd,
  title={Fedmd: Heterogenous federated learning via model distillation},
  author={Li, Daliang and Wang, Junpu},
  journal={arXiv preprint arXiv:1910.03581},
  year={2019}
}

@article{noniidsurvey,
  author    = {Hangyu Zhu and
               Jinjin Xu and
               Shiqing Liu and
               Yaochu Jin},
  title     = {Federated Learning on Non-IID Data: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2106.06843},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.06843},
  eprinttype = {arXiv},
  eprint    = {2106.06843},
  timestamp = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-06843.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{datasharing,
  author    = {Yue Zhao and
               Meng Li and
               Liangzhen Lai and
               Naveen Suda and
               Damon Civin and
               Vikas Chandra},
  title     = {Federated Learning with Non-IID Data},
  journal   = {CoRR},
  volume    = {abs/1806.00582},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.00582},
  eprinttype = {arXiv},
  eprint    = {1806.00582},
  timestamp = {Thu, 14 Oct 2021 09:14:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-00582.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{personalized_representations,
      title={Exploiting Shared Representations for Personalized Federated Learning}, 
      author={Liam Collins and Hamed Hassani and Aryan Mokhtari and Sanjay Shakkottai},
      year={2021},
      eprint={2102.07078},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{partial_fed,
  title={PartialFed: Cross-Domain Personalized Federated Learning via Partial Initialization},
  author={Sun, Benyuan and Huo, Hongxing and Yang, Yi and Bai, Bo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{privacy_whiteboxattack,
   title={Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning},
   url={http://dx.doi.org/10.1109/SP.2019.00065},
   DOI={10.1109/sp.2019.00065},
   journal={2019 IEEE Symposium on Security and Privacy (SP)},
   publisher={IEEE},
   author={Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
   year={2019},
   month={May}
}

@INPROCEEDINGS{privacy_membership,  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},  booktitle={2017 IEEE Symposium on Security and Privacy (SP)},   title={Membership Inference Attacks Against Machine Learning Models},   year={2017},  volume={},  number={},  pages={3-18},  doi={10.1109/SP.2017.41}}

@misc{privacy_overfit,
      title={Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting}, 
      author={Samuel Yeom and Irene Giacomelli and Matt Fredrikson and Somesh Jha},
      year={2018},
      eprint={1709.01604},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{data_sharing_2,
  author    = {Naoya Yoshida and
               Takayuki Nishio and
               Masahiro Morikura and
               Koji Yamamoto and
               Ryo Yonetani},
  title     = {Hybrid-FL: Cooperative Learning Mechanism Using Non-IID Data in Wireless
               Networks},
  journal   = {CoRR},
  volume    = {abs/1905.07210},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.07210},
  eprinttype = {arXiv},
  eprint    = {1905.07210},
  timestamp = {Tue, 28 May 2019 12:48:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-07210.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{dataaug_1,
  author    = {Moming Duan},
  title     = {Astraea: Self-balancing Federated Learning for Improving Classification
               Accuracy of Mobile Deep Learning Applications},
  journal   = {CoRR},
  volume    = {abs/1907.01132},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.01132},
  eprinttype = {arXiv},
  eprint    = {1907.01132},
  timestamp = {Mon, 08 Jul 2019 14:12:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-01132.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{xormixup,
  author    = {Myungjae Shin and
               Chihoon Hwang and
               Joongheon Kim and
               Jihong Park and
               Mehdi Bennis and
               Seong{-}Lyun Kim},
  title     = {{XOR} Mixup: Privacy-Preserving Data Augmentation for One-Shot Federated
               Learning},
  journal   = {CoRR},
  volume    = {abs/2006.05148},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.05148},
  eprinttype = {arXiv},
  eprint    = {2006.05148},
  timestamp = {Thu, 14 Oct 2021 09:18:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-05148.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{
fedmix,
title={FedMix: Approximation of Mixup under Mean Augmented Federated Learning},
author={Tehrim Yoon and Sumin Shin and Sung Ju Hwang and Eunho Yang},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Ogga20D2HO-}
}


@article{wu2020personalized,
  title={Personalized federated learning for intelligent IoT applications: A cloud-edge based framework},
  author={Wu, Qiong and He, Kaiwen and Chen, Xu},
  journal={IEEE Open Journal of the Computer Society},
  volume={1},
  pages={35--44},
  year={2020},
  publisher={IEEE}
}

@article{McMahan_2016,
  author    = {H. Brendan McMahan and
               Eider Moore and
               Daniel Ramage and
               Blaise Ag{\"{u}}era y Arcas},
  title     = {Federated Learning of Deep Networks using Model Averaging},
  journal   = {CoRR},
  volume    = {abs/1602.05629},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.05629},
  eprinttype = {arXiv},
  eprint    = {1602.05629},
  timestamp = {Mon, 13 Aug 2018 16:48:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/McMahanMRA16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Anit_2018,
  author    = {Anit Kumar Sahu and
               Tian Li and
               Maziar Sanjabi and
               Manzil Zaheer and
               Ameet Talwalkar and
               Virginia Smith},
  title     = {On the Convergence of Federated Optimization in Heterogeneous Networks},
  journal   = {CoRR},
  volume    = {abs/1812.06127},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.06127},
  eprinttype = {arXiv},
  eprint    = {1812.06127},
  timestamp = {Wed, 23 Dec 2020 09:35:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-06127.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Tian_2019,
  author    = {Tian Li and
               Maziar Sanjabi and
               Virginia Smith},
  title     = {Fair Resource Allocation in Federated Learning},
  journal   = {CoRR},
  volume    = {abs/1905.10497},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.10497},
  eprinttype = {arXiv},
  eprint    = {1905.10497},
  timestamp = {Wed, 23 Dec 2020 09:35:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-10497.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{
reddi2021adaptive,
title={Adaptive Federated Optimization},
author={Sashank J. Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone{\v{c}}n{\'y} and Sanjiv Kumar and Hugh Brendan McMahan},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=LkFG3lB13U5}
}
@article{Daniel2020,
  author    = {Daniel J. Beutel and
               Taner Topal and
               Akhil Mathur and
               Xinchi Qiu and
               Titouan Parcollet and
               Nicholas D. Lane},
  title     = {Flower: {A} Friendly Federated Learning Research Framework},
  journal   = {CoRR},
  volume    = {abs/2007.14390},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.14390},
  eprinttype = {arXiv},
  eprint    = {2007.14390},
  timestamp = {Mon, 03 Aug 2020 14:32:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-14390.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{cheng2017survey,
  title={A survey of model compression and acceleration for deep neural networks},
  author={Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  journal={arXiv preprint arXiv:1710.09282},
  year={2017}
}

@article{ba2013deep,
  title={Do deep nets really need to be deep?},
  author={Ba, Lei Jimmy and Caruana, Rich},
  journal={arXiv preprint arXiv:1312.6184},
  year={2013}
}

@article{smith2017federated,
  title={Federated multi-task learning},
  author={Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:1705.10467},
  year={2017}
}

@article{khodak2019adaptive,
  title={Adaptive gradient-based meta-learning methods},
  author={Khodak, Mikhail and Balcan, Maria-Florina and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:1906.02717},
  year={2019}
}

@article{chen2018federated,
  title={Federated meta-learning with fast convergence and efficient communication},
  author={Chen, Fei and Luo, Mi and Dong, Zhenhua and Li, Zhenguo and He, Xiuqiang},
  journal={arXiv preprint arXiv:1802.07876},
  year={2018}
}

@inproceedings{
qiu2022zerofl,
title={Zero{FL}: Efficient On-Device Training for  Federated Learning with Local Sparsity},
author={Xinchi Qiu and Javier Fernandez-Marques and Pedro PB Gusmao and Yan Gao and Titouan Parcollet and Nicholas Donald Lane},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=2sDQwC_hmnM}
}

%------PhD RP--------

@article{beutel2020flower,
  title={Flower: A friendly federated learning research framework},
  author={Beutel, Daniel J and Topal, Taner and Mathur, Akhil and Qiu, Xinchi and Parcollet, Titouan and de Gusm{\~a}o, Pedro PB and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2007.14390},
  year={2020}
}
@inproceedings{moritz2018ray,
  title={Ray: A distributed framework for emerging $\{$AI$\}$ applications},
  author={Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I and others},
  booktitle={13th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 18)},
  pages={561--577},
  year={2018}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{hard2018federated,
  title={Federated learning for mobile keyboard prediction},
  author={Hard, Andrew and Rao, Kanishka and Mathews, Rajiv and Ramaswamy, Swaroop and Beaufays, Fran{\c{c}}oise and Augenstein, Sean and Eichner, Hubert and Kiddon, Chlo{\'e} and Ramage, Daniel},
  journal={arXiv preprint arXiv:1811.03604},
  year={2018}
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th $\{$USENIX$\}$ symposium on operating systems design and implementation ($\{$OSDI$\}$ 16)},
  pages={265--283},
  year={2016}
}

@article{chen2015mxnet,
  title={Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems},
  author={Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
  journal={arXiv preprint arXiv:1512.01274},
  year={2015}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={8026--8037},
  year={2019}
}

@inproceedings{lai2021fedscale,
  title={FedScale: Benchmarking model and system performance of federated learning},
  author={Lai, Fan and Dai, Yinwei and Zhu, Xiangfeng and Madhyastha, Harsha V and Chowdhury, Mosharaf},
  booktitle={Proceedings of the First Workshop on Systems Challenges in Reliable and Secure Federated Learning},
  pages={1--3},
  year={2021}
}

@article{semwal2020fedperf,
  title={FedPerf: A Practitioners’ Guide to Performance of Federated Learning Algorithms},
  author={Semwal, Tushar and Mulay, Ajinkya and Agrawal, Ayush Manish},
  year={2020},
  publisher={OSF Preprints}
}

@article{bonawitz2019towards,
  title={Towards federated learning at scale: System design},
  author={Bonawitz, Keith and Eichner, Hubert and Grieskamp, Wolfgang and Huba, Dzmitry and Ingerman, Alex and Ivanov, Vladimir and Kiddon, Chloe and Kone{\v{c}}n{\`y}, Jakub and Mazzocchi, Stefano and McMahan, H Brendan and others},
  journal={arXiv preprint arXiv:1902.01046},
  year={2019}
}

@online{openai,
  title = {AI and Compute},
  author = {Amodei, Dario and Hernandez, Danny and Sastry, Girish and Clark, Jack and Brockman, Greg and Sutskever, Ilya},
  year = {2019},
  addendum = {(accessed: 25.02.2021)},
  url = {https://openai.com/blog/ai-and-compute/}
}

@article{strubell2019energy,
  title = {Energy and policy considerations for deep learning in NLP},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal = {arXiv preprint arXiv:1906.02243},
  year = {2019}
}

@misc{carbon_footprint,
  author = {Qiu, Xinchi and Parcollet, Titouan and Fernandez-Marques, Javier and de Gusmao, Pedro Porto Buarque and Beutel, Daniel J. and Topal, Taner and Mathur, Akhil and Lane, Nicholas D.},
  title = {A first look into the carbon footprint of federated learning},
  year = {2021},
  eprint = {arXiv:2102.07627}
}


@article{li2020federated,
  title={Federated learning: Challenges, methods, and future directions},
  author={Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={3},
  pages={50--60},
  year={2020},
  publisher={IEEE}
}

@article{wu2020personalized,
  title={Personalized federated learning for intelligent IoT applications: A cloud-edge based framework},
  author={Wu, Qiong and He, Kaiwen and Chen, Xu},
  journal={IEEE Open Journal of the Computer Society},
  volume={1},
  pages={35--44},
  year={2020},
  publisher={IEEE}
}

@article{sim2019investigation,
  title={An investigation into on-device personalization of end-to-end automatic speech recognition models},
  author={Sim, Khe Chai and Zadrazil, Petr and Beaufays, Fran{\c{c}}oise},
  journal={arXiv preprint arXiv:1909.06678},
  year={2019}
}

@article{li2019fedmd,
  title={Fedmd: Heterogenous federated learning via model distillation},
  author={Li, Daliang and Wang, Junpu},
  journal={arXiv preprint arXiv:1910.03581},
  year={2019}
}

@article{ozkara2021qupel,
  title={QuPeL: Quantized Personalization with Applications to Federated Learning},
  author={Ozkara, Kaan and Singh, Navjot and Data, Deepesh and Diggavi, Suhas},
  journal={arXiv preprint arXiv:2102.11786},
  year={2021}
}

@article{liu2021fate,
  title={FATE: An industrial grade platform for collaborative learning with data protection},
  author={Liu, Yang and Fan, Tao and Chen, Tianjian and Xu, Qian and Yang, Qiang},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={226},
  pages={1--6},
  year={2021}
}

@article{linardatos2021explainable,
  title={Explainable ai: A review of machine learning interpretability methods},
  author={Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  journal={Entropy},
  volume={23},
  number={1},
  pages={18},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@inproceedings{endert2017state,
  title={The state of the art in integrating machine learning into visual analytics},
  author={Endert, Alex and Ribarsky, William and Turkay, Cagatay and Wong, BL William and Nabney, Ian and Blanco, I D{\'\i}az and Rossi, Fabrice},
  booktitle={Computer Graphics Forum},
  volume={36},
  number={8},
  pages={458--486},
  year={2017},
  organization={Wiley Online Library}
}

@article{amershi2014power,
  title={Power to the people: The role of humans in interactive machine learning},
  author={Amershi, Saleema and Cakmak, Maya and Knox, William Bradley and Kulesza, Todd},
  journal={Ai Magazine},
  volume={35},
  number={4},
  pages={105--120},
  year={2014}
}

@article{choo2018visual,
  title={Visual analytics for explainable deep learning},
  author={Choo, Jaegul and Liu, Shixia},
  journal={IEEE computer graphics and applications},
  volume={38},
  number={4},
  pages={84--92},
  year={2018},
  publisher={IEEE}
}

@article{hohman2018visual,
  title={Visual analytics in deep learning: An interrogative survey for the next frontiers},
  author={Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
  journal={IEEE transactions on visualization and computer graphics},
  volume={25},
  number={8},
  pages={2674--2693},
  year={2018},
  publisher={IEEE}
}

@inproceedings{haroush2020knowledge,
  title={The knowledge within: Methods for data-free model compression},
  author={Haroush, Matan and Hubara, Itay and Hoffer, Elad and Soudry, Daniel},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8494--8502},
  year={2020}
}

@inproceedings{yin2020dreaming,
  title={Dreaming to distill: Data-free knowledge transfer via deepinversion},
  author={Yin, Hongxu and Molchanov, Pavlo and Alvarez, Jose M and Li, Zhizhong and Mallya, Arun and Hoiem, Derek and Jha, Niraj K and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8715--8724},
  year={2020}
}

@article{beutel2020flower,
  title={Flower: A friendly federated learning research framework},
  author={Beutel, Daniel J and Topal, Taner and Mathur, Akhil and Qiu, Xinchi and Parcollet, Titouan and de Gusm{\~a}o, Pedro PB and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2007.14390},
  year={2020}
}

@article{pye2021personalised,
  title={Personalised Federated Learning: A Combinational Approach},
  author={Pye, Sone Kyaw and Yu, Han},
  journal={arXiv preprint arXiv:2108.09618},
  year={2021}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{bagdasaryan2019differential,
  title={Differential privacy has disparate impact on model accuracy},
  author={Bagdasaryan, Eugene and Poursaeed, Omid and Shmatikov, Vitaly},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={15479--15488},
  year={2019}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@mastersthesis{chen2015convolutional,
  title={Convolutional neural network for sentence classification},
  author={Chen, Yahui},
  year={2015},
  school={University of Waterloo}
}

@article{bagdasaryan2019differential,
  title={Differential privacy has disparate impact on model accuracy},
  author={Bagdasaryan, Eugene and Poursaeed, Omid and Shmatikov, Vitaly},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={15479--15488},
  year={2019}
}

%-------FedProf----------

@article{DBLP:journals/corr/KonecnyMYRSB16,
  author    = {Jakub Kone{\v{c}}n{\'y} and
               H. Brendan McMahan and
               Felix X. Yu and
               Peter Richt{\'{a}}rik and
               Ananda Theertha Suresh and
               Dave Bacon},
  title     = {Federated Learning: Strategies for Improving Communication Efficiency},
  journal   = {CoRR},
  volume    = {abs/1610.05492},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.05492},
  eprinttype = {arXiv},
  eprint    = {1610.05492},
  timestamp = {Mon, 13 Aug 2018 16:48:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KonecnyMYRSB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1804-05271,
  author    = {Shiqiang Wang and
               Tiffany Tuor and
               Theodoros Salonidis and
               Kin K. Leung and
               Christian Makaya and
               Ting He and
               Kevin Chan},
  title     = {When Edge Meets Learning: Adaptive Control for Resource-Constrained
               Distributed Machine Learning},
  journal   = {CoRR},
  volume    = {abs/1804.05271},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.05271},
  eprinttype = {arXiv},
  eprint    = {1804.05271},
  timestamp = {Thu, 11 Mar 2021 15:48:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-05271.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{li2019convergence,
  title={On the convergence of fedavg on non-iid data},
  author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1907.02189},
  year={2019}
}

@article{DBLP:journals/corr/McMahanMRA16,
  author    = {H. Brendan McMahan and
               Eider Moore and
               Daniel Ramage and
               Blaise Ag{\"{u}}era y Arcas},
  title     = {Federated Learning of Deep Networks using Model Averaging},
  journal   = {CoRR},
  volume    = {abs/1602.05629},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.05629},
  eprinttype = {arXiv},
  eprint    = {1602.05629},
  timestamp = {Mon, 13 Aug 2018 16:48:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/McMahanMRA16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1811-12470,
  author    = {Arjun Nitin Bhagoji and
               Supriyo Chakraborty and
               Prateek Mittal and
               Seraphin B. Calo},
  title     = {Analyzing Federated Learning through an Adversarial Lens},
  journal   = {CoRR},
  volume    = {abs/1811.12470},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.12470},
  eprinttype = {arXiv},
  eprint    = {1811.12470},
  timestamp = {Mon, 03 Dec 2018 07:50:28 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-12470.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{pmlr-v108-bagdasaryan20a,
  title = 	 {How To Backdoor Federated Learning},
  author =       {Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2938--2948},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/bagdasaryan20a/bagdasaryan20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/bagdasaryan20a.html},
  abstract = 	 {Federated models are created by aggregating model updates submittedby participants.  To protect confidentiality of the training data,the aggregator by design has no visibility into how these updates aregenerated.  We show that this makes federated learning vulnerable to amodel-poisoning attack that is significantly more powerful than poisoningattacks that target only the training data.A single or multiple malicious participants can use modelreplacement to introduce backdoor functionality into the joint model,e.g., modify an image classifier so that it assigns an attacker-chosenlabel to images with certain features, or force a word predictor tocomplete certain sentences with an attacker-chosen word.  We evaluatemodel replacement under different assumptions for the standardfederated-learning tasks and show that it greatly outperformstraining-data poisoning.Federated learning employs secure aggregation to protect confidentialityof participants’ local models and thus cannot detect anomalies inparticipants’ contributions to the joint model.  To demonstrate thatanomaly detection would not have been effective in any case, we alsodevelop and evaluate a generic constrain-and-scale technique thatincorporates the evasion of defenses into the attacker’s loss functionduring training.}
}

@article{DBLP:journals/corr/abs-1911-11815,
  author    = {Minghong Fang and
               Xiaoyu Cao and
               Jinyuan Jia and
               Neil Zhenqiang Gong},
  title     = {Local Model Poisoning Attacks to Byzantine-Robust Federated Learning},
  journal   = {CoRR},
  volume    = {abs/1911.11815},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.11815},
  eprinttype = {arXiv},
  eprint    = {1911.11815},
  timestamp = {Tue, 03 Dec 2019 20:41:07 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-11815.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{6472238,  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Representation Learning: A Review and New Perspectives},   year={2013},  volume={35},  number={8},  pages={1798-1828},  doi={10.1109/TPAMI.2013.50}}

@inproceedings{10.5555/3327345.3327475,
author = {Morcos, Ari S. and Raghu, Maithra and Bengio, Samy},
title = {Insights on Representational Similarity in Neural Networks with Canonical Correlation},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method [22]. We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {5732–5741},
numpages = {10},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@article{DBLP:journals/corr/abs-2103-16257,
  author    = {Qinbin Li and
               Bingsheng He and
               Dawn Song},
  title     = {Model-Contrastive Federated Learning},
  journal   = {CoRR},
  volume    = {abs/2103.16257},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.16257},
  eprinttype = {arXiv},
  eprint    = {2103.16257},
  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-16257.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1804-08333,
  author    = {Takayuki Nishio and
               Ryo Yonetani},
  title     = {Client Selection for Federated Learning with Heterogeneous Resources
               in Mobile Edge},
  journal   = {CoRR},
  volume    = {abs/1804.08333},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.08333},
  eprinttype = {arXiv},
  eprint    = {1804.08333},
  timestamp = {Mon, 13 Aug 2018 16:47:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-08333.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1907-06040,
  author    = {Qunsong Zeng and
               Yuqing Du and
               Kin K. Leung and
               Kaibin Huang},
  title     = {Energy-Efficient Radio Resource Allocation for Federated Edge Learning},
  journal   = {CoRR},
  volume    = {abs/1907.06040},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.06040},
  eprinttype = {arXiv},
  eprint    = {1907.06040},
  timestamp = {Wed, 17 Jul 2019 10:27:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-06040.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2004-04314,
  author    = {Jie Xu and
               Heqiang Wang},
  title     = {Client Selection and Bandwidth Allocation in Wireless Federated Learning
               Networks: {A} Long-Term Perspective},
  journal   = {CoRR},
  volume    = {abs/2004.04314},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.04314},
  eprinttype = {arXiv},
  eprint    = {2004.04314},
  timestamp = {Fri, 22 Jan 2021 13:30:41 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-04314.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{9155494,  author={Wang, Hao and Kaplan, Zakhary and Niu, Di and Li, Baochun},  booktitle={IEEE INFOCOM 2020 - IEEE Conference on Computer Communications},   title={Optimizing Federated Learning on Non-IID Data with Reinforcement Learning},   year={2020},  volume={},  number={},  pages={1698-1707},  doi={10.1109/INFOCOM41043.2020.9155494}}

@inproceedings{yu2019parallel,
  title={Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning},
  author={Yu, Hao and Yang, Sen and Zhu, Shenghuo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={5693--5700},
  year={2019}
}
@article{DBLP:journals/corr/abs-2007-07481,
  author    = {Jianyu Wang and
               Qinghua Liu and
               Hao Liang and
               Gauri Joshi and
               H. Vincent Poor},
  title     = {Tackling the Objective Inconsistency Problem in Heterogeneous Federated
               Optimization},
  journal   = {CoRR},
  volume    = {abs/2007.07481},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.07481},
  eprinttype = {arXiv},
  eprint    = {2007.07481},
  timestamp = {Tue, 21 Jul 2020 12:53:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-07481.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1808-07576,
  author    = {Jianyu Wang and
               Gauri Joshi},
  title     = {Cooperative {SGD:} {A} unified Framework for the Design and Analysis
               of Communication-Efficient {SGD} Algorithms},
  journal   = {CoRR},
  volume    = {abs/1808.07576},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.07576},
  eprinttype = {arXiv},
  eprint    = {1808.07576},
  timestamp = {Sun, 02 Sep 2018 15:01:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-07576.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2102-01733,
  author    = {Wentai Wu and
               Ligang He and
               Weiwei Lin and
               Rui Mao and
               Chenlin Huang and
               Wei Song},
  title     = {FedProf: Optimizing Federated Learning with Dynamic Data Profiling},
  journal   = {CoRR},
  volume    = {abs/2102.01733},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.01733},
  eprinttype = {arXiv},
  eprint    = {2102.01733},
  timestamp = {Tue, 09 Feb 2021 13:35:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-01733.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2001-09249,
  author    = {Zheng Chai and
               Ahsan Ali and
               Syed Zawad and
               Stacey Truex and
               Ali Anwar and
               Nathalie Baracaldo and
               Yi Zhou and
               Heiko Ludwig and
               Feng Yan and
               Yue Cheng},
  title     = {TiFL: {A} Tier-based Federated Learning System},
  journal   = {CoRR},
  volume    = {abs/2001.09249},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.09249},
  eprinttype = {arXiv},
  eprint    = {2001.09249},
  timestamp = {Thu, 14 Oct 2021 09:16:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2001-09249.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Misc{PytorchProfiler,
note = {\url{https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html}},
title = {Profiling your PyTorch Module},
author = {Suraj Subramanian}
}

%-------zeroFL-------
@misc{47586,
title	= {Federated Learning for Mobile Keyboard Prediction},
author	= {Andrew Hard and Chloé M Kiddon and Daniel Ramage and Francoise Beaufays and Hubert Eichner and Kanishka Rao and Rajiv Mathews and Sean Augenstein},
year	= {2018},
URL	= {https://arxiv.org/abs/1811.03604}
}

@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}



@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@misc{cheng2021finetuning,
      title={Fine-tuning is Fine in Federated Learning}, 
      author={Gary Cheng and Karan Chadha and John Duchi},
      year={2021},
      eprint={2108.07313},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{10.1145/3208040.3208062, author = {Hong, Changwan and Sukumaran-Rajam, Aravind and Bandyopadhyay, Bortik and Kim, Jinsung and Kurt, S\"{u}reyya Emre and Nisa, Israt and Sabhlok, Shivani and \c{C}ataly\"{u}rek, \"{U}mit V. and Parthasarathy, Srinivasan and Sadayappan, P.}, title = {Efficient Sparse-Matrix Multi-Vector Product on GPUs}, year = {2018}, isbn = {9781450357852}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3208040.3208062}, doi = {10.1145/3208040.3208062}, booktitle = {Proceedings of the 27th International Symposium on High-Performance Parallel and Distributed Computing}, pages = {66–79}, numpages = {14}, keywords = {sparse matrix-matrix multiplication, sparse matrix-vector multiplication, GPU, sparse matrix multi-vector multiplication}, location = {Tempe, Arizona}, series = {HPDC '18} }

@INPROCEEDINGS{cambriconx2016micro,
    author={S. {Zhang} and Z. {Du} and L. {Zhang} and H. {Lan} and S. {Liu} and L. {Li} and Q. {Guo} and T. {Chen} and Y. {Chen}},
    booktitle={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
    title={{Cambricon-X: An Accelerator for Sparse Neural Networks}}, 
    year={2016},
    volume={},
    number={},
    pages={1-12},
}

@misc{han2015deep,
    title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
    author={Song Han and Huizi Mao and William J. Dally},
    year={2015},
    eprint={1510.00149},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}


@inproceedings{10.5555/3304889.3304970,
author = {He, Yang and Kang, Guoliang and Dong, Xuanyi and Fu, Yanwei and Yang, Yi},
title = {Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {2234–2240},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@misc{hard2020training,
      title={Training Keyword Spotting Models on Non-IID Data with Federated Learning}, 
      author={Andrew Hard and Kurt Partridge and Cameron Nguyen and Niranjan Subrahmanya and Aishanee Shah and Pai Zhu and Ignacio Lopez Moreno and Rajiv Mathews},
      year={2020},
      eprint={2005.10406},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@inproceedings{iccv2017ThiNet,
   title={{ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression}},
   author={Jian-Hao Luo, Jianxin Wu and Weiyao Lin},
   booktitle={International Conference on Computer Vision (ICCV)},
   month={October},
   year={2017}
}


@InProceedings{He_2017_ICCV,
author = {He, Yihui and Zhang, Xiangyu and Sun, Jian},
title = {Channel Pruning for Accelerating Very Deep Neural Networks},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@INPROCEEDINGS {8579056,
author = {R. Yu and A. Li and C. Chen and J. Lai and V. I. Morariu and X. Han and M. Gao and C. Lin and L. S. Davis},
booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {NISP: Pruning Networks Using Neuron Importance Score Propagation},
year = {2018},
volume = {},
issn = {},
pages = {9194-9203},
keywords = {neurons;redundancy;optimization;acceleration;biological neural networks;task analysis;feature extraction},
doi = {10.1109/CVPR.2018.00958},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00958},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@inproceedings{Molchanov_2019,
   title={{Importance Estimation for Neural Network Pruning}},
   //ISBN={9781728132938},
   //url={http://dx.doi.org/10.1109/CVPR.2019.01152},
   //DOI={10.1109/cvpr.2019.01152},
   booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
   author={Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan},
   year={2019},
}

@InProceedings{pmlr-v70-wang17m, 
    title = {{Beyond Filters: Compact Feature Map for Portable Deep Model}}, 
    author = {Yunhe Wang and Chang Xu and Chao Xu and Dacheng Tao}, 
    booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)}, 
    pages = {3703--3711}, 
    year = {2017}, 
}

@misc{ren2018sbnet,
      title={SBNet: Sparse Blocks Network for Fast Inference}, 
      author={Mengye Ren and Andrei Pokrovsky and Bin Yang and Raquel Urtasun},
      year={2018},
      booktitle = {2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
}

@article{Wen2020BlocksparseCT,
  title={Block-sparse CNN: towards a fast and memory-efficient framework for convolutional neural networks},
  author={N. Wen and R. Guo and B. He and Yong Fan and Ding Ma},
  journal={Applied Intelligence},
  year={2020},
  volume={51},
  pages={441-452}
}

@misc{hoefler2021sparsity,
      title={Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks}, 
      author={Torsten Hoefler and Dan Alistarh and Tal Ben-Nun and Nikoli Dryden and Alexandra Peste},
      year={2021},
      eprint={2102.00554},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@InProceedings{Elsen_2020_CVPR,
author = {Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
title = {Fast Sparse ConvNets},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@inproceedings{NIPS1992_303ed4c6,
 author = {Hassibi, Babak and Stork, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Second order derivatives for network pruning: Optimal Brain Surgeon},
 url = {https://proceedings.neurips.cc/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf},
 volume = {5},
 year = {1993}
}

@misc{openai_sparse,
      title={Block-sparse gpu kernels}, 
      author={S. Gray, A. Radford and D. P. Kingma},
      year={2017},
      url={https://blog.openai.com/block-sparse-gpu-kernels/}
}

@misc{verelst2020segblocks,
      title={SegBlocks: Block-Based Dynamic Resolution Networks for Real-Time Segmentation}, 
      author={Thomas Verelst and Tinne Tuytelaars},
      year={2020},
      eprint={2011.12025},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@inproceedings{10.5555/3157096.3157251,
author = {Guo, Yiwen and Yao, Anbang and Chen, Yurong},
title = {Dynamic Network Surgery for Efficient DNNs},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {1387–1395},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@inproceedings{10.5555/2969239.2969366,
author = {Han, Song and Pool, Jeff and Tran, John and Dally, William J.},
title = {Learning Both Weights and Connections for Efficient Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1135–1143},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@InProceedings{pmlr-v70-molchanov17a, title = {Variational Dropout Sparsifies Deep Neural Networks}, author = {Dmitry Molchanov and Arsenii Ashukha and Dmitry Vetrov}, booktitle = {Proceedings of the 34th International Conference on Machine Learning}, pages = {2498--2507}, year = {2017}, editor = {Precup, Doina and Teh, Yee Whye}, volume = {70}, series = {Proceedings of Machine Learning Research}, month = {06--11 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v70/molchanov17a/molchanov17a.pdf}, url = { http://proceedings.mlr.press/v70/molchanov17a.html }}


@misc{wang2021sparsednn,
      title={SparseDNN: Fast Sparse Deep Learning Inference on CPUs}, 
      author={Ziheng Wang},
      year={2021},
      eprint={2101.07948},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{10.1145/3293883.3295712, author = {Hong, Changwan and Sukumaran-Rajam, Aravind and Nisa, Israt and Singh, Kunal and Sadayappan, P.}, title = {Adaptive Sparse Tiling for Sparse Matrix Multiplication}, year = {2019}, isbn = {9781450362252}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3293883.3295712}, doi = {10.1145/3293883.3295712}, abstract = {Tiling is a key technique for data locality optimization and is widely used in high-performance implementations of dense matrix-matrix multiplication for multicore/manycore CPUs and GPUs. However, the irregular and matrix-dependent data access pattern of sparse matrix multiplication makes it challenging to use tiling to enhance data reuse. In this paper, we devise an adaptive tiling strategy and apply it to enhance the performance of two primitives: SpMM (product of sparse matrix and dense matrix) and SDDMM (sampled dense-dense matrix multiplication). In contrast to studies that have resorted to non-standard sparse-matrix representations to enhance performance, we use the standard Compressed Sparse Row (CSR) representation, within which intra-row reordering is performed to enable adaptive tiling. Experimental evaluation using an extensive set of matrices from the Sparse Suite collection demonstrates significant performance improvement over currently available state-of-the-art alternatives.}, booktitle = {Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming}, pages = {300–314}, numpages = {15}, keywords = {GPU, SDDMM, SpMM, sampled dense-dense matrix multiplication, multicore/manycore, sparse matrix-matrix multiplication, tiling}, location = {Washington, District of Columbia}, series = {PPoPP '19} }

@INPROCEEDINGS{tensaurus2020hpca,
    author={N. {Srivastava} and H. {Jin} and S. {Smith} and H. {Rong} and D. {Albonesi} and Z. {Zhang}},
    booktitle={{2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)}}, 
    title={{Tensaurus: A Versatile Accelerator for Mixed Sparse-Dense Tensor Computations}}, 
    year={2020},
    volume={},
    number={},
    pages={689-702},
}


@INPROCEEDINGS{sparsecnnaccel2019fccm,
  author={L. {Lu} and J. {Xie} and R. {Huang} and J. {Zhang} and W. {Lin} and Y. {Liang}},
  booktitle={IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)}, 
  title={{An Efficient Hardware Accelerator for Sparse Convolutional Neural Networks on FPGAs}}, 
  year={2019},
  volume={},
  number={},
  pages={17-25},
}

@article{Zachariadis_2020,
   title={Accelerating sparse matrix–matrix multiplication with GPU Tensor Cores},
   volume={88},
   ISSN={0045-7906},
   url={http://dx.doi.org/10.1016/j.compeleceng.2020.106848},
   DOI={10.1016/j.compeleceng.2020.106848},
   journal={Computers & Electrical Engineering},
   publisher={Elsevier BV},
   author={Zachariadis, Orestis and Satpute, Nitin and Gómez-Luna, Juan and Olivares, Joaquín},
   year={2020},
   month={Dec},
   pages={106848}
}


@inproceedings{10.1145/3410463.3414654, author = {Wang, Ziheng}, title = {SparseRT: Accelerating Unstructured Sparsity on GPUs for Deep Learning Inference}, year = {2020}, isbn = {9781450380751}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3410463.3414654}, doi = {10.1145/3410463.3414654},booktitle = {Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques}, pages = {31–42}, numpages = {12}, keywords = {deep learning, gpu, inference, sparse, unstructured, pruning}, location = {Virtual Event, GA, USA}, series = {PACT '20} }

@article{fedprox,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={arXiv preprint arXiv:1812.06127},
  year={2018}
}


@article{lim2020federated,
  title={Federated learning in mobile edge networks: A comprehensive survey},
  author={Lim, Wei Yang Bryan and Luong, Nguyen Cong and Hoang, Dinh Thai and Jiao, Yutao and Liang, Ying-Chang and Yang, Qiang and Niyato, Dusit and Miao, Chunyan},
  journal={IEEE Communications Surveys \& Tutorials},
  year={2020},
  publisher={IEEE}
}


@InProceedings{sun17meprop,
  title = 	 {me{P}rop: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting},
  author = 	 {Xu Sun and Xuancheng Ren and Shuming Ma and Houfeng Wang},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {3299--3308},
  year = 	 {2017},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia}
}

@inproceedings{
Wang2020Federated,
title={Federated Learning with Matched Averaging},
author={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BkluqlSFDS}
}

@article{gdpr,
  title={Privacy preservation in federated learning: An insightful survey from the GDPR perspective},
  author={Truong, Nguyen and Sun, Kai and Wang, Siyao and Guitton, Florian and Guo, YiKe},
  journal={Computers \& Security},
  pages={102402},
  year={2021},
  publisher={Elsevier}
}

@ARTICLE{1447941,
  author={Tinney, W.F. and Walker, J.W.},
  journal={Proceedings of the IEEE}, 
  title={Direct solutions of sparse network equations by optimally ordered triangular factorization}, 
  year={1967},
  volume={55},
  number={11},
  pages={1801-1809},
  doi={10.1109/PROC.1967.6011}}


@article{qiu2021first,
  title={A first look into the carbon footprint of federated learning},
  author={Qiu, Xinchi and Parcollet, Titouan and Fernandez-Marques, Javier and de Gusmao, Pedro Porto Buarque and Beutel, Daniel J and Topal, Taner and Mathur, Akhil and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2102.07627},
  year={2021}
}



@article{fjord,
  title={FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout},
  author={Horvath, Samuel and Laskaridis, Stefanos and Almeida, Mario and Leontiadis, Ilias and Venieris, Stylianos I and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2102.13451},
  year={2021}
}


@inproceedings{resprop,
  title={Resprop: Reuse sparsified backpropagation},
  author={Goli, Negar and Aamodt, Tor M},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1548--1558},
  year={2020}
}

@article{swat,
  title={Sparse weight activation training},
  author={Raihan, Md Aamir and Aamodt, Tor M},
  journal={arXiv preprint arXiv:2001.01969},
  year={2020}
}

@article{wangni2017gradient,
  title={Gradient sparsification for communication-efficient distributed optimization},
  author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
  journal={arXiv preprint arXiv:1710.09854},
  year={2017}
}

@article{lin2017deep,
  title={Deep gradient compression: Reducing the communication bandwidth for distributed training},
  author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  journal={arXiv preprint arXiv:1712.01887},
  year={2017}
}
@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{mobilebert,
  title={Mobilebert: a compact task-agnostic bert for resource-limited devices},
  author={Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  journal={arXiv preprint arXiv:2004.02984},
  year={2020}
}

@article{federateddistillation,
  title={Federated knowledge distillation},
  author={Seo, Hyowoon and Park, Jihong and Oh, Seungeun and Bennis, Mehdi and Kim, Seong-Lyun},
  journal={arXiv preprint arXiv:2011.02367},
  year={2020}
}

@article{seo2020federated,
  title={Federated knowledge distillation},
  author={Seo, Hyowoon and Park, Jihong and Oh, Seungeun and Bennis, Mehdi and Kim, Seong-Lyun},
  journal={arXiv preprint arXiv:2011.02367},
  year={2020}
}

@misc{Speedtest,
    author={Speedtest},
    howpublished ={\url{https://www.speedtest.net}
    }
}
@misc{alexa,
    author={AWS},
    howpublished ={\url{https://aws.amazon.com/iot/solutions/connected-home/iot-and-alexa/}
    }
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
}
@misc{beutel2020flower,
    title={Flower: A Friendly Federated Learning Research Framework},
    author={Daniel J. Beutel and Taner Topal and Akhil Mathur and Xinchi Qiu and Titouan Parcollet and Nicholas D. Lane},
    year={2020},
    eprint={2007.14390},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{leaf,
  title={Leaf: A benchmark for federated settings},
  author={Caldas, Sebastian and Duddu, Sai Meher Karthik and Wu, Peter and Li, Tian and Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Smith, Virginia and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:1812.01097},
  year={2018}
}

@misc{zhang2018hello,
      title={Hello Edge: Keyword Spotting on Microcontrollers}, 
      author={Yundong Zhang and Naveen Suda and Liangzhen Lai and Vikas Chandra},
      year={2018},
      eprint={1711.07128},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{kairouz2019advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={arXiv preprint arXiv:1912.04977},
  year={2019}
}

@inproceedings{emnist,
  title={EMNIST: Extending MNIST to handwritten letters},
  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
  booktitle={2017 International Joint Conference on Neural Networks (IJCNN)},
  pages={2921--2926},
  year={2017},
  organization={IEEE}
}

@inproceedings{
reddi2020adaptive,
title={Adaptive Federated Optimization},
author={Sashank J. Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone{\v{c}}n{\'y} and Sanjiv Kumar and Hugh Brendan McMahan},
booktitle={International Conference on Learning Representations},
year={2021},
}

@inproceedings{NEURIPS2020_18df51b9,
 author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U and Jaggi, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {2351--2363},
 publisher = {Curran Associates, Inc.},
 title = {Ensemble Distillation for Robust Model Fusion in Federated Learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{zhu2021datafree,
      title={Data-Free Knowledge Distillation for Heterogeneous Federated Learning}, 
      author={Zhuangdi Zhu and Junyuan Hong and Jiayu Zhou},
      year={2021},
      eprint={2105.10056},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shahid2021communication,
      title={Communication Efficiency in Federated Learning: Achievements and Challenges}, 
      author={Osama Shahid and Seyedamin Pouriyeh and Reza M. Parizi and Quan Z. Sheng and Gautam Srivastava and Liang Zhao},
      year={2021},
      eprint={2107.10996},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{amiri2020federated,
      title={Federated Learning With Quantized Global Model Updates}, 
      author={Mohammad Mohammadi Amiri and Deniz Gunduz and Sanjeev R. Kulkarni and H. Vincent Poor},
      year={2020},
      eprint={2006.10672},
      archivePrefix={arXiv},
      primaryClass={cs.IT}
}

@misc{liu2021hierarchical,
      title={Hierarchical Quantized Federated Learning: Convergence Analysis and System Design}, 
      author={Lumin Liu and Jun Zhang and Shenghui Song and Khaled B. Letaief},
      year={2021},
      eprint={2103.14272},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@INPROCEEDINGS{8327042,
  author={Hazelwood, Kim and Bird, Sarah and Brooks, David and Chintala, Soumith and Diril, Utku and Dzhulgakov, Dmytro and Fawzy, Mohamed and Jia, Bill and Jia, Yangqing and Kalro, Aditya and Law, James and Lee, Kevin and Lu, Jason and Noordhuis, Pieter and Smelyanskiy, Misha and Xiong, Liang and Wang, Xiaodong},
  booktitle={2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  title={Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective}, 
  year={2018},
  volume={},
  number={},
  pages={620-629},
  doi={10.1109/HPCA.2018.00059}
 }


@misc{hilmkil2021scaling,
      title={Scaling Federated Learning for Fine-tuning of Large Language Models}, 
      author={Agrin Hilmkil and Sebastian Callh and Matteo Barbieri and Leon René Sütfeld and Edvin Listo Zec and Olof Mogren},
      year={2021},
      eprint={2102.00875},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{arivazhagan2019federated,
      title={Federated Learning with Personalization Layers}, 
      author={Manoj Ghuhan Arivazhagan and Vinay Aggarwal and Aaditya Kumar Singh and Sunav Choudhary},
      year={2019},
      eprint={1912.00818},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{yurochkin2019bayesian,
  title={Bayesian nonparametric federated learning of neural networks},
  author={Yurochkin, Mikhail and Agarwal, Mayank and Ghosh, Soumya and Greenewald, Kristjan and Hoang, Nghia and Khazaeni, Yasaman},
  booktitle={International Conference on Machine Learning},
  pages={7252--7261},
  year={2019},
  organization={PMLR}
}

@article{hsu2019measuring,
  title={Measuring the effects of non-identical data distribution for federated visual classification},
  author={Hsu, Tzu-Ming Harry and Qi, Hang and Brown, Matthew},
  journal={arXiv preprint arXiv:1909.06335},
  year={2019}
}

@inproceedings{samie2016computation,
  title={Computation offloading and resource allocation for low-power IoT edge devices},
  author={Samie, Farzad and Tsoutsouras, Vasileios and Bauer, Lars and Xydis, Sotirios and Soudris, Dimitrios and Henkel, J{\"o}rg},
  booktitle={2016 IEEE 3rd World Forum on Internet of Things (WF-IoT)},
  pages={7--12},
  year={2016},
  organization={IEEE}
}

@article{han2020adaptive,
  title={Adaptive gradient sparsification for efficient federated learning: An online learning approach},
  author={Han, Pengchao and Wang, Shiqiang and Leung, Kin K},
  journal={arXiv preprint arXiv:2001.04756},
  year={2020}
}

@inproceedings{melis2019exploiting,
  title={Exploiting unintended feature leakage in collaborative learning},
  author={Melis, Luca and Song, Congzheng and De Cristofaro, Emiliano and Shmatikov, Vitaly},
  booktitle={2019 IEEE Symposium on Security and Privacy (SP)},
  pages={691--706},
  year={2019},
  organization={IEEE}
}

@misc{caldas2019expanding,
      title={Expanding the Reach of Federated Learning by Reducing Client Resource Requirements}, 
      author={Sebastian Caldas and Jakub Konečny and H. Brendan McMahan and Ameet Talwalkar},
      year={2019},
      eprint={1812.07210},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{wang2018atomo,
  title={Atomo: Communication-efficient learning via atomic sparsification},
  author={Wang, Hongyi and Sievert, Scott and Charles, Zachary and Liu, Shengchao and Wright, Stephen and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:1806.04090},
  year={2018}
}

@article{doherty2017large,
  title={Large scale population assessment of physical activity using wrist worn accelerometers: The UK Biobank Study},
  author={Doherty, Aiden and Jackson, Dan and Hammerla, Nils and Pl{\"o}tz, Thomas and Olivier, Patrick and Granat, Malcolm H and White, Tom and Van Hees, Vincent T and Trenell, Michael I and Owen, Christoper G and others},
  journal={PloS one},
  volume={12},
  number={2},
  pages={e0169649},
  year={2017},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{virtanen2020scipy,
  title={SciPy 1.0: fundamental algorithms for scientific computing in Python},
  author={Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and others},
  journal={Nature methods},
  volume={17},
  number={3},
  pages={261--272},
  year={2020},
  publisher={Nature Publishing Group}
}


@article{jiang2019model,
  title={Model pruning enables efficient federated learning on edge devices},
  author={Jiang, Yuang and Wang, Shiqiang and Valls, Victor and Ko, Bong Jun and Lee, Wei-Han and Leung, Kin K and Tassiulas, Leandros},
  journal={arXiv preprint arXiv:1909.12326},
  year={2019}
}

@article{he2020group,
  title={Group knowledge transfer: Federated learning of large cnns at the edge},
  author={He, Chaoyang and Annavaram, Murali and Avestimehr, Salman},
  journal={arXiv preprint arXiv:2007.14513},
  year={2020}
}
@article{speechcommands,
  title={Speech commands: A dataset for limited-vocabulary speech recognition},
  author={Warden, Pete},
  journal={arXiv preprint arXiv:1804.03209},
  year={2018}
}

%--------others------------

@inproceedings{
balakrishnan2022diverse,
title={Diverse Client Selection for Federated Learning via Submodular Maximization},
author={Ravikumar Balakrishnan and Tian Li and Tianyi Zhou and Nageen Himayat and Virginia Smith and Jeff Bilmes},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nwKXyFvaUm}
}

@article{DBLP:journals/corr/abs-1812-06127,
  author    = {Anit Kumar Sahu and
               Tian Li and
               Maziar Sanjabi and
               Manzil Zaheer and
               Ameet Talwalkar and
               Virginia Smith},
  title     = {On the Convergence of Federated Optimization in Heterogeneous Networks},
  journal   = {CoRR},
  volume    = {abs/1812.06127},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.06127},
  eprinttype = {arXiv},
  eprint    = {1812.06127},
  timestamp = {Wed, 23 Dec 2020 09:35:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-06127.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1902-00146,
  author    = {Mehryar Mohri and
               Gary Sivek and
               Ananda Theertha Suresh},
  title     = {Agnostic Federated Learning},
  journal   = {CoRR},
  volume    = {abs/1902.00146},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.00146},
  eprinttype = {arXiv},
  eprint    = {1902.00146},
  timestamp = {Tue, 21 May 2019 18:03:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-00146.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1905-10497,
  author    = {Tian Li and
               Maziar Sanjabi and
               Virginia Smith},
  title     = {Fair Resource Allocation in Federated Learning},
  journal   = {CoRR},
  volume    = {abs/1905.10497},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.10497},
  eprinttype = {arXiv},
  eprint    = {1905.10497},
  timestamp = {Wed, 23 Dec 2020 09:35:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-10497.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%-----------------R255 End---------------


@misc{flute,
author = {Dimitriadis, Dimitrios and Hipolito Garcia, Mirian and Madrigal, Daniel and Manoel, Andre and Sim, Robert},
title = {FLUTE: A Scalable, Extensible Framework for High-Performance Federated Learning Simulations},
year = {2022},
month = {March},
abstract = {In this paper we introduce "Federated Learning Utilities and Tools for Experimentation" (FLUTE), a high-performance open source platform for federated learning research and offline simulations. The goal of FLUTE is to enable rapid prototyping and simulation of new federated learning algorithms at scale, including novel optimization, privacy, and communications strategies. We describe the architecture of FLUTE, enabling arbitrary federated modeling schemes to be realized, we compare the platform with other state-of-the-art platforms, and we describe available features of FLUTE for experimentation in core areas of active research, such as optimization, privacy and scalability. We demonstrate the effectiveness of the platform with a series of experiments for text prediction and speech recognition, including the addition of differential privacy, quantization, scaling and a variety of optimization and federation approaches.},
url = {https://www.microsoft.com/en-us/research/publication/flute-a-scalable-extensible-framework-for-high-performance-federated-learning-simulations/},
}

@article{fedjax,
  author    = {Jae Hun Ro and
               Ananda Theertha Suresh and
               Ke Wu},
  title     = {FedJAX: Federated learning simulation with {JAX}},
  journal   = {CoRR},
  volume    = {abs/2108.02117},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.02117},
  eprinttype = {arXiv},
  eprint    = {2108.02117},
  timestamp = {Thu, 05 Aug 2021 14:27:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-02117.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tff,
  title={Communication-Efficient Learning of Deep Networks from
  Decentralized Data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson,
  Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  pages={1273--1282},
  year={2017}
  }

@inproceedings{PySyft,
  title={PySyft: A Library for Easy Federated Learning},
  author={Alexander Ziller and Andrew Trask and Antonio Lopardo and Benjamin Szymkow and Bobby Wagner and Emma Bluemke and Jean-Mickael Nounahon and Jonathan Passerat-Palmbach and Kritika Prakash and Nick Rose and Theo Ryffel and Zarreen Naowal Reza and G. Kaissis},
  year={2021}
}

@article{FLHealthcare,
author = {Antunes, Rodolfo Stoffel and Andr\'{e} da Costa, Cristiano and K\"{u}derle, Arne and Yari, Imrana Abdullahi and Eskofier, Bj\"{o}rn},
title = {Federated Learning for Healthcare: Systematic Review and Architecture Proposal},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3501813},
doi = {10.1145/3501813},
abstract = {The use of machine learning (ML) with electronic health records (EHR) is growing in popularity as a means to extract knowledge that can improve the decision-making process in healthcare. Such methods require training of high-quality learning models based on diverse and comprehensive datasets, which are hard to obtain due to the sensitive nature of medical data from patients. In this context, federated learning (FL) is a methodology that enables the distributed training of machine learning models with remotely hosted datasets without the need to accumulate data and, therefore, compromise it. FL is a promising solution to improve ML-based systems, better aligning them to regulatory requirements, improving trustworthiness and data sovereignty. However, many open questions must be addressed before the use of FL becomes widespread. This article aims at presenting a systematic literature review on current research about FL in the context of EHR data for healthcare applications. Our analysis highlights the main research topics, proposed solutions, case studies, and respective ML methods. Furthermore, the article discusses a general architecture for FL applied to healthcare data based on the main insights obtained from the literature review. The collected literature corpus indicates that there is extensive research on the privacy and confidentiality aspects of training data and model sharing, which is expected given the sensitive nature of medical data. Studies also explore improvements to the aggregation mechanisms required to generate the learning model from distributed contributions and case studies with different types of medical data.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {may},
articleno = {54},
numpages = {23},
keywords = {systematic review, federated learning, Electronic health records}
}

@article{carbon footprint,
  author    = {Xinchi Qiu and
               Titouan Parcollet and
               Javier Fern{\'{a}}ndez{-}Marqu{\'{e}}s and
               Pedro Porto Buarque de Gusm{\~{a}}o and
               Daniel J. Beutel and
               Taner Topal and
               Akhil Mathur and
               Nicholas D. Lane},
  title     = {A first look into the carbon footprint of federated learning},
  journal   = {CoRR},
  volume    = {abs/2102.07627},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.07627},
  eprinttype = {arXiv},
  eprint    = {2102.07627},
  timestamp = {Fri, 09 Apr 2021 15:00:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-07627.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{appleFL,
  author    = {Matthias Paulik and
               Matt Seigel and
               Henry Mason and
               Dominic Telaar and
               Joris Kluivers and
               Rogier C. van Dalen and
               Chi Wai Lau and
               Luke Carlson and
               Filip Granqvist and
               Chris Vandevelde and
               Sudeep Agarwal and
               Julien Freudiger and
               Andrew Byde and
               Abhishek Bhowmick and
               Gaurav Kapoor and
               Si Beaumont and
               {\'{A}}ine Cahill and
               Dominic Hughes and
               Omid Javidbakht and
               Fei Dong and
               Rehan Rishi and
               Stanley Hung},
  title     = {Federated Evaluation and Tuning for On-Device Personalization: System
               Design {\&} Applications},
  journal   = {CoRR},
  volume    = {abs/2102.08503},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.08503},
  eprinttype = {arXiv},
  eprint    = {2102.08503},
  timestamp = {Fri, 19 Feb 2021 11:02:14 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-08503.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{papaya,
  author    = {Dzmitry Huba and
               John Nguyen and
               Kshitiz Malik and
               Ruiyu Zhu and
               Mike Rabbat and
               Ashkan Yousefpour and
               Carole{-}Jean Wu and
               Hongyuan Zhan and
               Pavel Ustinov and
               Harish Srinivas and
               Kaikai Wang and
               Anthony Shoumikhin and
               Jesik Min and
               Mani Malek},
  title     = {Papaya: Practical, Private, and Scalable Federated Learning},
  journal   = {CoRR},
  volume    = {abs/2111.04877},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.04877},
  eprinttype = {arXiv},
  eprint    = {2111.04877},
  timestamp = {Tue, 19 Apr 2022 15:09:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-04877.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{moritz2018ray,
  title={Ray: A distributed framework for emerging $\{$AI$\}$ applications},
  author={Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I and others},
  booktitle={13th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 18)},
  pages={561--577},
  year={2018}
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th $\{$USENIX$\}$ symposium on operating systems design and implementation ($\{$OSDI$\}$ 16)},
  pages={265--283},
  year={2016}
}

@article{chen2015mxnet,
  title={Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems},
  author={Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
  journal={arXiv preprint arXiv:1512.01274},
  year={2015}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={8026--8037},
  year={2019}
}

@article{chaoyanghe2020fedml,
  Author = {He, Chaoyang and Li, Songze and So, Jinhyun and Zhang, Mi and Wang, Hongyi and Wang, Xiaoyang and Vepakomma, Praneeth and Singh, Abhishek and Qiu, Hang and Shen, Li and Zhao, Peilin and Kang, Yan and Liu, Yang and Raskar, Ramesh and Yang, Qiang and Annavaram, Murali and Avestimehr, Salman},
  Journal = {Advances in Neural Information Processing Systems, Best Paper Award at Federate Learning Workshop},
  Title = {FedML: A Research Library and Benchmark for Federated Machine Learning},
  Year = {2020}
}

@inproceedings{Oort-osdi21,
  title={Efficient Federated Learning via Guided Participant Selection},
  author={Fan Lai and Xiangfeng Zhu and Harsha V. Madhyastha and Mosharaf Chowdhury},
  booktitle={USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year={2021}
}

@inproceedings{
reddi2020adaptive,
title={Adaptive Federated Optimization},
author={Sashank J. Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone{\v{c}}n{\'y} and Sanjiv Kumar and Hugh Brendan McMahan},
booktitle={International Conference on Learning Representations},
year={2021},
}
@inproceedings{
reddi2020adaptive,
title={Adaptive Federated Optimization},
author={Sashank J. Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone{\v{c}}n{\'y} and Sanjiv Kumar and Hugh Brendan McMahan},
booktitle={International Conference on Learning Representations},
year={2021},
}

@inproceedings{yurochkin2019bayesian,
  title={Bayesian nonparametric federated learning of neural networks},
  author={Yurochkin, Mikhail and Agarwal, Mayank and Ghosh, Soumya and Greenewald, Kristjan and Hoang, Nghia and Khazaeni, Yasaman},
  booktitle={International Conference on Machine Learning},
  pages={7252--7261},
  year={2019},
  organization={PMLR}
}

@article{hsu2019measuring,
  title={Measuring the effects of non-identical data distribution for federated visual classification},
  author={Hsu, Tzu-Ming Harry and Qi, Hang and Brown, Matthew},
  journal={arXiv preprint arXiv:1909.06335},
  year={2019}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@misc{reddi2021adaptive,
      title={Adaptive Federated Optimization}, 
      author={Sashank Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Konečný and Sanjiv Kumar and H. Brendan McMahan},
      year={2021},
      eprint={2003.00295},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

%----------add manually

@misc{BHAA+21,
author	  =  	{Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`e}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
title	  =  	{On the Opportunities and Risks of Foundation Models},
year	  =  	{2021},
howpublished	  =  	{arXiv preprint arXiv:2108.07258},
url	  =  	{https://arxiv.org/abs/2108.07258}
}

@misc{zhang2023gptfl,
      title={GPT-FL: Generative Pre-trained Model-Assisted Federated Learning}, 
      author={Tuo Zhang and Tiantian Feng and Samiul Alam and Dimitrios Dimitriadis and Mi Zhang and Shrikanth S. Narayanan and Salman Avestimehr},
      year={2023},
      eprint={2306.02210},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{kirillov2023segany,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv:2304.02643},
  year={2023}
}

@misc{villalobos2022run,
      title={Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning}, 
      author={Pablo Villalobos and Jaime Sevilla and Lennart Heim and Tamay Besiroglu and Marius Hobbhahn and Anson Ho},
      year={2022},
      eprint={2211.04325},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{li2019privacypreserving,
      title={Privacy-preserving Federated Brain Tumour Segmentation}, 
      author={Wenqi Li and Fausto Milletarì and Daguang Xu and Nicola Rieke and Jonny Hancox and Wentao Zhu and Maximilian Baust and Yan Cheng and Sébastien Ourselin and M. Jorge Cardoso and Andrew Feng},
      year={2019},
      eprint={1910.00962},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhang2023privacypreserving,
      title={A Privacy-Preserving Hybrid Federated Learning Framework for Financial Crime Detection}, 
      author={Haobo Zhang and Junyuan Hong and Fan Dong and Steve Drew and Liangjie Xue and Jiayu Zhou},
      year={2023},
      eprint={2302.03654},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{10.1145/3394486.3403176,
author = {Muhammad, Khalil and Wang, Qinqin and O'Reilly-Morgan, Diarmuid and Tragos, Elias and Smyth, Barry and Hurley, Neil and Geraci, James and Lawlor, Aonghus},
title = {FedFast: Going Beyond Average for Faster Training of Federated Recommender Systems},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403176},
doi = {10.1145/3394486.3403176},
abstract = {Federated learning (FL) is quickly becoming the de facto standard for the distributed training of deep recommendation models, using on-device user data and reducing server costs. In a typical FL process, a central server tasks end-users to train a shared recommendation model using their local data. The local models are trained over several rounds on the users' devices and the server combines them into a global model, which is sent to the devices for the purpose of providing recommendations. Standard FL approaches use randomly selected users for training at each round, and simply average their local models to compute the global model. The resulting federated recommendation models require significant client effort to train and many communication rounds before they converge to a satisfactory accuracy. Users are left with poor quality recommendations until the late stages of training. We present a novel technique, FedFast, to accelerate distributed learning which achieves good accuracy for all users very early in the training process. We achieve this by sampling from a diverse set of participating clients in each training round and applying an active aggregation method that propagates the updated model to the other clients. Consequently, with FedFast the users benefit from far lower communication costs and more accurate models that can be consumed anytime during the training process even at the very early stages. We demonstrate the efficacy of our approach across a variety of benchmark datasets and in comparison to state-of-the-art recommendation techniques.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1234–1242},
numpages = {9},
keywords = {recommender systems, communication costs, faster training, active sampling, federated learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@misc{hard2019federated,
      title={Federated Learning for Mobile Keyboard Prediction}, 
      author={Andrew Hard and Kanishka Rao and Rajiv Mathews and Swaroop Ramaswamy and Françoise Beaufays and Sean Augenstein and Hubert Eichner and Chloé Kiddon and Daniel Ramage},
      year={2019},
      eprint={1811.03604},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{yosinski2014transferable,
  title={How transferable are features in deep neural networks?},
  author={Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@inproceedings{StochasticParrots,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ��},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}

@misc{lv2023parameter,
      title={Full Parameter Fine-tuning for Large Language Models with Limited Resources}, 
      author={Kai Lv and Yuqing Yang and Tengxiao Liu and Qinghui Gao and Qipeng Guo and Xipeng Qiu},
      year={2023},
      eprint={2306.09782},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{aroyo2021data,
      title={Data Excellence for AI: Why Should You Care}, 
      author={Lora Aroyo and Matthew Lease and Praveen Paritosh and Mike Schaekermann},
      year={2021},
      eprint={2111.10391},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{NIPS2017_d5e2c0ad,
 author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Reinforcement Learning from Human Preferences},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf},
 volume = {30},
 year = {2017}
}


@misc{mazumder2023dataperf,
      title={DataPerf: Benchmarks for Data-Centric AI Development}, 
      author={Mark Mazumder and Colby Banbury and Xiaozhe Yao and Bojan Karlaš and William Gaviria Rojas and Sudnya Diamos and Greg Diamos and Lynn He and Alicia Parrish and Hannah Rose Kirk and Jessica Quaye and Charvi Rastogi and Douwe Kiela and David Jurado and David Kanter and Rafael Mosquera and Juan Ciro and Lora Aroyo and Bilge Acun and Lingjiao Chen and Mehul Smriti Raje and Max Bartolo and Sabri Eyuboglu and Amirata Ghorbani and Emmett Goodman and Oana Inel and Tariq Kane and Christine R. Kirkpatrick and Tzu-Sheng Kuo and Jonas Mueller and Tristan Thrush and Joaquin Vanschoren and Margaret Warren and Adina Williams and Serena Yeung and Newsha Ardalani and Praveen Paritosh and Lilith Bat-Leah and Ce Zhang and James Zou and Carole-Jean Wu and Cody Coleman and Andrew Ng and Peter Mattson and Vijay Janapa Reddi},
      year={2023},
      eprint={2207.10062},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{https://doi.org/10.48550/arxiv.1806.00582,
  doi = {10.48550/ARXIV.1806.00582},
  
  url = {https://arxiv.org/abs/1806.00582},
  
  author = {Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Federated Learning with Non-IID Data},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
