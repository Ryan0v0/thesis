@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{A_Primer_on_Pretrained_Multilingual_LM,
  author       = {Sumanth Doddapaneni and
                  Gowtham Ramesh and
                  Anoop Kunchukuttan and
                  Pratyush Kumar and
                  Mitesh M. Khapra},
  title        = {A Primer on Pretrained Multilingual Language Models},
  journal      = {CoRR},
  volume       = {abs/2107.00676},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.00676},
  eprinttype    = {arXiv},
  eprint       = {2107.00676},
  timestamp    = {Wed, 07 Jul 2021 15:23:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-00676.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{prompt_tuning,
  author       = {Brian Lester and
                  Rami Al{-}Rfou and
                  Noah Constant},
  title        = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  journal      = {CoRR},
  volume       = {abs/2104.08691},
  year         = {2021},
  url          = {https://arxiv.org/abs/2104.08691},
  eprinttype    = {arXiv},
  eprint       = {2104.08691},
  timestamp    = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2104-08691.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Weller2022PretrainedMF,
  title={Pretrained Models for Multilingual Federated Learning},
  author={Orion Weller and Marc Marone and Vladimir Braverman and Dawn J Lawrie and Benjamin Van Durme},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
  year={2022}
}

@misc{xue2020mt5,
    title = {{mT5}: A massively multilingual pre-trained text-to-text transformer},
    author = {Linting Xue and Noah Constant and Adam Roberts and Mihir Kale and Rami Al-Rfou and Aditya Siddhant and Aditya Barua and Colin Raffel},
    year = {2020},
    eprint = {2010.11934},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL}
}

@article{tirumala2022memorization,
  title={Memorization without overfitting: Analyzing the training dynamics of large language models},
  author={Tirumala, Kushal and Markosyan, Aram and Zettlemoyer, Luke and Aghajanyan, Armen},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38274--38290},
  year={2022}
}

@inproceedings{sitaram-etal-2023-everything,
    title = "Everything you need to know about Multilingual {LLM}s: Towards fair, performant and reliable models for languages of the world",
    author = "Sitaram, Sunayana  and
      Choudhury, Monojit  and
      Patra, Barun  and
      Chaudhary, Vishrav  and
      Ahuja, Kabir  and
      Bali, Kalika",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-tutorials.3",
    doi = "10.18653/v1/2023.acl-tutorials.3",
    pages = "21--26",
    abstract = "This tutorial will describe various aspects of scaling up language technologies to many of the world{'}s languages by describing the latest research in Massively Multilingual Language Models (MMLMs). We will cover topics such as data collection, training and fine-tuning of models, Responsible AI issues such as fairness, bias and toxicity, linguistic diversity and evaluation in the context of MMLMs, specifically focusing on issues in non-English and low-resource languages. Further, we will also talk about some of the real-world challenges in deploying these models in language communities in the field. With the performance of MMLMs improving in the zero-shot setting for many languages, it is now becoming feasible to use them for building language technologies in many languages of the world, and this tutorial will provide the computational linguistics community with unique insights from the latest research in multilingual models.",
}

@article{konevcny2016federated2,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@inproceedings{fl1,
  title={Privacy-preserving deep learning},
  author={Shokri, Reza and Shmatikov, Vitaly},
  booktitle={Proceedings of the 22nd ACM SIGSAC conference on computer and communications security},
  pages={1310--1321},
  year={2015}
}

@inproceedings{fedavg,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  year={2017},
  organization={PMLR}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{chowdhery2022palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  year={2022}
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@inproceedings{Prefix-Tuning,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
}

@article{p-tuning,
    title={GPT Understands, Too},
    author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
    journal={arXiv:2103.10385},
    year={2021}
    }


@InProceedings{adapter,
  title = 	 {Parameter-Efficient Transfer Learning for {NLP}},
  author =       {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2790--2799},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/houlsby19a.html},
  abstract = 	 {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to $26$ diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within $0.8%$ of the performance of full fine-tuning, adding only $3.6%$ parameters per task. By contrast, fine-tuning trains $100%$ of the parameters per task.}
}


@article{lora,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Weizhu Chen},
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2106.09685},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.09685},
  eprinttype    = {arXiv},
  eprint       = {2106.09685},
  timestamp    = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-09685.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bitfit,
    title = "{B}it{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
    author = "Ben Zaken, Elad  and
      Goldberg, Yoav  and
      Ravfogel, Shauli",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.1",
    doi = "10.18653/v1/2022.acl-short.1",
    pages = "1--9",
    abstract = "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
}

@misc{flower,
  doi = {10.48550/ARXIV.2007.14390},
  
  url = {https://arxiv.org/abs/2007.14390},
  
  author = {Beutel, Daniel J. and Topal, Taner and Mathur, Akhil and Qiu, Xinchi and Fernandez-Marques, Javier and Gao, Yan and Sani, Lorenzo and Li, Kwing Hei and Parcollet, Titouan and de Gusmão, Pedro Porto Buarque and Lane, Nicholas D.},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Flower: A Friendly Federated Learning Research Framework},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{huggingface-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}

@inproceedings{pytorch,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{adamw,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Decoupled Weight Decay Regularization},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=Bkg6RiCqY7},
  timestamp    = {Thu, 25 Jul 2019 14:26:04 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/LoshchilovH19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{adam,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1412.6980},
  timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lang2vec,
  title={Uriel and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors},
  author={Littell, Patrick and Mortensen, David R and Lin, Ke and Kairis, Katherine and Turner, Carlisle and Levin, Lori},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  volume={2},
  pages={8--14},
  year={2017}
}

@article{XGLUE,
  title={XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation},
  author={Yaobo Liang and Nan Duan and Yeyun Gong and Ning Wu and Fenfei Guo and Weizhen Qi and Ming Gong and Linjun Shou and Daxin Jiang and Guihong Cao and Xiaodong Fan and Ruofei Zhang and Rahul Agrawal and Edward Cui and Sining Wei and Taroon Bharti and Ying Qiao and Jiun-Hung Chen and Winnie Wu and Shuguang Liu and Fan Yang and Daniel Campos and Rangan Majumder and Ming Zhou},
  journal={arXiv},
  year={2020},
  volume={abs/2004.01401}
}

@inproceedings{XNLI,
  title={XNLI: Evaluating Cross-lingual Sentence Representations},
  author={Alexis Conneau and Guillaume Lample and Ruty Rinott and Adina Williams and Samuel R. Bowman and Holger Schwenk and Veselin Stoyanov},
  booktitle={EMNLP},
  year={2018}
}

@inproceedings{XLM-R,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
    abstract = "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}

@inproceedings{mBERT,
    title = "How Multilingual is Multilingual {BERT}?",
    author = "Pires, Telmo  and
      Schlinger, Eva  and
      Garrette, Dan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1493",
    doi = "10.18653/v1/P19-1493",
    pages = "4996--5001",
    abstract = "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",
}

@inbook{XLM,
author = {Conneau, Alexis and Lample, Guillaume},
title = {Cross-Lingual Language Model Pretraining},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsu-pervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models are publicly available.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {634},
numpages = {11}
}

@inproceedings{mT5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498",
    abstract = "The recent {``}Text-to-Text Transfer Transformer{''} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {``}accidental translation{''} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
}

@article{SeamlessM4T,
  title={SeamlessM4T—Massively Multilingual \& Multimodal Machine Translation},
  author={{Seamless Communication}, Lo\"{i}c Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye,  Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-juss\`{a} \footnotemark[3], Onur \,{C}elebi,Maha Elbayad,Cynthia Gao, Francisco Guzm\'an, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang},
  journal={ArXiv},
  year={2023}
}

@inproceedings{wu-dredze-2020-languages,
    title = "Are All Languages Created Equal in Multilingual {BERT}?",
    author = "Wu, Shijie  and
      Dredze, Mark",
    booktitle = "Proceedings of the 5th Workshop on Representation Learning for NLP",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.repl4nlp-1.16",
    doi = "10.18653/v1/2020.repl4nlp-1.16",
    pages = "120--130",
    abstract = "Multilingual BERT (mBERT) trained on 104 languages has shown surprisingly good cross-lingual performance on several NLP tasks, even without explicit cross-lingual signals. However, these evaluations have focused on cross-lingual transfer with high-resource languages, covering only a third of the languages covered by mBERT. We explore how mBERT performs on a much wider set of languages, focusing on the quality of representation for low-resource languages, measured by within-language performance. We consider three tasks: Named Entity Recognition (99 languages), Part-of-speech Tagging and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data.",
}

@article{hu2020xtreme,
      author    = {Junjie Hu and Sebastian Ruder and Aditya Siddhant and Graham Neubig and Orhan Firat and Melvin Johnson},
      title     = {XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization},
      journal   = {CoRR},
      volume    = {abs/2003.11080},
      year      = {2020},
      archivePrefix = {arXiv},
      eprint    = {2003.11080}
}

@inproceedings{lauscher-etal-2020-zero,
    title = "From Zero to Hero: {O}n the Limitations of Zero-Shot Language Transfer with Multilingual {T}ransformers",
    author = "Lauscher, Anne  and
      Ravishankar, Vinit  and
      Vuli{\'c}, Ivan  and
      Glava{\v{s}}, Goran",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.363",
    doi = "10.18653/v1/2020.emnlp-main.363",
    pages = "4483--4499",
    abstract = "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.",
}

@inproceedings{artetxe-etal-2020-cross,
    title = "On the Cross-lingual Transferability of Monolingual Representations",
    author = "Artetxe, Mikel  and
      Ruder, Sebastian  and
      Yogatama, Dani",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.421",
    doi = "10.18653/v1/2020.acl-main.421",
    pages = "4623--4637",
    abstract = "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
}

@inproceedings{ebrahimi-etal-2022-americasnli,
    title = "{A}mericas{NLI}: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages",
    author = "Ebrahimi, Abteen  and
      Mager, Manuel  and
      Oncevay, Arturo  and
      Chaudhary, Vishrav  and
      Chiruzzo, Luis  and
      Fan, Angela  and
      Ortega, John  and
      Ramos, Ricardo  and
      Rios, Annette  and
      Meza Ruiz, Ivan Vladimir  and
      Gim{\'e}nez-Lugo, Gustavo  and
      Mager, Elisabeth  and
      Neubig, Graham  and
      Palmer, Alexis  and
      Coto-Solano, Rolando  and
      Vu, Thang  and
      Kann, Katharina",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.435",
    doi = "10.18653/v1/2022.acl-long.435",
    pages = "6279--6299",
    }


@inproceedings{mager2021findings,
  title={Findings of the AmericasNLP 2021 shared task on open machine translation for indigenous languages of the Americas},
  author={Mager, Manuel and Oncevay, Arturo and Ebrahimi, Abteen and Ortega, John and Gonzales, Annette Rios and Fan, Angela and Gutierrez-Vasques, Ximena and Chiruzzo, Luis and Gim{\'e}nez-Lugo, Gustavo and Ramos, Ricardo and others},
  booktitle={Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas},
  pages={202--217},
  year={2021}
}

@article{muhammad2022naijasenti,
  title={Naijasenti: A nigerian twitter sentiment corpus for multilingual sentiment analysis},
  author={Muhammad, Shamsuddeen Hassan and Adelani, David Ifeoluwa and Ruder, Sebastian and Ahmad, Ibrahim Said and Abdulmumin, Idris and Bello, Bello Shehu and Choudhury, Monojit and Emezue, Chris Chinenye and Abdullahi, Saheed Salahudeen and Aremu, Anuoluwapo and others},
  journal={arXiv preprint arXiv:2201.08277},
  year={2022}
}

@inproceedings{akera2022machine,
  title={Machine translation for african languages: Community creation of datasets and models in uganda},
  author={Akera, Benjamin and Mukiibi, Jonathan and Naggayi, Lydia Sanyu and Babirye, Claire and Owomugisha, Isaac and Nsumba, Solomon and Nakatumba-Nabende, Joyce and Bainomugisha, Engineer and Mwebaze, Ernest and Quinn, John},
  booktitle={3rd Workshop on African Natural Language Processing},
  year={2022}
}

@article{adelani2021masakhaner,
  title={MasakhaNER: Named entity recognition for African languages},
  author={Adelani, David Ifeoluwa and Abbott, Jade and Neubig, Graham and D’souza, Daniel and Kreutzer, Julia and Lignos, Constantine and Palen-Michel, Chester and Buzaaba, Happy and Rijhwani, Shruti and Ruder, Sebastian and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1116--1131},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@inproceedings{ansell2022composable,
  title={Composable Sparse Fine-Tuning for Cross-Lingual Transfer},
  author={Ansell, Alan and Ponti, Edoardo and Korhonen, Anna and Vuli{\'c}, Ivan},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1778--1796},
  year={2022}
}

@inproceedings{adebara-abdul-mageed-2022-towards,
    title = "Towards Afrocentric {NLP} for {A}frican Languages: Where We Are and Where We Can Go",
    author = "Adebara, Ife  and
      Abdul-Mageed, Muhammad",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.265",
    doi = "10.18653/v1/2022.acl-long.265",
    pages = "3814--3841",
    abstract = "Aligning with ACL 2022 special Theme on {``}Language Diversity: from Low Resource to Endangered Languages{''}, we discuss the major linguistic and sociopolitical challenges facing development of NLP technologies for African languages. Situating African languages in a typological framework, we discuss how the particulars of these languages can be harnessed. To facilitate future research, we also highlight current efforts, communities, venues, datasets, and tools. Our main objective is to motivate and advocate for an Afrocentric approach to technology development. With this in mind, we recommend \textit{what} technologies to build and \textit{how} to build, evaluate, and deploy them based on the needs of local African communities.",
}

@article{chen2023improving,
  title={Improving Language Plasticity via Pretraining with Active Forgetting},
  author={Chen, Yihong and Marchisio, Kelly and Raileanu, Roberta and Adelani, David Ifeoluwa and Stenetor, Pontus and Riedel, Sebastian and Artetx, Mikel},
  journal={NeurIPS 2023},
  year={2023}
}

@article{marchisio2022mini,
  title={Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training},
  author={Marchisio, Kelly and Lewis, Patrick and Chen, Yihong and Artetxe, Mikel},
  journal={ACL 2023, Findings of the Association for Computational Linguistics},
  year={2022}
}
@inproceedings{chau-etal-2020-parsing,
    title = "Parsing with Multilingual {BERT}, a Small Corpus, and a Small Treebank",
    author = "Chau, Ethan C.  and
      Lin, Lucy H.  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.118",
    doi = "10.18653/v1/2020.findings-emnlp.118",
    pages = "1324--1334",
    abstract = "Pretrained multilingual contextual representations have shown great success, but due to the limits of their pretraining data, their benefits do not apply equally to all language varieties. This presents a challenge for language varieties unfamiliar to these models, whose labeled and unlabeled data is too limited to train a monolingual model effectively. We propose the use of additional language-specific pretraining and vocabulary augmentation to adapt multilingual models to low-resource settings. Using dependency parsing of four diverse low-resource language varieties as a case study, we show that these methods significantly improve performance over baselines, especially in the lowest-resource cases, and demonstrate the importance of the relationship between such models{'} pretraining data and target language varieties.",
}

@inproceedings{pfeiffer-etal-2020-mad,
    title = "{MAD-X}: {A}n {A}dapter-{B}ased {F}ramework for {M}ulti-{T}ask {C}ross-{L}ingual {T}ransfer",
    author = "Pfeiffer, Jonas  and
      Vuli{\'c}, Ivan  and
      Gurevych, Iryna  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.617",
    doi = "10.18653/v1/2020.emnlp-main.617",
    pages = "7654--7673",
    abstract = "The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.",
}

@inproceedings{ponti-etal-2020-xcopa,
    title = "{XCOPA}: A Multilingual Dataset for Causal Commonsense Reasoning",
    author = "Ponti, Edoardo Maria  and
      Glava{\v{s}}, Goran  and
      Majewska, Olga  and
      Liu, Qianchu  and
      Vuli{\'c}, Ivan  and
      Korhonen, Anna",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.185",
    doi = "10.18653/v1/2020.emnlp-main.185",
    pages = "2362--2376",
    abstract = "In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects. Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences. Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apur{\'\i}mac Quechua and Haitian Creole. We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer. Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.",
}

@inproceedings{visualloss,
  title={Visualizing the Loss Landscape of Neural Nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Neural Information Processing Systems},
  year={2018}
}

@inproceedings{Izmailov2018AveragingWL,
  title={Averaging Weights Leads to Wider Optima and Better Generalization},
  author={Pavel Izmailov and Dmitrii Podoprikhin and T. Garipov and Dmitry P. Vetrov and Andrew Gordon Wilson},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:3833416}
}

@article{Keskar2016,
	author = {Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy and Ping Tak Peter Tang},
	title = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
	journal = {arXiv preprint arXiv:1609.04836},
	year = {2016}
}

@inproceedings{fedavg,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}
@article{fedprox,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={429--450},
  year={2020}
}

@article{yang2019federated,
  title={Federated machine learning: Concept and applications},
  author={Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={10},
  number={2},
  pages={1--19},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@inproceedings{learnedvector,
    title = {Learning Language Representations for Typology Prediction},
    author = {Malaviya, Chaitanya and Neubig, Graham and Littell, Patrick},
    booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    address = {Copenhagen, Denmark},
    month = {September},
    year = {2017}
}

@article{lim2020federated,
  title={Federated learning in mobile edge networks: A comprehensive survey},
  author={Lim, Wei Yang Bryan and Luong, Nguyen Cong and Hoang, Dinh Thai and Jiao, Yutao and Liang, Ying-Chang and Yang, Qiang and Niyato, Dusit and Miao, Chunyan},
  journal={IEEE Communications Surveys \& Tutorials},
  volume={22},
  number={3},
  pages={2031--2063},
  year={2020},
  publisher={IEEE}
}



@article{leaf,
  title={Leaf: A benchmark for federated settings},
  author={Caldas, Sebastian and Duddu, Sai Meher Karthik and Wu, Peter and Li, Tian and Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Smith, Virginia and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:1812.01097},
  year={2018}
}

@inproceedings{scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International conference on machine learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

@Misc{PEFT,
  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}

@inproceedings{FedKC,
author = {Wang, Haoyu and Zhao, Handong and Wang, Yaqing and Yu, Tong and Gu, Jiuxiang and Gao, Jing},
title = {FedKC: Federated Knowledge Composition for Multilingual Natural Language Understanding},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511988},
doi = {10.1145/3485447.3511988},
abstract = {Multilingual natural language understanding, which aims to comprehend multilingual documents, is an important task. Existing efforts have been focusing on the analysis of centrally stored text data, but in real practice, multilingual data is usually distributed. Federated learning is a promising paradigm to solve this problem, which trains local models with decentralized data on local clients and aggregates local models on the central server to achieve a good global model. However, existing federated learning methods assume that data are independent and identically distributed (IID), and cannot handle multilingual data, that are usually non-IID with severely skewed distributions: First, multilingual data is stored on local client devices such that there are only monolingual or bilingual data stored on each client. This makes it difficult for local models to know the information of documents in other languages. Second, the distribution over different languages could be skewed. High resource language data is much more abundant than low resource language data. The model trained on such skewed data may focus more on high resource languages but fail to consider the key information of low resource languages. To solve the aforementioned challenges of multilingual federated NLU, we propose a plug-and-play knowledge composition&nbsp;(KC) module, called FedKC, which exchanges knowledge among clients without sharing raw data. Specifically, we propose an effective way to calculate a consistency loss defined based on the shared knowledge across clients, which enables models trained on different clients achieve similar predictions on similar data. Leveraging this consistency loss, joint training is thus conducted on distributed data respecting the privacy constraints. We also analyze the potential risk of FedKC and provide theoretical bound to show that it is difficult to recover data from the corrupted data. We conduct extensive experiments on three public multilingual datasets for three typical NLU tasks, including paraphrase identification, question answering matching, and news classification. The experiment results show that the proposed FedKC can outperform state-of-the-art baselines on the three datasets significantly.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {1839–1850},
numpages = {12},
keywords = {Federated learning, Multilingual natural language understanding},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{FedNLP,
    title = "{F}ed{NLP}: Benchmarking Federated Learning Methods for Natural Language Processing Tasks",
    author = "Lin, Bill Yuchen  and
      He, Chaoyang  and
      Ze, Zihang  and
      Wang, Hulin  and
      Hua, Yufen  and
      Dupuy, Christophe  and
      Gupta, Rahul  and
      Soltanolkotabi, Mahdi  and
      Ren, Xiang  and
      Avestimehr, Salman",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.13",
    doi = "10.18653/v1/2022.findings-naacl.13",
    pages = "157--175",
    abstract = "Increasing concerns and regulations about data privacy and sparsity necessitate the study of privacy-preserving, decentralized learning methods for natural language processing (NLP) tasks. Federated learning (FL) provides promising approaches for a large number of clients (e.g., personal devices or organizations) to collaboratively learn a shared global model to benefit all clients while allowing users to keep their data locally. Despite interest in studying FL methods for NLP tasks, a systematic comparison and analysis is lacking in the literature. Herein, we present the FedNLP, a benchmarking framework for evaluating federated learning methods on four different task formulations: text classification, sequence tagging, question answering, and seq2seq. We propose a universal interface between Transformer-based language models (e.g., BERT, BART) and FL methods (e.g., FedAvg, FedOPT, etc.) under various non-IID partitioning strategies. Our extensive experiments with FedNLP provide empirical comparisons between FL methods and help us better understand the inherent challenges of this direction. The comprehensive analysis points to intriguing and exciting future research aimed at developing FL methods for NLP tasks.",
}

@article{BANABILAH2022103061,
title = {Federated learning review: Fundamentals, enabling technologies, and future applications},
journal = {Information Processing & Management},
volume = {59},
number = {6},
pages = {103061},
year = {2022},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103061},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322001649},
author = {Syreen Banabilah and Moayad Aloqaily and Eitaa Alsayed and Nida Malik and Yaser Jararweh},
keywords = {Federated learning, Decentralized learning, Distributed learning, Machine learning, Mobile edge networks, Data privacy, Data security},
abstract = {Federated Learning (FL) has been foundational in improving the performance of a wide range of applications since it was first introduced by Google. Some of the most prominent and commonly used FL-powered applications are Android’s Gboard for predictive text and Google Assistant. FL can be defined as a setting that makes on-device, collaborative Machine Learning possible. A wide range of literature has studied FL technical considerations, frameworks, and limitations with several works presenting a survey of the prominent literature on FL. However, prior surveys have focused on technical considerations and challenges of FL, and there has been a limitation in more recent work that presents a comprehensive overview of the status and future trends of FL in applications and markets. In this survey, we introduce the basic fundamentals of FL, describing its underlying technologies, architectures, system challenges, and privacy-preserving methods. More importantly, the contribution of this work is in scoping a wide variety of FL current applications and future trends in technology and markets today. We present a classification and clustering of literature progress in FL in application to technologies including Artificial Intelligence, Internet of Things, blockchain, Natural Language Processing, autonomous vehicles, and resource allocation, as well as in application to market use cases in domains of Data Science, healthcare, education, and industry. We discuss future open directions and challenges in FL within recommendation engines, autonomous vehicles, IoT, battery management, privacy, fairness, personalization, and the role of FL for governments and public sectors. By presenting a comprehensive review of the status and prospects of FL, this work serves as a reference point for researchers and practitioners to explore FL applications under a wide range of domains.}
}

@inproceedings{pires-etal-2019-multilingual,
    title = "How Multilingual is Multilingual {BERT}?",
    author = "Pires, Telmo  and
      Schlinger, Eva  and
      Garrette, Dan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1493",
    doi = "10.18653/v1/P19-1493",
    pages = "4996--5001",
    abstract = "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",
}

@inproceedings{wu-dredze-2019-beto,
    title = "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of {BERT}",
    author = "Wu, Shijie  and
      Dredze, Mark",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1077",
    doi = "10.18653/v1/D19-1077",
    pages = "833--844",
    abstract = "Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.",
}

@inproceedings{artetxe-etal-2020-cross,
    title = "On the Cross-lingual Transferability of Monolingual Representations",
    author = "Artetxe, Mikel  and
      Ruder, Sebastian  and
      Yogatama, Dani",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.421",
    doi = "10.18653/v1/2020.acl-main.421",
    pages = "4623--4637",
    abstract = "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
}

@inproceedings{xu-etal-2020-understanding,
    title = "Understanding Pre-trained {BERT} for Aspect-based Sentiment Analysis",
    author = "Xu, Hu  and
      Shu, Lei  and
      Yu, Philip  and
      Liu, Bing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.21",
    doi = "10.18653/v1/2020.coling-main.21",
    pages = "244--250",
    abstract = "This paper analyzes the pre-trained hidden representations learned from reviews on BERT for tasks in aspect-based sentiment analysis (ABSA). Our work is motivated by the recent progress in BERT-based language models for ABSA. However, it is not clear how the general proxy task of (masked) language model trained on unlabeled corpus without annotations of aspects or opinions can provide important features for downstream tasks in ABSA. By leveraging the annotated datasets in ABSA, we investigate both the attentions and the learned representations of BERT pre-trained on reviews. We found that BERT uses very few self-attention heads to encode context words (such as prepositions or pronouns that indicating an aspect) and opinion words for an aspect. Most features in the representation of an aspect are dedicated to the fine-grained semantics of the domain (or product category) and the aspect itself, instead of carrying summarized opinions from its context. We hope this investigation can help future research in improving self-supervised learning, unsupervised learning and fine-tuning for ABSA. The pre-trained model and code can be found at \url{https://github.com/howardhsu/BERT-for-RRC-ABSA}.",
}

@inproceedings{chung-etal-2020-improving,
    title = "Improving Multilingual Models with Language-Clustered Vocabularies",
    author = "Chung, Hyung Won  and
      Garrette, Dan  and
      Tan, Kiat Chuan  and
      Riesa, Jason",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.367",
    doi = "10.18653/v1/2020.emnlp-main.367",
    pages = "4536--4546",
    abstract = "State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications. In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies. Our experiments show improvements across languages on key multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1{\%}), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.",
}

@misc{chowdhery2022palm,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liupengfei,
  author       = {Pengfei Liu and
                  Weizhe Yuan and
                  Jinlan Fu and
                  Zhengbao Jiang and
                  Hiroaki Hayashi and
                  Graham Neubig},
  title        = {Pre-train, Prompt, and Predict: {A} Systematic Survey of Prompting
                  Methods in Natural Language Processing},
  journal      = {CoRR},
  volume       = {abs/2107.13586},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.13586},
  eprinttype    = {arXiv},
  eprint       = {2107.13586},
  timestamp    = {Tue, 03 Aug 2021 14:53:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-13586.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{schick-schutze-2022-true,
    title = "True Few-Shot Learning with {P}rompts{---}{A} Real-World Perspective",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.41",
    doi = "10.1162/tacl_a_00485",
    pages = "716--731",
    abstract = "Prompt-based approaches excel at few-shot learning. However, Perez et al. (2021) recently cast doubt on their performance as they had difficulty getting good results in a {``}true{''} few-shot setting in which prompts and hyperparameters cannot be tuned on a dev set. In view of this, we conduct an extensive study of Pet, a method that combines textual instructions with example-based finetuning. We show that, if correctly configured, Pet performs strongly in true few-shot settings without a dev set. Crucial for this strong performance is a number of design choices, including Pet{'}s ability to intelligently handle multiple prompts. We put our findings to a real-world test by running Pet on RAFT, a benchmark of tasks taken from realistic NLP applications for which no labeled dev or test sets are available. Pet achieves a new state of the art on RAFT and performs close to non-expert humans for 7 out of 11 tasks. These results demonstrate that prompt-based learners can successfully be applied in true few-shot settings and underpin our belief that learning from instructions will play an important role on the path towards human-like few-shot learning capabilities.",
}

@inproceedings{autoprompt,
  author = {Taylor Shin and Yasaman Razeghi and Robert L. Logan IV and Eric Wallace and Sameer Singh},
  title = { {AutoPrompt}: Eliciting Knowledge from Language Models with Automatically Generated Prompts },
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2020}
}

%--------------------vFedSec:

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2018}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


@INPROCEEDINGS{9634881,
  author={Zhang, Junpeng and Li, Mengqian and Zeng, Shuiguang and Xie, Bin and Zhao, Dongmei},
  booktitle={2021 International Conference on Networking and Network Applications (NaNA)}, 
  title={A survey on security and privacy threats to federated learning}, 
  year={2021},
  volume={},
  number={},
  pages={319-326},
  doi={10.1109/NaNA53684.2021.00062}
  }

  @article{konevcny2016federated1,
  title={Federated optimization: Distributed machine learning for on-device intelligence},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Ramage, Daniel and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1610.02527},
  year={2016}
}

@misc{phe,
  author = {CSIRO's Data61},
  title = {Python Paillier Library},
  year = {2013},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/data61/python-paillier}},
}

@misc{sealpy,
  author = {Huelse},
  title = {Microsoft SEAL For Python},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/Huelse/SEAL-Python}},
}

@misc{pybind11,
   author = {Wenzel Jakob and Jason Rhinelander and Dean Moldovan},
   year = {2017},
   note = {https://github.com/pybind/pybind11},
   title = {pybind11 -- Seamless operability between C++11 and Python}
}

@misc{sealcrypto,
    title = {{M}icrosoft {SEAL} (release 4.1)},
    howpublished = {\url{https://github.com/Microsoft/SEAL}},
    month = jan,
    year = 2023,
    note = {Microsoft Research, Redmond, WA.},
    key = {SEAL}
}

@article{zhou2021psi,
  title={Privacy-preserving federated learning framework with general aggregation and multiparty entity matching},
  author={Zhou, Zhou and Tian, Youliang and Peng, Changgen},
  journal={Wireless Communications and Mobile Computing},
  volume={2021},
  pages={1--14},
  year={2021},
  publisher={Hindawi Limited}
}
@inproceedings{lu2020psi,
  title={Multi-party private set intersection in vertical federated learning},
  author={Lu, Linpeng and Ding, Ning},
  booktitle={2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)},
  pages={707--714},
  year={2020},
  organization={IEEE}
}

@article{konevcny2016federated2,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@misc{tf_experiment,
    author = {Tensorflow Federated},
    title = {Federated Learning with Differential Privacy},
    year = {2022},
    howpublished = {\url{https://www.tensorflow.org/federated/tutorials/federated_learning_with_differential_privacy}}
}%   note = "accessed 25-Augr-22"

@inproceedings{carlini2019secret,
  title={The secret sharer: Evaluating and testing unintended memorization in neural networks},
  author={Carlini, Nicholas and Liu, Chang and Erlingsson, {\'U}lfar and Kos, Jernej and Song, Dawn},
  booktitle={28th USENIX Security Symposium (USENIX Security 19)},
  pages={267--284},
  year={2019}
}

@article{kasiviswanathan2011can,
  title={What can we learn privately?},
  author={Kasiviswanathan, Shiva Prasad and Lee, Homin K and Nissim, Kobbi and Raskhodnikova, Sofya and Smith, Adam},
  journal={SIAM Journal on Computing},
  volume={40},
  number={3},
  pages={793--826},
  year={2011},
  publisher={SIAM}
}

@inproceedings{kairouz2016discrete,
  title={Discrete distribution estimation under local privacy},
  author={Kairouz, Peter and Bonawitz, Keith and Ramage, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={2436--2444},
  year={2016},
  organization={PMLR}
}


@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{abadi2016tensorflow,
  title={$\{$TensorFlow$\}$: A System for $\{$Large-Scale$\}$ Machine Learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th USENIX symposium on operating systems design and implementation (OSDI 16)},
  pages={265--283},
  year={2016}
}

@article{beutel2020flower,
  title={Flower: A friendly federated learning research framework},
  author={Beutel, Daniel J and Topal, Taner and Mathur, Akhil and Qiu, Xinchi and Parcollet, Titouan and de Gusm{\~a}o, Pedro PB and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2007.14390},
  year={2020}
}

@article{li2019fair,
  title={Fair resource allocation in federated learning},
  author={Li, Tian and Sanjabi, Maziar and Beirami, Ahmad and Smith, Virginia},
  journal={arXiv preprint arXiv:1905.10497},
  year={2019}
}
@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE symposium on security and privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}
@inproceedings{fredrikson2015model,
  title={Model inversion attacks that exploit confidence information and basic countermeasures},
  author={Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
  booktitle={Proceedings of the 22nd ACM SIGSAC conference on computer and communications security},
  pages={1322--1333},
  year={2015}
}

@inproceedings{melis2019exploiting,
  title={Exploiting unintended feature leakage in collaborative learning},
  author={Melis, Luca and Song, Congzheng and De Cristofaro, Emiliano and Shmatikov, Vitaly},
  booktitle={2019 IEEE symposium on security and privacy (SP)},
  pages={691--706},
  year={2019},
  organization={IEEE}
}

@article{mcmahan2017learning,
  title={Learning differentially private recurrent language models},
  author={McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  journal={arXiv preprint arXiv:1710.06963},
  year={2017}
}

@article{andrew2021differentially,
  title={Differentially private learning with adaptive clipping},
  author={Andrew, Galen and Thakkar, Om and McMahan, Brendan and Ramaswamy, Swaroop},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{nilsonreport,
 author = {Ullmann, Julian R.},
 title = {Bit-vector Algorithms for Binary Constraint Satisfaction and Subgraph Isomorphism},
 journal = {J. Exp. Algorithmics},
 issue_date = {2010},
 volume = {15},
 month = feb,
 year = {2011},
 issn = {1084-6654},
 pages = {1.6:1.1--1.6:1.64},
 articleno = {1.6},
 numpages = {1.54},
 url = {http://doi.acm.org/10.1145/1671970.1921702},
 doi = {10.1145/1671970.1921702},
 acmid = {1921702},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AllDifferent constraint, backtrack, binary constraints, bit-vector, constraint propagation, constraint satisfaction, domain reduction, focus search, forward checking, graph indexing, molecule matching, prematching, signature file, subgraph isomorphism},
}
@misc{ukfinance,
  title = {Fraud- The Facts 2021:
The Definite Overview of Payment Industry Fraud},
  author = {UK Finance},
  note  =  {\url{https://www.ukfinance.org.uk/system/files/Fraud%20The%20Facts%202021-%20FINAL.pdf}},
  note = {Accessed: 14-Sep-2021}
}
@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}
@article{pang2021deep,
  title={Deep learning for anomaly detection: A review},
  author={Pang, Guansong and Shen, Chunhua and Cao, Longbing and Hengel, Anton Van Den},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={2},
  pages={1--38},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@article{chandola2009anomaly,
  title={Anomaly detection: A survey},
  author={Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  journal={ACM computing surveys (CSUR)},
  volume={41},
  number={3},
  pages={1--58},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@software{np_galois,
   title = {{Galois: A performant NumPy extension for Galois fields}},
   author = {Hostetter, Matt},
   month = {11},
   year = {2020},
   url = {https://github.com/mhostetter/galois},
}

@article{he2020fedml,
  title={Fedml: A research library and benchmark for federated machine learning},
  author={He, Chaoyang and Li, Songze and So, Jinhyun and Zeng, Xiao and Zhang, Mi and Wang, Hongyi and Wang, Xiaoyang and Vepakomma, Praneeth and Singh, Abhishek and Qiu, Hang and others},
  journal={arXiv preprint arXiv:2007.13518},
  year={2020}
}

@inproceedings{cld_cmp_privacy,
  title={Security and privacy in cloud computing: A survey},
  author={Zhou, Minqi and Zhang, Rong and Xie, Wei and Qian, Weining and Zhou, Aoying},
  booktitle={2010 Sixth International Conference on Semantics, Knowledge and Grids},
  pages={105--112},
  year={2010},
  organization={IEEE}
}
@inproceedings{bonawitz2017secagg,
  title={Practical secure aggregation for privacy-preserving machine learning},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1175--1191},
  year={2017}
}
@inproceedings{bell2020secaggplus,
  title={Secure single-server aggregation with (poly) logarithmic overhead},
  author={Bell, James Henry and Bonawitz, Kallista A and Gasc{\'o}n, Adri{\`a} and Lepoint, Tancr{\`e}de and Raykova, Mariana},
  booktitle={Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1253--1269},
  year={2020}
}

@article{shamir1979share,
  title={How to share a secret},
  author={Shamir, Adi},
  journal={Communications of the ACM},
  volume={22},
  number={11},
  pages={612--613},
  year={1979},
  publisher={ACm New York, NY, USA}
}

@inproceedings{fedavg,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{mothukuri2021survey,
  title={A survey on security and privacy of federated learning},
  author={Mothukuri, Viraaji and Parizi, Reza M and Pouriyeh, Seyedamin and Huang, Yan and Dehghantanha, Ali and Srivastava, Gautam},
  journal={Future Generation Computer Systems},
  volume={115},
  pages={619--640},
  year={2021},
  publisher={Elsevier}
}

@article{papernot2016ml_privacy,
  title={Towards the science of security and privacy in machine learning},
  author={Papernot, Nicolas and McDaniel, Patrick and Sinha, Arunesh and Wellman, Michael},
  journal={arXiv preprint arXiv:1611.03814},
  year={2016}
}

@article{liu2021ml_privacy,
  title={When machine learning meets privacy: A survey and outlook},
  author={Liu, Bo and Ding, Ming and Shaham, Sina and Rahayu, Wenny and Farokhi, Farhad and Lin, Zihuai},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={2},
  pages={1--36},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@inproceedings{fl1,
  title={Privacy-preserving deep learning},
  author={Shokri, Reza and Shmatikov, Vitaly},
  booktitle={Proceedings of the 22nd ACM SIGSAC conference on computer and communications security},
  pages={1310--1321},
  year={2015}
}

@misc{AES,
  author = {Morris Dworkin and Elaine Barker and James Nechvatal and James Foti and Lawrence Bassham and E. Roback and James Dray},
  title = {Advanced Encryption Standard (AES)},
  year = {2001},
  month = {2001-11-26},
  publisher = {Federal Inf. Process. Stds. (NIST FIPS), National Institute of Standards and Technology, Gaithersburg, MD},
  doi = {https://doi.org/10.6028/NIST.FIPS.197},
  language = {en},
}
@Article{numpy,
 title         = {Array programming with {NumPy}},
 author        = {Charles R. Harris and K. Jarrod Millman and St{\'{e}}fan J.
                 van der Walt and Ralf Gommers and Pauli Virtanen and David
                 Cournapeau and Eric Wieser and Julian Taylor and Sebastian
                 Berg and Nathaniel J. Smith and Robert Kern and Matti Picus
                 and Stephan Hoyer and Marten H. van Kerkwijk and Matthew
                 Brett and Allan Haldane and Jaime Fern{\'{a}}ndez del
                 R{\'{i}}o and Mark Wiebe and Pearu Peterson and Pierre
                 G{\'{e}}rard-Marchant and Kevin Sheppard and Tyler Reddy and
                 Warren Weckesser and Hameer Abbasi and Christoph Gohlke and
                 Travis E. Oliphant},
 year          = {2020},
 month         = sep,
 journal       = {Nature},
 volume        = {585},
 number        = {7825},
 pages         = {357--362},
 doi           = {10.1038/s41586-020-2649-2},
 publisher     = {Springer Science and Business Media {LLC}},
 url           = {https://doi.org/10.1038/s41586-020-2649-2}
}

@online{keras,
  title={Keras},
  author={Chollet, Francois and others},
  year={2015},
  publisher={GitHub},
  url={https://github.com/fchollet/keras},
}
@article{fedprox,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={429--450},
  year={2020}
}
@article{fedoptim,
  title={Adaptive federated optimization},
  author={Reddi, Sashank and Charles, Zachary and Zaheer, Manzil and Garrett, Zachary and Rush, Keith and Kone{\v{c}}n{\`y}, Jakub and Kumar, Sanjiv and McMahan, H Brendan},
  journal={arXiv preprint arXiv:2003.00295},
  year={2020}
}
@inproceedings{salvia,
  title={Secure aggregation for federated learning in flower},
  author={Li, Kwing Hei and de Gusm{\~a}o, Pedro Porto Buarque and Beutel, Daniel J and Lane, Nicholas D},
  booktitle={Proceedings of the 2nd ACM International Workshop on Distributed Machine Learning},
  pages={8--14},
  year={2021}
}
@article{kadhe2020fastsecagg,
  title={Fastsecagg: Scalable secure aggregation for privacy-preserving federated learning},
  author={Kadhe, Swanand and Rajaraman, Nived and Koyluoglu, O Ozan and Ramchandran, Kannan},
  journal={arXiv preprint arXiv:2009.11248},
  year={2020}
}

@article{vfl,
  title={Vertical Federated Learning}, 
  author={Yang Liu and Yan Kang and Tianyuan Zou and Yanhong Pu and Yuanqin He and Xiaozhou Ye and Ye Ouyang and Ya-Qin Zhang and Qiang Yang},
  year={2022},
  eprint={2211.12814},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}


@misc{pycrypto,
   author = {{Python Cryptographic Authority}},
   year = {2021},
   note = {https://github.com/pyca/cryptography},
   title = {Cryptography (version 3.4.8)}
} 

@techreport{ecdh,
  title={Recommendation for pair-wise key-establishment schemes using discrete logarithm cryptography},
  author={Barker, Elaine and Chen, Lily and Keller, Sharon and Roginsky, Allen and Vassilev, Apostol and Davis, Richard},
  year={2017},
  institution={National Institute of Standards and Technology}
}

@article{diffie-hellman,
  title={New directions in cryptography},
  author={Diffie, Whitfield and Hellman, Martin},
  journal={IEEE transactions on Information Theory},
  volume={22},
  number={6},
  pages={644--654},
  year={1976},
  publisher={IEEE}
}

@inproceedings{LCC,
  title={Lagrange coded computing: Optimal design for resiliency, security, and privacy},
  author={Yu, Qian and Li, Songze and Raviv, Netanel and Kalan, Seyed Mohammadreza Mousavi and Soltanolkotabi, Mahdi and Avestimehr, Salman A},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1215--1225},
  year={2019},
  organization={PMLR}
}

@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE symposium on security and privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}

@inproceedings{wang2019user_lvl,
  title={Beyond inferring class representatives: User-level privacy leakage from federated learning},
  author={Wang, Zhibo and Song, Mengkai and Zhang, Zhifei and Song, Yang and Wang, Qian and Qi, Hairong},
  booktitle={IEEE INFOCOM 2019-IEEE Conference on Computer Communications},
  pages={2512--2520},
  year={2019},
  organization={IEEE}
}

@article{wei2020dp_fl,
  title={Federated learning with differential privacy: Algorithms and performance analysis},
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H and Farokhi, Farhad and Jin, Shi and Quek, Tony QS and Poor, H Vincent},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={15},
  pages={3454--3469},
  year={2020},
  publisher={IEEE}
}

@inproceedings{sabt2015TEE,
  title={Trusted execution environment: what it is, and what it is not},
  author={Sabt, Mohamed and Achemlal, Mohammed and Bouabdallah, Abdelmadjid},
  booktitle={2015 IEEE Trustcom/BigDataSE/ISPA},
  volume={1},
  pages={57--64},
  year={2015},
  organization={IEEE}
}

@article{pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{tensorflow,
  title={$\{$TensorFlow$\}$: A System for $\{$Large-Scale$\}$ Machine Learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th USENIX symposium on operating systems design and implementation (OSDI 16)},
  pages={265--283},
  year={2016}
}

@article{knott2021crypten,
  title={Crypten: Secure multi-party computation meets machine learning},
  author={Knott, Brian and Venkataraman, Shobha and Hannun, Awni and Sengupta, Shubho and Ibrahim, Mark and van der Maaten, Laurens},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{ibmfl,
title={IBM Federated Learning: an Enterprise Framework White Paper V0. 1},
author={Ludwig, Heiko and Baracaldo, Nathalie and Thomas, Gegi and Zhou, Yi and Anwar, Ali and Rajamoni, Shashank and Ong, Yuya and Radhakrishnan, Jayaram and Verma, Ashish and Sinn, Mathieu and others},
journal={arXiv preprint arXiv:2007.10987},
year={2020}
}

@incollection{pysyft,
  title={Pysyft: A library for easy federated learning},
  author={Ziller, Alexander and Trask, Andrew and Lopardo, Antonio and Szymkow, Benjamin and Wagner, Bobby and Bluemke, Emma and Nounahon, Jean-Mickael and Passerat-Palmbach, Jonathan and Prakash, Kritika and Rose, Nick and others},
  booktitle={Federated Learning Systems},
  pages={111--139},
  year={2021},
  publisher={Springer}
}

@article{openfl,
  title={OpenFL: An open-source framework for Federated Learning},
  author={Reina, G Anthony and Gruzdev, Alexey and Foley, Patrick and Perepelkina, Olga and Sharma, Mansi and Davidyuk, Igor and Trushkin, Ilya and Radionov, Maksim and Mokrov, Aleksandr and Agapov, Dmitry and others},
  journal={arXiv preprint arXiv:2105.06413},
  year={2021}
}

@misc{tffl,
  title={TensorFlow federated: machine learning on decentralized data.},
  author={Bonawitz, K and Eichner, H and Grieskamp, W and others},
  year={2020}
}

@inproceedings{homo_en,
  title={Multiparty computation from somewhat homomorphic encryption},
  author={Damg{\aa}rd, Ivan and Pastro, Valerio and Smart, Nigel and Zakarias, Sarah},
  booktitle={Annual Cryptology Conference},
  pages={643--662},
  year={2012},
  organization={Springer}
}

@misc{flower,
  doi = {10.48550/ARXIV.2007.14390},
  
  url = {https://arxiv.org/abs/2007.14390},
  
  author = {Beutel, Daniel J. and Topal, Taner and Mathur, Akhil and Qiu, Xinchi and Fernandez-Marques, Javier and Gao, Yan and Sani, Lorenzo and Li, Kwing Hei and Parcollet, Titouan and de Gusmão, Pedro Porto Buarque and Lane, Nicholas D.},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Flower: A Friendly Federated Learning Research Framework},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{dwork_dp_survey,
  title={Differential privacy: A survey of results},
  author={Dwork, Cynthia},
  booktitle={International conference on theory and applications of models of computation},
  pages={1--19},
  year={2008},
  organization={Springer}
}

@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy.},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Found. Trends Theor. Comput. Sci.},
  volume={9},
  number={3-4},
  pages={211--407},
  year={2014}
}

@inproceedings{abadi_dp_dl,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@article{wei_dp_fl,
  title={Federated learning with differential privacy: Algorithms and performance analysis},
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H and Farokhi, Farhad and Jin, Shi and Quek, Tony QS and Poor, H Vincent},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={15},
  pages={3454--3469},
  year={2020},
  publisher={IEEE}
}

@inproceedings{truex_hb,
  title={A hybrid approach to privacy-preserving federated learning},
  author={Truex, Stacey and Baracaldo, Nathalie and Anwar, Ali and Steinke, Thomas and Ludwig, Heiko and Zhang, Rui and Zhou, Yi},
  booktitle={Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security},
  pages={1--11},
  year={2019}
}

@inproceedings{paillier1999public,
  title={Public-key cryptosystems based on composite degree residuosity classes},
  author={Paillier, Pascal},
  booktitle={International conference on the theory and applications of cryptographic techniques},
  pages={223--238},
  year={1999},
  organization={Springer}
}

@inproceedings{fedscale,
  title={FedScale: Benchmarking model and system performance of federated learning},
  author={Lai, Fan and Dai, Yinwei and Zhu, Xiangfeng and Madhyastha, Harsha V and Chowdhury, Mosharaf},
  booktitle={Proceedings of the First Workshop on Systems Challenges in Reliable and Secure Federated Learning},
  pages={1--3},
  year={2021}
}

@article{mathur2021device,
  title={On-device federated learning with flower},
  author={Mathur, Akhil and Beutel, Daniel J and de Gusmao, Pedro Porto Buarque and Fernandez-Marques, Javier and Topal, Taner and Qiu, Xinchi and Parcollet, Titouan and Gao, Yan and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2104.03042},
  year={2021}
}

@article{li2021fedbn,
  title={Fedbn: Federated learning on non-iid features via local batch normalization},
  author={Li, Xiaoxiao and Jiang, Meirui and Zhang, Xiaofei and Kamp, Michael and Dou, Qi},
  journal={arXiv preprint arXiv:2102.07623},
  year={2021}
}

@article{horvath2021fjord,
  title={Fjord: Fair and accurate federated learning under heterogeneous targets with ordered dropout},
  author={Horvath, Samuel and Laskaridis, Stefanos and Almeida, Mario and Leontiadis, Ilias and Venieris, Stylianos and Lane, Nicholas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{nasr2019comprehensive,
  title={Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning},
  author={Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
  booktitle={2019 IEEE symposium on security and privacy (SP)},
  pages={739--753},
  year={2019},
  organization={IEEE}
}
@inproceedings{10.1145/3488659.3493776,
author = {Li, Kwing Hei and de Gusm\~{a}o, Pedro Porto Buarque and Beutel, Daniel J. and Lane, Nicholas D.},
title = {Secure Aggregation for Federated Learning in Flower},
year = {2021},
isbn = {9781450391344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488659.3493776},
doi = {10.1145/3488659.3493776},
abstract = {Federated Learning (FL) allows parties to learn a shared prediction model by delegating the training computation to clients and aggregating all the separately trained models on the server. To prevent private information being inferred from local models, Secure Aggregation (SA) protocols are used to ensure that the server is unable to inspect individual trained models as it aggregates them. However, current implementations of SA in FL frameworks have limitations, including vulnerability to client dropouts or configuration difficulties.In this paper, we present Salvia, an implementation of SA for Python users in the Flower FL framework. Based on the SecAgg(+) protocols for a semi-honest threat model, Salvia is robust against client dropouts and exposes a flexible and easy-to-use API that is compatible with various machine learning frameworks. We show that Salvia's experimental performance is consistent with SecAgg(+)'s theoretical computation and communication complexities.},
booktitle = {Proceedings of the 2nd ACM International Workshop on Distributed Machine Learning},
pages = {8–14},
numpages = {7},
keywords = {federated learning, secure aggregation, secure multi-party computation},
location = {Virtual Event, Germany},
series = {DistributedML '21}
}

@article{yasargao2022eccv,
  title={Federated Self-supervised Learning for Video Understanding},
  author={ Abbas ur Rehman, Yasar and  Gao, Yan and Shen, Jiajun and  Porto Buarque de Gusm{\~a}o, Pedro and  Lane, Nicholas D },
  journal={European Conference on Computer Vision (ECCV)},
  year={2022}
}
@inproceedings{gao2021end,
  title={End-to-end speech recognition from federated acoustic models}, 
  author={Gao, Yan and Parcollet, Titouan and Zaiem, Salah and Fernandez-Marques, Javier and Porto Buarque de Gusmão, Pedro and Beutel, Daniel J and Lane, Nicholas D},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2022},
  volume={},
  number={},
}
@inproceedings{qiu2021zerofl,
  title={ZeroFL: Efficient On-Device Training for Federated Learning with Local Sparsity},
  author={Qiu, Xinchi and Fernandez-Marques, Javier and Porto Buarque de Gusmão, Pedro and Gao, Yan and Parcollet, Titouan and Lane, Nicholas Donald},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2022}
}
@article{so2022lightsecagg,
  title={Lightsecagg: a lightweight and versatile design for secure aggregation in federated learning},
  author={So, Jinhyun and Nolet, Corey J and Yang, Chien-Sheng and Li, Songze and Yu, Qian and E Ali, Ramy and Guler, Basak and Avestimehr, Salman},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={694--720},
  year={2022}
}



@article{wei2022vertical,
  title={Vertical federated learning: Challenges, methodologies and experiments},
  author={Wei, Kang and Li, Jun and Ma, Chuan and Ding, Ming and Wei, Sha and Wu, Fan and Chen, Guihai and Ranbaduge, Thilina},
  journal={arXiv preprint arXiv:2202.04309},
  year={2022}
}

@article{yang2019federated,
  title={Federated machine learning: Concept and applications},
  author={Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={10},
  number={2},
  pages={1--19},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{yu2021toward,
  title={Toward resource-efficient federated learning in mobile edge computing},
  author={Yu, Rong and Li, Peichun},
  journal={IEEE Network},
  volume={35},
  number={1},
  pages={148--155},
  year={2021},
  publisher={IEEE}
}

@article{secagg,
  title={Practical secure aggregation for federated learning on user-held data},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  journal={arXiv preprint arXiv:1611.04482},
  year={2016}
}

@article{dlg,
  title={Deep leakage from gradients},
  author={Zhu, Ligeng and Liu, Zhijian and Han, Song},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{idlg,
  title={idlg: Improved deep leakage from gradients},
  author={Zhao, Bo and Mopuri, Konda Reddy and Bilen, Hakan},
  journal={arXiv preprint arXiv:2001.02610},
  year={2020}
}

@inproceedings{fu2021vf2boost,
  title={Vf2boost: Very fast vertical federated gradient boosting for cross-enterprise learning},
  author={Fu, Fangcheng and Shao, Yingxia and Yu, Lele and Jiang, Jiawei and Xue, Huanran and Tao, Yangyu and Cui, Bin},
  booktitle={Proceedings of the 2021 International Conference on Management of Data},
  pages={563--576},
  year={2021}
}

@article{jin2021cafe,
  title={CAFE: Catastrophic data leakage in vertical federated learning},
  author={Jin, Xiao and Chen, Pin-Yu and Hsu, Chia-Yi and Yu, Chia-Mu and Chen, Tianyi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={994--1006},
  year={2021}
}


@article{liu2020asymmetrical,
  title={Asymmetrical vertical federated learning},
  author={Liu, Yang and Zhang, Xiong and Wang, Libin},
  journal={arXiv preprint arXiv:2004.07427},
  year={2020}
}

@article{li2021survey,
  title={A survey on federated learning systems: vision, hype and reality for data privacy and protection},
  author={Li, Qinbin and Wen, Zeyi and Wu, Zhaomin and Hu, Sixu and Wang, Naibo and Li, Yuan and Liu, Xu and He, Bingsheng},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2021},
  publisher={IEEE}
}

@article{chen2020vafl,
  title={Vafl: a method of vertical asynchronous federated learning},
  author={Chen, Tianyi and Jin, Xiao and Sun, Yuejiao and Yin, Wotao},
  journal={arXiv preprint arXiv:2007.06081},
  year={2020}
}

@article{xgboost,
  title={Xgboost: extreme gradient boosting},
  author={Chen, Tianqi and He, Tong and Benesty, Michael and Khotilovich, Vadim and Tang, Yuan and Cho, Hyunsu and Chen, Kailong and Mitchell, Rory and Cano, Ignacio and Zhou, Tianyi and others},
  journal={R package version 0.4-2},
  volume={1},
  number={4},
  pages={1--4},
  year={2015}
}

\textbf{@inproceedings{taobao,
  title={A Novel CTR Prediction Model Based On DeepFM For Taobao Data},
  author={Li, LinShu and Hong, Jianbo and Min, Sitao and Xue, Yunfan},
  booktitle={2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID)},
  pages={184--187},
  year={2021},
  organization={IEEE}
}}

@article{bankdataset,
  title={Using data mining for bank direct marketing: An application of the crisp-dm methodology},
  author={Moro, Sergio and Laureano, Raul and Cortez, Paulo},
  year={2011},
  publisher={EUROSIS-ETI}
}

@inproceedings{fu2022blindfl,
  title={Blindfl: Vertical federated machine learning without peeking into your data},
  author={Fu, Fangcheng and Xue, Huanran and Cheng, Yong and Tao, Yangyu and Cui, Bin},
  booktitle={Proceedings of the 2022 International Conference on Management of Data},
  pages={1316--1330},
  year={2022}
}
@article{zhang2020acml,
  title={Additively homomorphical encryption based deep neural network for asymmetrically collaborative machine learning},
  author={Zhang, Yifei and Zhu, Hao},
  journal={arXiv preprint arXiv:2007.06849},
  year={2020}
}
@article{kang2022prada,
  title={Privacy-preserving federated adversarial domain adaptation over feature groups for interpretability},
  author={Kang, Yan and He, Yuanqin and Luo, Jiahuan and Fan, Tao and Liu, Yang and Yang, Qiang},
  journal={IEEE Transactions on Big Data},
  year={2022},
  publisher={IEEE}
}
@article{cai2022sfa,
  title={Secure Forward Aggregation for Vertical Federated Neural Networks},
  author={Cai, Shuowei and Chai, Di and Yang, Liu and Zhang, Junxue and Jin, Yilun and Wang, Leye and Guo, Kun and Chen, Kai},
  journal={arXiv preprint arXiv:2207.00165},
  year={2022}
}
@inproceedings{adultincome,
  title={Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid.},
  author={Kohavi, Ron and others},
  booktitle={Kdd},
  volume={96},
  pages={202--207},
  year={1996}
}

@article{mimic-benchmark,
  author={Harutyunyan, Hrayr and Khachatrian, Hrant and Kale, David C. and Ver Steeg, Greg and Galstyan, Aram},
  title={Multitask learning and benchmarking with clinical time series data},
  journal={Scientific Data},
  year={2019},
  volume={6},
  number={1},
  pages={96},
  issn={2052-4463},
  doi={10.1038/s41597-019-0103-9},
  url={https://doi.org/10.1038/s41597-019-0103-9}
}

@article{mimic3,
author = {Johnson, Alistair and Pollard, Tom and Shen, Lu and Lehman, Li-wei and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Celi, Leo and Mark, Roger},
year = {2016},
month = {05},
pages = {160035},
title = {MIMIC-III, a freely accessible critical care database},
volume = {3},
journal = {Scientific Data},
doi = {10.1038/sdata.2016.35}
}

@inproceedings{yin2021see,
  title={See through gradients: Image batch recovery via gradinversion},
  author={Yin, Hongxu and Mallya, Arun and Vahdat, Arash and Alvarez, Jose M and Kautz, Jan and Molchanov, Pavlo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16337--16346},
  year={2021}
}

@misc{dpvfl,
      title={Differentially Private Vertical Federated Learning}, 
      author={Thilina Ranbaduge and Ming Ding},
      year={2022},
      eprint={2211.06782},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tradeoffvfl,
      title={A Framework for Evaluating Privacy-Utility Trade-off in Vertical Federated Learning}, 
      author={Yan Kang and Jiahuan Luo and Yuanqin He and Xiaojin Zhang and Lixin Fan and Qiang Yang},
      year={2022},
      eprint={2209.03885},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{cai2023secure,
  title={Secure forward aggregation for vertical federated neural networks},
  author={Cai, Shuowei and Chai, Di and Yang, Liu and Zhang, Junxue and Jin, Yilun and Wang, Leye and Guo, Kun and Chen, Kai},
  booktitle={Trustworthy Federated Learning: First International Workshop, FL 2022, Held in Conjunction with IJCAI 2022, Vienna, Austria, July 23, 2022, Revised Selected Papers},
  pages={115--129},
  year={2023},
  organization={Springer}
}


@article{zheng2022making,
  title={Making Split Learning Resilient to Label Leakage by Potential Energy Loss},
  author={Zheng, Fei and Chen, Chaochao and Yao, Binhui and Zheng, Xiaolin},
  journal={arXiv preprint arXiv:2210.09617},
  year={2022}
}

@misc{sun2022label,
      title={Label Leakage and Protection from Forward Embedding in Vertical Federated Learning}, 
      author={Jiankai Sun and Xin Yang and Yuanshun Yao and Chong Wang},
      year={2022},
      eprint={2203.01451},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{jiang2022vfps,
title={{VF}-{PS}: How to Select Important Participants in Vertical Federated Learning, Efficiently and Securely?},
author={Jiawei Jiang and Lukas Burkhalter and Fangcheng Fu and Bolin Ding and Bo Du and Anwar Hithnawi and Bo Li and Ce Zhang},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=vNrSXIFJ9wz}
}



@inproceedings{Fang_2021,
  title={Large-scale secure XGB for vertical federated learning},
  author={Fang, Wenjing and Zhao, Derun and Tan, Jin and Chen, Chaochao and Yu, Chaofan and Wang, Li and Wang, Lei and Zhou, Jun and Zhang, Benyu},
  booktitle={Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
  pages={443--452},
  year={2021}
}


@article{gu2021privacy,
  title={Privacy-preserving asynchronous vertical federated learning algorithms for multiparty collaborative learning},
  author={Gu, Bin and Xu, An and Huo, Zhouyuan and Deng, Cheng and Huang, Heng},
  journal={IEEE transactions on neural networks and learning systems},
  volume={33},
  number={11},
  pages={6103--6115},
  year={2021},
  publisher={IEEE}
}

@inproceedings{shi2022,
author = {Yan, Yang and Yang, Guozheng and Gao, Yan and Zang, Cheng and Chen, Jiajun and Wang, Qiang},
title = {Multi-Participant Vertical Federated Learning Based Time Series Prediction},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532238},
doi = {10.1145/3532213.3532238},
abstract = {Federated learning (FL) ensures multi-party can train a model together while avoiding privacy leakage. Our vertical federated learning (VFL) task tackles the following scenarios: i) all parties share the same sample space but differ in feature space, ii) only one party holds the label data. Our contribution is twofold: i) proposing a novel aggregation strategy to show that embedding learning is qualified to handle the challenge of VFL, ii) Incorporating specific strategy of Secure Multi-party Computation (MPC) into the training phase to remain the dataset at each local machine. We focus on time series scenarios and choose Gated Recurrent Unit (GRU) as our basic algorithm. We evaluate our method on both Google stock data for regression prediction and Kyoto University Benchmark Data for classification prediction to illustrate the performance of the results in terms of computational and communication complexities.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {165–171},
numpages = {7},
keywords = {time series, federated learning, privacy, vertical federated learning, secure multi-party computation, ensemble learning},
location = {Tianjin, China},
series = {ICCAI '22}
}

@misc{li2023fedvs,
      title={FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models}, 
      author={Songze Li and Duanyi Yao and Jin Liu},
      year={2023},
      eprint={2304.13407},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{cohen2017emnist,
  title={EMNIST: Extending MNIST to handwritten letters},
  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
  booktitle={2017 international joint conference on neural networks (IJCNN)},
  pages={2921--2926},
  year={2017},
  organization={IEEE}
}

@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

%-----------------latent representation:

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{mixmatch,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{semifl,
  title={SemiFL: Semi-supervised federated learning for unlabeled clients with alternate training},
  author={Diao, Enmao and Ding, Jie and Tarokh, Vahid},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17871--17884},
  year={2022}
}

@article{fixmatch,
  title={Fixmatch: Simplifying semi-supervised learning with consistency and confidence},
  author={Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={596--608},
  year={2020}
}

@article{fedmatch,
  title={Federated semi-supervised learning with inter-client consistency \& disjoint learning},
  author={Jeong, Wonyong and Yoon, Jaehong and Yang, Eunho and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:2006.12097},
  year={2020}
}

@inproceedings{fedavg,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  year={2017},
  organization={PMLR}
}


@article{jin2020towards,
  title={Towards utilizing unlabeled data in federated learning: A survey and prospective},
  author={Jin, Yilun and Wei, Xiguang and Liu, Yang and Yang, Qiang},
  journal={arXiv preprint arXiv:2002.11545},
  year={2020}
}

@inproceedings{zhang2021improving,
  title={Improving semi-supervised federated learning by reducing the gradient diversity of models},
  author={Zhang, Zhengming and Yang, Yaoqing and Yao, Zhewei and Yan, Yujun and Gonzalez, Joseph E and Ramchandran, Kannan and Mahoney, Michael W},
  booktitle={2021 IEEE International Conference on Big Data (Big Data)},
  pages={1214--1225},
  year={2021},
  organization={IEEE}
}

@article{xie2020unsupervised,
  title={Unsupervised data augmentation for consistency training},
  author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6256--6268},
  year={2020}
}

@article{yang2021federated,
  title={Federated semi-supervised learning for COVID region segmentation in chest CT using multi-national data from China, Italy, Japan},
  author={Yang, Dong and Xu, Ziyue and Li, Wenqi and Myronenko, Andriy and Roth, Holger R and Harmon, Stephanie and Xu, Sheng and Turkbey, Baris and Turkbey, Evrim and Wang, Xiaosong and others},
  journal={Medical image analysis},
  volume={70},
  pages={101992},
  year={2021},
  publisher={Elsevier}
}

@article{zhou2005tri,
  title={Tri-training: Exploiting unlabeled data using three classifiers},
  author={Zhou, Zhi-Hua and Li, Ming},
  journal={IEEE Transactions on knowledge and Data Engineering},
  volume={17},
  number={11},
  pages={1529--1541},
  year={2005},
  publisher={IEEE}
}

@article{rasmus2015semi,
  title={Semi-supervised learning with ladder networks},
  author={Rasmus, Antti and Berglund, Mathias and Honkala, Mikko and Valpola, Harri and Raiko, Tapani},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{jeong2020federated,
  title={Federated semi-supervised learning with inter-client consistency \& disjoint learning},
  author={Jeong, Wonyong and Yoon, Jaehong and Yang, Eunho and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:2006.12097},
  year={2020}
}

@article{gao2022federated,
  title={Federated self-supervised speech representations: Are we there yet?},
  author={Gao, Yan and Fernandez-Marques, Javier and Parcollet, Titouan and Mehrotra, Abhinav and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2204.02804},
  year={2022}
}

@inproceedings{lee2013pseudo,
  title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
  author={Lee, Dong-Hyun and others},
  booktitle={Workshop on challenges in representation learning, ICML},
  volume={3},
  number={2},
  pages={896},
  year={2013},
  organization={Atlanta}
}

@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={702--703},
  year={2020}
}

@article{thulasidasan2019mixup,
  title={On mixup training: Improved calibration and predictive uncertainty for deep neural networks},
  author={Thulasidasan, Sunil and Chennupati, Gopinath and Bilmes, Jeff A and Bhattacharya, Tanmoy and Michalak, Sarah},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{rehman2022federated,
  title={Federated self-supervised learning for video understanding},
  author={Rehman, Yasar Abbas Ur and Gao, Yan and Shen, Jiajun and de Gusmao, Pedro Porto Buarque and Lane, Nicholas},
  booktitle={European Conference on Computer Vision},
  pages={506--522},
  year={2022},
  organization={Springer}
}

@article{rehman2023dawa,
  title={L-DAWA: Layer-wise Divergence Aware Weight Aggregation in Federated Self-Supervised Visual Representation Learning},
  author={Rehman, Yasar Abbas Ur and Gao, Yan and de Gusm{\~a}o, Pedro Porto Buarque and Alibeigi, Mina and Shen, Jiajun and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2307.07393},
  year={2023}
}

@article{french2017self,
  title={Self-ensembling for visual domain adaptation},
  author={French, Geoffrey and Mackiewicz, Michal and Fisher, Mark},
  journal={arXiv preprint arXiv:1706.05208},
  year={2017}
}

@inproceedings{simclr,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@article{yang2022survey,
  title={A survey on deep semi-supervised learning},
  author={Yang, Xiangli and Song, Zixing and King, Irwin and Xu, Zenglin},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2022},
  publisher={IEEE}
}

@article{chapelle2009semi,
  title={Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]},
  author={Chapelle, Olivier and Scholkopf, Bernhard and Zien, Alexander},
  journal={IEEE Transactions on Neural Networks},
  volume={20},
  number={3},
  pages={542--542},
  year={2009},
  publisher={IEEE}
}

@article{miyato2018virtual,
  title={Virtual adversarial training: a regularization method for supervised and semi-supervised learning},
  author={Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={8},
  pages={1979--1993},
  year={2018},
  publisher={IEEE}
}

@article{sajjadi2016regularization,
  title={Regularization with stochastic transformations and perturbations for deep semi-supervised learning},
  author={Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen, Tolga},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}
@article{tarvainen2017mean,
  title={Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
  author={Tarvainen, Antti and Valpola, Harri},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{lim2020federated,
  title={Federated learning in mobile edge networks: A comprehensive survey},
  author={Lim, Wei Yang Bryan and Luong, Nguyen Cong and Hoang, Dinh Thai and Jiao, Yutao and Liang, Ying-Chang and Yang, Qiang and Niyato, Dusit and Miao, Chunyan},
  journal={IEEE Communications Surveys \& Tutorials},
  volume={22},
  number={3},
  pages={2031--2063},
  year={2020},
  publisher={IEEE}
}

@article{nguyen2023boosting,
  title={Boosting Semi-Supervised Learning by bridging high and low-confidence predictions},
  author={Nguyen, Khanh-Binh and Yang, Joon-Sung},
  journal={arXiv preprint arXiv:2308.07509},
  year={2023}
}

@inproceedings{DBLP:conf/iclr/MoschellaMFNLR23,
  author       = {Luca Moschella and
                  Valentino Maiorca and
                  Marco Fumero and
                  Antonio Norelli and
                  Francesco Locatello and
                  Emanuele Rodol{\`{a}}},
  title        = {Relative representations enable zero-shot latent space communication},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=SrC-nwieGJ},
  timestamp    = {Fri, 30 Jun 2023 14:55:52 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/MoschellaMFNLR23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/MorcosRB18,
  author       = {Ari S. Morcos and
                  Maithra Raghu and
                  Samy Bengio},
  editor       = {Samy Bengio and
                  Hanna M. Wallach and
                  Hugo Larochelle and
                  Kristen Grauman and
                  Nicol{\`{o}} Cesa{-}Bianchi and
                  Roman Garnett},
  title        = {Insights on representational similarity in neural networks with canonical
                  correlation},
  booktitle    = {Advances in Neural Information Processing Systems 31: Annual Conference
                  on Neural Information Processing Systems 2018, NeurIPS 2018, December
                  3-8, 2018, Montr{\'{e}}al, Canada},
  pages        = {5732--5741},
  year         = {2018},
  url          = {https://proceedings.neurips.cc/paper/2018/hash/a7a3d70c6d17a73140918996d03c014f-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/MorcosRB18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:journals/corr/LiYCLH15,
  author       = {Yixuan Li and
                  Jason Yosinski and
                  Jeff Clune and
                  Hod Lipson and
                  John E. Hopcroft},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Convergent Learning: Do different neural networks learn the same representations?},
  booktitle    = {4th International Conference on Learning Representations, {ICLR} 2016,
                  San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year         = {2016},
  url          = {http://arxiv.org/abs/1511.07543},
  timestamp    = {Fri, 18 Nov 2022 15:40:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/LiYCLH15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/Kornblith0LH19,
  author       = {Simon Kornblith and
                  Mohammad Norouzi and
                  Honglak Lee and
                  Geoffrey E. Hinton},
  editor       = {Kamalika Chaudhuri and
                  Ruslan Salakhutdinov},
  title        = {Similarity of Neural Network Representations Revisited},
  booktitle    = {Proceedings of the 36th International Conference on Machine Learning,
                  {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {97},
  pages        = {3519--3529},
  publisher    = {{PMLR}},
  year         = {2019},
  url          = {http://proceedings.mlr.press/v97/kornblith19a.html},
  timestamp    = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/Kornblith0LH19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/iclr/TsitsulinMMKBOM20,
  author       = {Anton Tsitsulin and
                  Marina Munkhoeva and
                  Davide Mottin and
                  Panagiotis Karras and
                  Alexander M. Bronstein and
                  Ivan V. Oseledets and
                  Emmanuel M{\"{u}}ller},
  title        = {The Shape of Data: Intrinsic Distance for Data Distributions},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=HyebplHYwB},
  timestamp    = {Thu, 21 Jan 2021 17:36:45 +0100},
  biburl       = {https://dblp.org/rec/conf/iclr/TsitsulinMMKBOM20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/VulicRS20,
  author       = {Ivan Vulic and
                  Sebastian Ruder and
                  Anders S{\o}gaard},
  editor       = {Bonnie Webber and
                  Trevor Cohn and
                  Yulan He and
                  Yang Liu},
  title        = {Are All Good Word Vector Spaces Isomorphic?},
  booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages        = {3178--3192},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.emnlp-main.257},
  doi          = {10.18653/v1/2020.emnlp-main.257},
  timestamp    = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/VulicRS20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/ZaheerKRPSS17,
  author       = {Manzil Zaheer and
                  Satwik Kottur and
                  Siamak Ravanbakhsh and
                  Barnab{\'{a}}s P{\'{o}}czos and
                  Ruslan Salakhutdinov and
                  Alexander J. Smola},
  editor       = {Isabelle Guyon and
                  Ulrike von Luxburg and
                  Samy Bengio and
                  Hanna M. Wallach and
                  Rob Fergus and
                  S. V. N. Vishwanathan and
                  Roman Garnett},
  title        = {Deep Sets},
  booktitle    = {Advances in Neural Information Processing Systems 30: Annual Conference
                  on Neural Information Processing Systems 2017, December 4-9, 2017,
                  Long Beach, CA, {USA}},
  pages        = {3391--3401},
  year         = {2017},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/ZaheerKRPSS17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/frai/HenselMR21,
  author       = {Felix Hensel and
                  Michael Moor and
                  Bastian Rieck},
  title        = {A Survey of Topological Machine Learning Methods},
  journal      = {Frontiers Artif. Intell.},
  volume       = {4},
  pages        = {681108},
  year         = {2021},
  url          = {https://doi.org/10.3389/frai.2021.681108},
  doi          = {10.3389/frai.2021.681108},
  timestamp    = {Wed, 07 Dec 2022 23:04:15 +0100},
  biburl       = {https://dblp.org/rec/journals/frai/HenselMR21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/MoorHRB20,
  author       = {Michael Moor and
                  Max Horn and
                  Bastian Rieck and
                  Karsten M. Borgwardt},
  title        = {Topological Autoencoders},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning,
                  {ICML} 2020, 13-18 July 2020, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {7045--7054},
  publisher    = {{PMLR}},
  year         = {2020},
  url          = {http://proceedings.mlr.press/v119/moor20a.html},
  timestamp    = {Tue, 15 Dec 2020 17:40:19 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/MoorHRB20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/cvpr/HeZRS16,
  author       = {Kaiming He and
                  Xiangyu Zhang and
                  Shaoqing Ren and
                  Jian Sun},
  title        = {Deep Residual Learning for Image Recognition},
  booktitle    = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages        = {770--778},
  publisher    = {{IEEE} Computer Society},
  year         = {2016},
  url          = {https://doi.org/10.1109/CVPR.2016.90},
  doi          = {10.1109/CVPR.2016.90},
  timestamp    = {Fri, 24 Mar 2023 00:02:57 +0100},
  biburl       = {https://dblp.org/rec/conf/cvpr/HeZRS16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{yurochkin2019bayesian,
  title={Bayesian nonparametric federated learning of neural networks},
  author={Yurochkin, Mikhail and Agarwal, Mayank and Ghosh, Soumya and Greenewald, Kristjan and Hoang, Nghia and Khazaeni, Yasaman},
  booktitle={International Conference on Machine Learning},
  pages={7252--7261},
  year={2019},
  organization={PMLR}
}

@inproceedings{
reddi2020adaptive,
title={Adaptive Federated Optimization},
author={Sashank J. Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone{\v{c}}n{\'y} and Sanjiv Kumar and Hugh Brendan McMahan},
booktitle={International Conference on Learning Representations},
year={2021},
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}

@article{bachman2014learning,
  title={Learning with pseudo-ensembles},
  author={Bachman, Philip and Alsharif, Ouais and Precup, Doina},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}


@article{fedprox,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={Proceedings of Machine learning and systems},
  volume={2},
  pages={429--450},
  year={2020}
}

@inproceedings{scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International conference on machine learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

@article{zhao2018federated,
  title={Federated learning with non-iid data},
  author={Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  journal={arXiv preprint arXiv:1806.00582},
  year={2018}
}